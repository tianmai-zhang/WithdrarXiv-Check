{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, time, re, httpx, json, arxiv, tarfile, pathlib\n",
    "from tqdm import tqdm\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "from openai import OpenAI\n",
    "import anthropic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Gemini_client = genai.Client(api_key='')\n",
    "GPT_client = OpenAI(api_key='')\n",
    "Claude_client = anthropic.Anthropic(api_key='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('WithdrarXiv-Check_train.csv')\n",
    "df_test = pd.read_csv('WithdrarXiv-Check_test.csv')\n",
    "df_train.shape, df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download paper source files from arXiv\n",
    "for i in tqdm(range(len(df_test))):\n",
    "    entry = df_test.iloc[i]\n",
    "    paper_id = entry['paper_id']\n",
    "    paper_src_path = next(arxiv.Client().results(arxiv.Search(id_list=[paper_id]))).download_source(dirpath='arxiv_src')\n",
    "    with tarfile.open(paper_src_path, \"r:gz\") as tar:\n",
    "        tar.extractall('arxiv_src/' + paper_id)\n",
    "    time.sleep(4) # arXiv recommends no more than 1 request every 3 seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding Problems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_checker = '''Please check the attached paper for critical errors and unsoundness problems that would invalidate the conclusions. You can ignore minor issues (e.g, typos and formatting errors) and limitations that have been properly acknowledged.\n",
    "In your final output, give me up to 5 most critical problems as a JSON object using the following schema: Entry = {\"Problem\": str, \"Location\": str, \"Explanation\": str}, Return: list[Entry]. For location, give page number, section number, equation number, or whatever applicable. You can end the list early if there are fewer problems. No need to provide references.'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF\n",
    "checker_problems_GeminiPro = []\n",
    "checker_model = 'Gemini 2.5 Pro'\n",
    "for i in tqdm(range(0, len(df_test))):\n",
    "    entry = df_test.iloc[i]\n",
    "    paper_pdf_url = 'https://arxiv.org/pdf/' + entry['paper_id']\n",
    "    paper_pdf_data = httpx.get(paper_pdf_url).content\n",
    "\n",
    "    Gemini_checker_response = Gemini_client.models.generate_content(\n",
    "        model=\"gemini-2.5-pro-preview-05-06\",\n",
    "        contents=[prompt_checker,\n",
    "                  types.Part.from_bytes(data=paper_pdf_data, mime_type='application/pdf')],\n",
    "        config=types.GenerateContentConfig(tools=[], response_mime_type='application/json',\n",
    "                                           temperature=0, seed=42,\n",
    "                                           thinking_config=types.ThinkingConfig(include_thoughts=True)),\n",
    "    )\n",
    "    if Gemini_checker_response.text != '':\n",
    "        try: Gemini_checker_response_json = json.loads(Gemini_checker_response.text)\n",
    "        except json.JSONDecodeError:\n",
    "            Gemini_checker_response_json = json.loads(re.sub(r'(?<!\\\\)\\\\(?![\"\\\\/bfnrtu])', r'\\\\\\\\', Gemini_checker_response.text))\n",
    "    else:\n",
    "        Gemini_checker_response_json = []\n",
    "    token_usage = Gemini_checker_response.usage_metadata\n",
    "\n",
    "    checker_problem_entry = {'entry_id': i,\n",
    "                             'retraction_id': entry['retraction_id'],\n",
    "                             'paper_id': entry['paper_id'],\n",
    "                             'retraction_comment': entry['retraction_comment'],\n",
    "                             'checker_model': checker_model,\n",
    "                             'attempt_id': 0,\n",
    "                             'problems': Gemini_checker_response_json,\n",
    "                             'token_usage': {'input': token_usage.prompt_token_count,\n",
    "                                             'thinking': token_usage.thoughts_token_count,\n",
    "                                             'output': token_usage.candidates_token_count}\n",
    "    }\n",
    "    checker_problems_GeminiPro.append(checker_problem_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('checker_problems_GeminiPro.json', 'w') as out:\n",
    "    json.dump(checker_problems_GeminiPro, out, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TeX\n",
    "checker_problems_GeminiPro_tex = []\n",
    "checker_model = 'Gemini 2.5 Flash'\n",
    "check_problems_pdf = checker_problems_GeminiPro\n",
    "for i in tqdm(range(0, len(df_test))):\n",
    "    entry = df_test.iloc[i]\n",
    "    paper_id = entry['paper_id']\n",
    "    latex = ''\n",
    "    if entry['tex_available']:\n",
    "        for tex_file in pathlib.Path('arxiv_src/'+paper_id).rglob(\"*.tex\"):\n",
    "            latex += tex_file.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "        if latex == '':\n",
    "            with open('arxiv_src/'+paper_id+'/'+paper_id, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                latex = f.read()\n",
    "        Gemini_checker_response = Gemini_client.models.generate_content(\n",
    "            model=\"gemini-2.5-pro-preview-05-06\",\n",
    "            contents=[prompt_checker+'\\n\\n'+latex],\n",
    "            config=types.GenerateContentConfig(tools=[], response_mime_type='application/json',\n",
    "                                               temperature=0, seed=42,\n",
    "                                               thinking_config=types.ThinkingConfig(include_thoughts=True)),\n",
    "        )\n",
    "        if Gemini_checker_response.text != '':\n",
    "            try: Gemini_checker_response_json = json.loads(Gemini_checker_response.text)\n",
    "            except json.JSONDecodeError:\n",
    "                Gemini_checker_response_json = json.loads(re.sub(r'(?<!\\\\)\\\\(?![\"\\\\/bfnrt])', r'\\\\\\\\', Gemini_checker_response.text))\n",
    "        else:\n",
    "            Gemini_checker_response_json = []\n",
    "        token_usage = Gemini_checker_response.usage_metadata\n",
    "\n",
    "        checker_problem_entry = {'entry_id': i,\n",
    "                                'retraction_id': entry['retraction_id'],\n",
    "                                'paper_id': entry['paper_id'],\n",
    "                                'retraction_comment': entry['retraction_comment'],\n",
    "                                'checker_model': checker_model,\n",
    "                                'attempt_id': 0,\n",
    "                                'problems': Gemini_checker_response_json,\n",
    "                                'token_usage': {'input': token_usage.prompt_token_count,\n",
    "                                                'thinking': token_usage.thoughts_token_count,\n",
    "                                                'output': token_usage.candidates_token_count}\n",
    "        }\n",
    "    else: # If no TeX available, use the corresponding PDF checker problems\n",
    "        for e in check_problems_pdf:\n",
    "            if e['paper_id'] == paper_id:\n",
    "                checker_problem_entry = e\n",
    "                break\n",
    "    checker_problems_GeminiPro_tex.append(checker_problem_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('checker_problems_GeminiPro_tex.json', 'w') as out:\n",
    "    json.dump(checker_problems_GeminiPro_tex, out, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get uploaded file list\n",
    "file_list = GPT_client.files.list(purpose=\"user_data\", order=\"asc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF\n",
    "checker_problems_o3 = []\n",
    "checker_model = 'o3'\n",
    "for i in tqdm(range(0, len(df_test))):\n",
    "    entry = df_test.iloc[i]\n",
    "    file_id = None\n",
    "    for f in file_list.data: # if already uploaded, use the existing file\n",
    "        if f.filename == entry['paper_id']+\".pdf\":\n",
    "            file_id = f.id\n",
    "            break\n",
    "    if file_id is None: # if not, upload it\n",
    "        paper_pdf_url = 'https://arxiv.org/pdf/' + entry['paper_id']\n",
    "        paper_pdf_data = httpx.get(paper_pdf_url).content\n",
    "        paper_pdf_upload = GPT_client.files.create(\n",
    "            file=(entry['paper_id']+\".pdf\", paper_pdf_data, \"application/pdf\"),\n",
    "            purpose=\"user_data\")\n",
    "        file_id = paper_pdf_upload.id\n",
    "\n",
    "    GPT_checker_response = GPT_client.responses.create(\n",
    "        model=\"o3-2025-04-16\",\n",
    "        input=[{\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"input_text\", \"text\": prompt_checker},\n",
    "            {\"type\": \"input_file\", \"file_id\": file_id},\n",
    "        ]}],\n",
    "        reasoning={'summary': 'auto'},\n",
    "        tools=[], #temperature=0, seed=42\n",
    "    )\n",
    "    GPT_checker_response_raw = GPT_checker_response.output[-1].content[0].text.lstrip('```json').rstrip('```')\n",
    "    try: GPT_checker_response_json = json.loads(GPT_checker_response_raw)\n",
    "    except json.JSONDecodeError:\n",
    "        # try: GPT_checker_response_raw = GPT_checker_response_raw.split(':\\n\\n')[1].lstrip('```json').rstrip('```') # for o4-mini\n",
    "        # except IndexError: GPT_checker_response_raw = GPT_checker_response_raw.split('.\\n\\n')[1].lstrip('```json').rstrip('```')\n",
    "        GPT_checker_response_json = json.loads(re.sub(r'(?<!\\\\)\\\\(?![\"\\\\/bfnrt])', r'\\\\\\\\', GPT_checker_response_raw))\n",
    "    GPT_checker_response_summary = [s.text for s in GPT_checker_response.output[0].summary] if GPT_checker_response.output[0].summary else []\n",
    "    token_usage = GPT_checker_response.usage\n",
    "\n",
    "    checker_problem_entry = {'entry_id': i,\n",
    "                            'retraction_id': entry['retraction_id'],\n",
    "                            'paper_id': entry['paper_id'],\n",
    "                            'retraction_comment': entry['retraction_comment'],\n",
    "                            'checker_model': checker_model,\n",
    "                            'attempt_id': 0,\n",
    "                            'problems': GPT_checker_response_json,\n",
    "                            'think_summary': GPT_checker_response_summary,\n",
    "                            'token_usage': {'input': token_usage.input_tokens,\n",
    "                                            'thinking': token_usage.output_tokens_details.reasoning_tokens,\n",
    "                                            'output': token_usage.output_tokens - token_usage.output_tokens_details.reasoning_tokens}\n",
    "    }\n",
    "    checker_problems_o3.append(checker_problem_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('checker_problems_o3.json', 'w') as out:\n",
    "    json.dump(checker_problems_o3, out, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TeX\n",
    "checker_problems_o3_tex = []\n",
    "checker_model = 'o3'\n",
    "check_problems_pdf = checker_problems_o3\n",
    "for i in tqdm(range(0, len(df_test))):\n",
    "    entry = df_test.iloc[i]\n",
    "    paper_id = entry['paper_id']\n",
    "    latex = ''\n",
    "    if entry['tex_available']:\n",
    "        for tex_file in pathlib.Path('arxiv_src/'+paper_id).rglob(\"*.tex\"):\n",
    "            latex += tex_file.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "        if latex == '':\n",
    "            with open('arxiv_src/'+paper_id+'/'+paper_id, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                latex = f.read()\n",
    "\n",
    "        GPT_checker_response = GPT_client.responses.create(\n",
    "            model=\"o3-2025-04-16\",\n",
    "            input=[{\"role\": \"user\", \"content\": prompt_checker+'\\n\\n'+latex}],\n",
    "            reasoning={'summary': 'detailed'},\n",
    "            tools=[], #temperature=0, seed=42\n",
    "        )\n",
    "        GPT_checker_response_raw = GPT_checker_response.output[-1].content[0].text.lstrip('```json').rstrip('```')\n",
    "        try: GPT_checker_response_json = json.loads(GPT_checker_response_raw)\n",
    "        except json.JSONDecodeError:\n",
    "            # try: GPT_checker_response_raw = GPT_checker_response_raw.split(':\\n\\n')[1].lstrip('```json').rstrip('```') # for o4-mini\n",
    "            # except IndexError: GPT_checker_response_raw = GPT_checker_response_raw.split('.\\n\\n')[1].lstrip('```json').rstrip('```')\n",
    "            GPT_checker_response_json = json.loads(re.sub(r'(?<!\\\\)\\\\(?![\"\\\\/bfnrt])', r'\\\\\\\\', GPT_checker_response_raw))\n",
    "        GPT_checker_response_summary = [s.text for s in GPT_checker_response.output[0].summary] if GPT_checker_response.output[0].summary else []\n",
    "        token_usage = GPT_checker_response.usage\n",
    "\n",
    "        checker_problem_entry = {'entry_id': i,\n",
    "                                'retraction_id': entry['retraction_id'],\n",
    "                                'paper_id': entry['paper_id'],\n",
    "                                'retraction_comment': entry['retraction_comment'],\n",
    "                                'checker_model': checker_model,\n",
    "                                'attempt_id': 0,\n",
    "                                'problems': GPT_checker_response_json,\n",
    "                                'think_summary': GPT_checker_response_summary,\n",
    "                                'token_usage': {'input': token_usage.input_tokens,\n",
    "                                                'thinking': token_usage.output_tokens_details.reasoning_tokens,\n",
    "                                                'output': token_usage.output_tokens - token_usage.output_tokens_details.reasoning_tokens}\n",
    "        }\n",
    "    else: # If no TeX available, use the corresponding PDF checker problems\n",
    "        for e in check_problems_pdf:\n",
    "            if e['paper_id'] == paper_id:\n",
    "                checker_problem_entry = e\n",
    "                break\n",
    "    checker_problems_o3_tex.append(checker_problem_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('checker_problems_o3_tex.json', 'w') as out:\n",
    "    json.dump(checker_problems_o3_tex, out, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Claude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF\n",
    "checker_problems_Claude = []\n",
    "checker_model = 'Claude 3.7 Sonnet'\n",
    "for i in tqdm(range(0, len(df_test))):\n",
    "    entry = df_test.iloc[i]\n",
    "    paper_pdf_url = 'https://arxiv.org/pdf/' + entry['paper_id']\n",
    "\n",
    "    Claude_checker_response = Claude_client.messages.create(\n",
    "        model=\"claude-3-7-sonnet-20250219\",\n",
    "        messages=[{\"role\": \"user\", \"content\": [\n",
    "            {\"type\": \"document\", \"source\": {\"type\": \"url\", \"url\": paper_pdf_url}}, # Anthropic recommends putting the document first\n",
    "            {\"type\": \"text\", \"text\": prompt_checker},\n",
    "        ]}],\n",
    "        max_tokens=16000,\n",
    "        thinking={\"type\": \"enabled\", \"budget_tokens\": 14000},\n",
    "        tools=[], temperature=1, #seed=42,\n",
    "    )\n",
    "    Claude_checker_response_raw = Claude_checker_response.content[-1].text.lstrip('```json').rstrip('```')\n",
    "    try: Claude_checker_response_json = json.loads(Claude_checker_response_raw)\n",
    "    except json.JSONDecodeError:\n",
    "        if re.search('\\\"Problem\\\":', Claude_checker_response_raw) is None:\n",
    "            Claude_checker_response_json = []\n",
    "        else:\n",
    "            Claude_checker_response_raw = Claude_checker_response_raw.split(':\\n\\n')[1].lstrip('```json').split('\\n\\n')[0].rstrip('```')\n",
    "            Claude_checker_response_json = json.loads(re.sub(r'(?<!\\\\)\\\\(?![\"\\\\/bfnrt])', r'\\\\\\\\', Claude_checker_response_raw))\n",
    "    Claude_checker_response_think = [block.thinking for block in Claude_checker_response.content[:-1]]\n",
    "    token_usage = Claude_checker_response.usage\n",
    "    token_usage_output = Claude_client.messages.count_tokens(\n",
    "        model=\"claude-3-7-sonnet-20250219\",\n",
    "        messages=[{\"role\": \"user\", \"content\": Claude_checker_response.content[-1].text}],\n",
    "    ).input_tokens\n",
    "\n",
    "    checker_problem_entry = {'entry_id': i,\n",
    "                            'retraction_id': entry['retraction_id'],\n",
    "                            'paper_id': entry['paper_id'],\n",
    "                            'retraction_comment': entry['retraction_comment'],\n",
    "                            'checker_model': checker_model,\n",
    "                            'attempt_id': 0,\n",
    "                            'problems': Claude_checker_response_json,\n",
    "                            'think_process': Claude_checker_response_think,\n",
    "                            'token_usage': {'input': token_usage.input_tokens,\n",
    "                                            'thinking': token_usage.output_tokens - token_usage_output,\n",
    "                                            'output': token_usage_output}\n",
    "    }\n",
    "    checker_problems_Claude.append(checker_problem_entry)\n",
    "    if token_usage.input_tokens > 80000: # for rate limit\n",
    "        time.sleep(90)\n",
    "    elif token_usage.input_tokens > 40000:\n",
    "        time.sleep(45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('checker_problems_Claude.json', 'w') as out:\n",
    "    json.dump(checker_problems_Claude, out, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TeX\n",
    "checker_problems_Claude_tex = []\n",
    "checker_model = 'Claude 3.7 Sonnet'\n",
    "check_problems_pdf = checker_problems_Claude\n",
    "for i in tqdm(range(0, len(df_test))):\n",
    "    entry = df_test.iloc[i]\n",
    "    paper_id = entry['paper_id']\n",
    "    latex = ''\n",
    "    if entry['tex_available']:\n",
    "        for tex_file in pathlib.Path('arxiv_src/'+paper_id).rglob(\"*.tex\"):\n",
    "            latex += tex_file.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "        if latex == '':\n",
    "            with open('arxiv_src/'+paper_id+'/'+paper_id, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "                latex = f.read()\n",
    "\n",
    "        Claude_checker_response = Claude_client.messages.create(\n",
    "            model=\"claude-3-7-sonnet-20250219\",\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt_checker+'\\n\\n'+latex}],\n",
    "            max_tokens=16000,\n",
    "            thinking={\"type\": \"enabled\", \"budget_tokens\": 14000},\n",
    "            tools=[], temperature=1, #seed=42,\n",
    "        )\n",
    "        Claude_checker_response_raw = Claude_checker_response.content[-1].text.lstrip('```json').rstrip('```')\n",
    "        try: Claude_checker_response_json = json.loads(Claude_checker_response_raw)\n",
    "        except json.JSONDecodeError:\n",
    "            if re.search('\\\"Problem\\\":', Claude_checker_response_raw) is None:\n",
    "                Claude_checker_response_json = []\n",
    "            else:\n",
    "                try: Claude_checker_response_raw = Claude_checker_response_raw.split(':\\n\\n')[1].lstrip('```json').split('\\n\\n')[0].rstrip('```')\n",
    "                except IndexError: Claude_checker_response_raw = Claude_checker_response_raw.split('.\\n\\n')[1].lstrip('```json').split('\\n\\n')[0].rstrip('```')\n",
    "                Claude_checker_response_json = json.loads(re.sub(r'(?<!\\\\)\\\\(?![\"\\\\/bfnrt])', r'\\\\\\\\', Claude_checker_response_raw))\n",
    "        Claude_checker_response_think = [block.thinking for block in Claude_checker_response.content[:-1]]\n",
    "        token_usage = Claude_checker_response.usage\n",
    "        token_usage_output = Claude_client.messages.count_tokens(\n",
    "            model=\"claude-3-7-sonnet-20250219\",\n",
    "            messages=[{\"role\": \"user\", \"content\": Claude_checker_response.content[-1].text}],\n",
    "        ).input_tokens\n",
    "\n",
    "        checker_problem_entry = {'entry_id': i,\n",
    "                                'retraction_id': entry['retraction_id'],\n",
    "                                'paper_id': entry['paper_id'],\n",
    "                                'retraction_comment': entry['retraction_comment'],\n",
    "                                'checker_model': checker_model,\n",
    "                                'attempt_id': 0,\n",
    "                                'problems': Claude_checker_response_json,\n",
    "                                'think_process': Claude_checker_response_think,\n",
    "                                'token_usage': {'input': token_usage.input_tokens,\n",
    "                                                'thinking': token_usage.output_tokens - token_usage_output,\n",
    "                                                'output': token_usage_output}\n",
    "        }\n",
    "    else: # If no TeX available, use the corresponding PDF checker problems\n",
    "        for e in check_problems_pdf:\n",
    "            if e['paper_id'] == paper_id:\n",
    "                checker_problem_entry = e\n",
    "                break\n",
    "    checker_problems_Claude_tex.append(checker_problem_entry)\n",
    "    if token_usage.input_tokens > 80000: # for rate limit\n",
    "        time.sleep(90)\n",
    "    elif token_usage.input_tokens > 40000:\n",
    "        time.sleep(45)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('checker_problems_Claude_tex.json', 'w') as out:\n",
    "    json.dump(checker_problems_Claude_tex, out, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of identified problems\n",
    "nProb = [len(entry['problems']) for entry in checker_problems_GeminiPro]\n",
    "np.mean(nProb), np.quantile(nProb, [0, 0.25, 0.5, 0.75, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_cost(checker_problems, input_price, output_price):\n",
    "    input_total, think_total, output_total = 0, 0, 0\n",
    "    n = len(checker_problems)\n",
    "    for entry in checker_problems:\n",
    "        token_usage = entry['token_usage']\n",
    "        if token_usage['input'] is not None:\n",
    "            input_total += token_usage['input']\n",
    "            think_total += token_usage['thinking']\n",
    "            if token_usage['output'] is not None:\n",
    "                output_total += token_usage['output']\n",
    "        else:\n",
    "            n -= 1\n",
    "    input_avg = input_total / n\n",
    "    think_avg = think_total / n\n",
    "    output_avg = output_total / n\n",
    "    print(n, round(input_avg), round(think_avg), round(output_avg))\n",
    "    cost_avg = input_avg/1e6*input_price + (think_avg + output_avg)/1e6*output_price\n",
    "    print(round(cost_avg, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimate_cost(checker_problems_GeminiPro, 1.25, 10) # Gemini 2.5 Pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimate_cost(checker_problems_GeminiFlash, 0.15, 3.50) # Gemini 2.5 Flash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimate_cost(checker_problems_o3, 10, 40) # o3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimate_cost(checker_problems_o4mini, 1.1, 4.4) # o4mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimate_cost(checker_problems_Claude, 3, 15) # Claude 3.7 Sonnet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load checker problems\n",
    "checker_problems_GeminiPro = json.loads(open('checker_problems_GeminiPro.json').read())\n",
    "checker_problems_GeminiFlash = json.loads(open('checker_problems_GeminiFlash.json').read())\n",
    "checker_problems_GeminiPro_tex = json.loads(open('checker_problems_GeminiPro_tex.json').read())\n",
    "checker_problems_GeminiFlash_tex = json.loads(open('checker_problems_GeminiFlash_tex.json').read())\n",
    "\n",
    "checker_problems_o3 = json.loads(open('checker_problems_o3.json').read())\n",
    "checker_problems_o4mini = json.loads(open('checker_problems_o4mini.json').read())\n",
    "checker_problems_o3_tex = json.loads(open('checker_problems_o3_tex.json').read())\n",
    "checker_problems_o4mini_tex = json.loads(open('checker_problems_o4mini_tex.json').read())\n",
    "\n",
    "checker_problems_Claude = json.loads(open('checker_problems_Claude.json').read())\n",
    "checker_problems_Claude_tex = json.loads(open('checker_problems_Claude_tex.json').read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hit Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_judge_hit = '''My colleague was reading a paper and said there is a problem in it, as described below:\n",
    "Problem: {problem}\n",
    "Location: {location}\n",
    "Explanation: {explanation}\n",
    "\n",
    "I checked the paper and noticed that the authors have the following retraction comment:\n",
    "{retraction_comment}\n",
    "\n",
    "Is my colleague referring to exactly the same problem mentioned in the retraction comment? Your final answer should be \"Yes\" or \"No\". Default your answer to \"No\" and only give \"Yes\" if you are certain. You may explain your decision but please be concise.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gemini 2.5 Pro judge\n",
    "eval_hit_o4mini_Gemini = []\n",
    "checker_problems = checker_problems_o4mini\n",
    "judge_model = 'Gemini 2.5 Pro'\n",
    "for i in tqdm(range(0, len(df_test))):\n",
    "    eval_entry = checker_problems[i]\n",
    "    problems = eval_entry['problems']\n",
    "    if len(problems) == 0:\n",
    "        hit_entry = {'entry_id': eval_entry['entry_id'],\n",
    "                    'retraction_id': eval_entry['retraction_id'],\n",
    "                    'paper_id': eval_entry['paper_id'],\n",
    "                    'checker_model': eval_entry['checker_model'],\n",
    "                    'attempt_id': 0,\n",
    "                    'problem_id': None,\n",
    "                    'judge_model': judge_model,\n",
    "                    'judge_response': None,\n",
    "                    'hit': False,\n",
    "                    'judge_think_summary': None,\n",
    "                    'token_usage': {'input': None,\n",
    "                                    'thinking': None,\n",
    "                                    'output': None}\n",
    "        }\n",
    "        eval_hit_o4mini_Gemini.append(hit_entry)\n",
    "        continue\n",
    "    for j in range(0, len(problems)):\n",
    "        problem = problems[j]\n",
    "        Gemini_hit_response = Gemini_client.models.generate_content(\n",
    "            model=\"gemini-2.5-pro-preview-06-05\",\n",
    "            contents=[prompt_judge_hit.format(\n",
    "                problem=problem['Problem'],\n",
    "                location=problem['Location'],\n",
    "                explanation=problem['Explanation'],\n",
    "                retraction_comment=eval_entry['retraction_comment'])],\n",
    "            config=types.GenerateContentConfig(tools=[], temperature=0, seed=42,\n",
    "                                               thinking_config=types.ThinkingConfig(include_thoughts=True))\n",
    "        )\n",
    "        Gemini_hit_response_summary = [part.text for part in Gemini_hit_response.candidates[0].content.parts[:-1]]\n",
    "        token_usage = Gemini_hit_response.usage_metadata\n",
    "        hit_entry = {'entry_id': eval_entry['entry_id'],\n",
    "                    'retraction_id': eval_entry['retraction_id'],\n",
    "                    'paper_id': eval_entry['paper_id'],\n",
    "                    'checker_model': eval_entry['checker_model'],\n",
    "                    'attempt_id': 0,\n",
    "                    'problem_id': j,\n",
    "                    'judge_model': judge_model,\n",
    "                    'judge_response': Gemini_hit_response.text,\n",
    "                    'hit': re.search(r'[Yy]es', Gemini_hit_response.text) is not None,\n",
    "                    'judge_think_summary': Gemini_hit_response_summary,\n",
    "                    'token_usage': {'input': token_usage.prompt_token_count,\n",
    "                                    'thinking': token_usage.thoughts_token_count,\n",
    "                                    'output': token_usage.candidates_token_count}\n",
    "        }\n",
    "        eval_hit_o4mini_Gemini.append(hit_entry)\n",
    "        if hit_entry['hit']:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('eval_hit_o4mini_Gemini.json', 'w') as out:\n",
    "    json.dump(eval_hit_o4mini_Gemini, out, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# o3 judge\n",
    "eval_hit_o4mini_o3 = []\n",
    "checker_problems = checker_problems_o4mini\n",
    "judge_model = 'o3'\n",
    "for i in tqdm(range(0, len(df_test))):\n",
    "    eval_entry = checker_problems[i]\n",
    "    problems = eval_entry['problems']\n",
    "    if len(problems) == 0:\n",
    "        hit_entry = {'entry_id': eval_entry['entry_id'],\n",
    "                    'retraction_id': eval_entry['retraction_id'],\n",
    "                    'paper_id': eval_entry['paper_id'],\n",
    "                    'checker_model': eval_entry['checker_model'],\n",
    "                    'attempt_id': 0,\n",
    "                    'problem_id': None,\n",
    "                    'judge_model': judge_model,\n",
    "                    'judge_response': None,\n",
    "                    'hit': False,\n",
    "                    'judge_think_summary': None,\n",
    "                    'token_usage': {'input': None,\n",
    "                                    'thinking': None,\n",
    "                                    'output': None}\n",
    "        }\n",
    "        eval_hit_o4mini_o3.append(hit_entry)\n",
    "        continue\n",
    "    for j in range(0, len(problems)):\n",
    "        problem = problems[j]\n",
    "        GPT_hit_response = GPT_client.responses.create(\n",
    "            model=\"o3-2025-04-16\",\n",
    "            input=[{\"role\": \"user\", \"content\": prompt_judge_hit.format(\n",
    "                problem=problem['Problem'],\n",
    "                location=problem['Location'],\n",
    "                explanation=problem['Explanation'],\n",
    "                retraction_comment=eval_entry['retraction_comment'])}],\n",
    "            reasoning={'summary': 'auto'},\n",
    "            tools=[], #temperature=0, seed=42\n",
    "            service_tier=\"flex\",\n",
    "        )\n",
    "        GPT_hit_response_text = GPT_hit_response.output[-1].content[0].text\n",
    "        GPT_hit_response_summary = [s.text for s in GPT_hit_response.output[0].summary] if GPT_hit_response.output[0].summary else []\n",
    "        token_usage = GPT_hit_response.usage\n",
    "\n",
    "        hit_entry = {'entry_id': eval_entry['entry_id'],\n",
    "                    'retraction_id': eval_entry['retraction_id'],\n",
    "                    'paper_id': eval_entry['paper_id'],\n",
    "                    'checker_model': eval_entry['checker_model'],\n",
    "                    'attempt_id': 0,\n",
    "                    'problem_id': j,\n",
    "                    'judge_model': judge_model,\n",
    "                    'judge_response': GPT_hit_response_text,\n",
    "                    'hit': re.search(r'[Yy]es', GPT_hit_response_text) is not None,\n",
    "                    'judge_think_summary': GPT_hit_response_summary,\n",
    "                    'token_usage': {'input': token_usage.input_tokens,\n",
    "                                    'thinking': token_usage.output_tokens_details.reasoning_tokens,\n",
    "                                    'output': token_usage.output_tokens - token_usage.output_tokens_details.reasoning_tokens}\n",
    "        }\n",
    "        eval_hit_o4mini_o3.append(hit_entry)\n",
    "        if hit_entry['hit']:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('eval_hit_o4mini_o3.json', 'w') as out:\n",
    "    json.dump(eval_hit_o4mini_o3, out, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hit rate by a single judge\n",
    "eval_hit_df = pd.DataFrame(eval_hit_o4mini_o3)\n",
    "eval_hit_df_groups = eval_hit_df.groupby('entry_id').sum('hit')\n",
    "eval_hit_df_groups.shape[0], (eval_hit_df_groups['hit'] > 0).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine votes from both judges to get the final hit rate\n",
    "eval_hit_o4mini_Gemini = json.loads(open('eval_hit_o4mini_Gemini.json').read())\n",
    "eval_hit_o4mini_o3 = json.loads(open('eval_hit_o4mini_o3.json').read())\n",
    "hr1 = pd.DataFrame(eval_hit_o4mini_Gemini).groupby('entry_id').sum('hit')['hit']\n",
    "hr2 = pd.DataFrame(eval_hit_o4mini_o3).groupby('entry_id').sum('hit')['hit']\n",
    "len(hr1), len(hr2), (hr1 + hr2 == 2).sum()/len(hr2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_judge_tp = '''My colleague was reading this paper and said there is a critical problem in it, as described below:\n",
    "Problem: {problem}\n",
    "Location: {location}\n",
    "Explanation: {explanation}\n",
    "\n",
    "Is this problem a true problem or a false alarm? Please be careful because I don't want to get the authors into trouble by mistake. In your final answer, clearly indicate \"Yes, it is a true problem\" or \"No, it is a false alarm\". Make your best decision if you are unsure. You may explain your decision but please be concise.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gemini 2.5 Pro judge\n",
    "eval_tp_Claude_Gemini = []\n",
    "checker_problems = checker_problems_Claude\n",
    "judge_model = 'Gemini 2.5 Pro'\n",
    "for i in tqdm(range(0, len(df_test))):\n",
    "    eval_entry = checker_problems[i]\n",
    "    problems = eval_entry['problems']\n",
    "    paper_pdf_url = 'https://arxiv.org/pdf/' + eval_entry['paper_id']\n",
    "    paper_pdf_data = httpx.get(paper_pdf_url).content\n",
    "\n",
    "    if len(problems) == 0:\n",
    "        continue\n",
    "    for j in range(0, len(problems)):\n",
    "        problem = problems[j]\n",
    "        Gemini_tp_response = Gemini_client.models.generate_content(\n",
    "            model=\"gemini-2.5-pro-preview-06-05\",\n",
    "            contents=[\n",
    "                types.Part.from_bytes(data=paper_pdf_data, mime_type='application/pdf'), # put document first to enable prompt caching\n",
    "                prompt_judge_tp.format(\n",
    "                    problem=problem['Problem'],\n",
    "                    location=problem['Location'],\n",
    "                    explanation=problem['Explanation'])\n",
    "            ],\n",
    "            config=types.GenerateContentConfig(tools=[], temperature=0, seed=42,\n",
    "                thinking_config=types.ThinkingConfig(include_thoughts=True))\n",
    "        )\n",
    "        Gemini_tp_response_summary = [part.text for part in Gemini_tp_response.candidates[0].content.parts[:-1]]\n",
    "        token_usage = Gemini_tp_response.usage_metadata\n",
    "\n",
    "        tp_entry = {'entry_id': eval_entry['entry_id'],\n",
    "                    'retraction_id': eval_entry['retraction_id'],\n",
    "                    'paper_id': eval_entry['paper_id'],\n",
    "                    'checker_model': eval_entry['checker_model'],\n",
    "                    'attempt_id': 0,\n",
    "                    'problem_id': j,\n",
    "                    'judge_model': judge_model,\n",
    "                    'judge_response': Gemini_tp_response.text,\n",
    "                    'true_positive': re.search(r'[Yy]es', Gemini_tp_response.text) is not None,\n",
    "                    'judge_think_summary': Gemini_tp_response_summary,\n",
    "                    'token_usage': {'input': token_usage.prompt_token_count,\n",
    "                                    'thinking': token_usage.thoughts_token_count,\n",
    "                                    'output': token_usage.candidates_token_count}\n",
    "        }\n",
    "        eval_tp_Claude_Gemini.append(tp_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('eval_tp_Claude_Gemini.json', 'w') as out:\n",
    "    json.dump(eval_tp_Claude_Gemini, out, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get uploaded file list\n",
    "file_list = GPT_client.files.list(purpose=\"user_data\", order=\"asc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# o3 judge\n",
    "eval_tp_Claude_o3 = []\n",
    "checker_problems = checker_problems_Claude\n",
    "judge_model = 'o3'\n",
    "for i in tqdm(range(0, len(df_test))):\n",
    "    eval_entry = checker_problems[i]\n",
    "    problems = eval_entry['problems']\n",
    "    if len(problems) == 0:\n",
    "        continue\n",
    "\n",
    "    file_id = None\n",
    "    for f in file_list.data: # if already uploaded, use the existing file\n",
    "        if f.filename == eval_entry['paper_id']+\".pdf\":\n",
    "            file_id = f.id\n",
    "            break\n",
    "    if file_id is None: # if not, upload it\n",
    "        paper_pdf_url = 'https://arxiv.org/pdf/' + eval_entry['paper_id']\n",
    "        paper_pdf_data = httpx.get(paper_pdf_url).content\n",
    "        paper_pdf_upload = GPT_client.files.create(\n",
    "            file=(eval_entry['paper_id']+\".pdf\", paper_pdf_data, \"application/pdf\"),\n",
    "            purpose=\"user_data\")\n",
    "        file_id = paper_pdf_upload.id\n",
    "\n",
    "    for j in range(0, len(problems)):\n",
    "        problem = problems[j]\n",
    "        GPT_tp_response = GPT_client.responses.create(\n",
    "            model=\"o3-2025-04-16\",\n",
    "            input=[{\"role\": \"user\", \"content\": [\n",
    "                {\"type\": \"input_file\", \"file_id\": file_id}, # put document first to enable prompt caching\n",
    "                {\"type\": \"input_text\", \"text\": prompt_judge_tp.format(\n",
    "                    problem=problem['Problem'],\n",
    "                    location=problem['Location'],\n",
    "                    explanation=problem['Explanation'])},\n",
    "            ]}],\n",
    "            reasoning={'summary': 'auto'},\n",
    "            tools=[], #temperature=0, seed=42\n",
    "            service_tier=\"flex\",\n",
    "        )\n",
    "        GPT_tp_response_text = GPT_tp_response.output[-1].content[0].text\n",
    "        GPT_tp_response_summary = [s.text for s in GPT_tp_response.output[0].summary] if GPT_tp_response.output[0].summary else []\n",
    "        token_usage = GPT_tp_response.usage\n",
    "\n",
    "        tp_entry = {'entry_id': eval_entry['entry_id'],\n",
    "                    'retraction_id': eval_entry['retraction_id'],\n",
    "                    'paper_id': eval_entry['paper_id'],\n",
    "                    'checker_model': eval_entry['checker_model'],\n",
    "                    'attempt_id': 0,\n",
    "                    'problem_id': j,\n",
    "                    'judge_model': judge_model,\n",
    "                    'judge_response': GPT_tp_response_text,\n",
    "                    'true_positive': re.search(r'[Yy]es', GPT_tp_response_text) is not None,\n",
    "                    'judge_think_summary': GPT_tp_response_summary,\n",
    "                    'token_usage': {'input': token_usage.input_tokens,\n",
    "                                    'thinking': token_usage.output_tokens_details.reasoning_tokens,\n",
    "                                    'output': token_usage.output_tokens - token_usage.output_tokens_details.reasoning_tokens}\n",
    "        }\n",
    "        eval_tp_Claude_o3.append(tp_entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('eval_tp_Claude_o3.json', 'w') as out:\n",
    "    json.dump(eval_tp_Claude_o3, out, indent=2, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Average precision by a single judge\n",
    "eval_tp_df = pd.DataFrame(eval_tp_Claude_o3)\n",
    "eval_tp_df_groups = eval_tp_df.groupby('entry_id').mean('true_positive')\n",
    "eval_tp_df_groups.shape[0], eval_tp_df_groups['true_positive'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine votes from both judges to get the final average precision\n",
    "tp1 = pd.DataFrame(json.loads(open('eval_tp_Claude_Gemini.json').read()))\n",
    "tp2 = pd.DataFrame(json.loads(open('eval_tp_Claude_o3.json').read()))\n",
    "tp = pd.merge(tp1, tp2, on=['entry_id', 'problem_id'], how='inner')[['entry_id', 'problem_id', 'true_positive_x', 'true_positive_y']]\n",
    "tp['true_positive'] = (tp['true_positive_x'] & tp['true_positive_y'])\n",
    "tp_groups = tp.groupby('entry_id').mean('true_positive')\n",
    "tp_groups.shape[0], tp_groups['true_positive'].mean(), tp['true_positive'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# An edge case: test paper 103 has more than 100 pages, which exceeds the capacity of OpenAI models, so we use LaTeX instead\n",
    "checker_problems = checker_problems_GeminiFlash\n",
    "judge_model = 'o3'\n",
    "for i in tqdm(range(103, 104)):\n",
    "    eval_entry = checker_problems[i]\n",
    "    problems = eval_entry['problems']\n",
    "    paper_id = eval_entry['paper_id']\n",
    "    if len(problems) == 0:\n",
    "        continue\n",
    "\n",
    "    latex = ''\n",
    "    for tex_file in pathlib.Path('arxiv_src/'+paper_id).rglob(\"*.tex\"):\n",
    "        latex += tex_file.read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "    if latex == '':\n",
    "        with open('arxiv_src/'+paper_id+'/'+paper_id, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "            latex = f.read()\n",
    "\n",
    "    for j in range(0, len(problems)):\n",
    "        problem = problems[j]\n",
    "        GPT_tp_response = GPT_client.responses.create(\n",
    "            model=\"o3-2025-04-16\",\n",
    "            input=[{\"role\": \"user\", \"content\": [\n",
    "                {\"type\": \"input_text\", \"text\": prompt_judge_tp.format(\n",
    "                    problem=problem['Problem'],\n",
    "                    location=problem['Location'],\n",
    "                    explanation=problem['Explanation'])},\n",
    "                {\"type\": \"input_text\", \"text\": latex},\n",
    "            ]}],\n",
    "            reasoning={'summary': 'auto'},\n",
    "            tools=[], #temperature=0, seed=42\n",
    "            service_tier=\"flex\",\n",
    "        )\n",
    "        GPT_tp_response_text = GPT_tp_response.output[-1].content[0].text\n",
    "        GPT_tp_response_summary = [s.text for s in GPT_tp_response.output[0].summary] if GPT_tp_response.output[0].summary else []\n",
    "        token_usage = GPT_tp_response.usage\n",
    "\n",
    "        tp_entry = {'entry_id': eval_entry['entry_id'],\n",
    "                    'retraction_id': eval_entry['retraction_id'],\n",
    "                    'paper_id': eval_entry['paper_id'],\n",
    "                    'checker_model': eval_entry['checker_model'],\n",
    "                    'attempt_id': 0,\n",
    "                    'problem_id': j,\n",
    "                    'judge_model': judge_model,\n",
    "                    'judge_response': GPT_tp_response_text,\n",
    "                    'true_positive': re.search(r'[Yy]es', GPT_tp_response_text) is not None,\n",
    "                    'judge_think_summary': GPT_tp_response_summary,\n",
    "                    'token_usage': {'input': token_usage.input_tokens,\n",
    "                                    'thinking': token_usage.output_tokens_details.reasoning_tokens,\n",
    "                                    'output': token_usage.output_tokens - token_usage.output_tokens_details.reasoning_tokens}\n",
    "        }\n",
    "        eval_tp_GeminiFlash_o3.append(tp_entry)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
