[
  {
    "entry_id": 0,
    "retraction_id": "2303.17613v10",
    "paper_id": "2303.17613v9",
    "retraction_comment": "The theoretical structure, in particular the existence of the Riemannian metric, was flawed and will be resubmitted after reconsideration",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "No Riemannian metric is defined, yet PGA and ‘distance’ / geodesic concepts are used as if G were a Riemannian manifold",
        "Location": "Sections 2.3–2.4 (geodesics and group mean) and Appendix A",
        "Explanation": "The paper relies on geodesic distance, gradient of squared distance, and PGA—which all presuppose a Riemannian metric—yet only a Cartan–Schouten (non‐metric) connection is introduced. Without a genuine metric, autoparallels need not minimize any distance, and ∇md²=−2 Logₘx (Eq. 6) no longer holds. This invalidates the core use of geodesic projections and variance minimization."
      },
      {
        "Problem": "Derivation of ∇ₘd²(m,x)=−2 Logₘx assumes a metric connection but uses the non-metric Cartan–Schouten connection",
        "Location": "Appendix B (Eq. B.47) and Section 2.4 (Eq. 6)",
        "Explanation": "The classical identity ∇ₘd²(m,x)=−2 Logₘx holds only for the Levi–Civita connection of a Riemannian metric. The paper instead uses the torsionful Cartan–Schouten connection, so the gradient of squared ‘distance’ derived in Appendix B is unjustified and the stationary‐value conditions for PGA are unsound."
      },
      {
        "Problem": "Stationarity conditions for projecting data onto a geodesic (Eqs. 12–15) do not guarantee a true distance‐minimizing projection",
        "Location": "Section 2.7 (Eqs. 12–15)",
        "Explanation": "Replacing the minimization over t and v by setting their gradients to zero yields necessary but not sufficient conditions. In a nonconvex, non-metric setting, these stationary points may be saddles or maxima, so the algorithm may not find or even characterize the best‐fitting geodesic."
      },
      {
        "Problem": "PGA modes depend on an arbitrary scaling (dilation) of the signature, so results are not intrinsic",
        "Location": "Section 2.7 (note after Eq. 13, referring to Appendix C) and Appendix C",
        "Explanation": "Appendix C shows that under the homogeneity δₗ, the gradients and hence the PGA solution v change nontrivially. Any rescaling of the path or signature alters the principal geodesic modes, rendering the climatic interpretations (e.g. early El Niño indicators) artifacts of an arbitrary choice of units or normalization."
      },
      {
        "Problem": "Including time as an extra coordinate in the signature unduly biases the analysis toward trivial temporal trends",
        "Location": "Section 3.1 (definition of x^(0)=time)",
        "Explanation": "By lifting the scalar ‘time’ into the same signature as the SST anomalies, the dominant variation in PCA/PGA will largely capture the progression of time rather than intrinsic climate dynamics. This conflation undermines the claim that extracted modes reflect genuine early signs of El Niño rather than mere temporal ordering."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 13203,
      "thinking": 1664,
      "output": 718
    }
  },
  {
    "entry_id": 1,
    "retraction_id": "2103.13332v3",
    "paper_id": "2103.13332v1",
    "retraction_comment": "The notion of stabilizing ordinal is not well-defined, i.e., Definition 14 is flawed. As a consequence the results presented in the paper are either incorrect or remain unproved",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Flawed definition of stabilizing ordinal ρ_f – recursion not well-founded in presence of non-positive occurrences",
        "Location": "Definition 14",
        "Explanation": "The clause-by-clause recursion defining ρ_f(w)(φ) presumes a well-founded ordering on T-embeddings, but formulas with K (safe-belief) embed their subformulas non-positively (through the implicit conditional in Kφ). Because Kφ simultaneously depends on Bφ and φ itself, the structural recursion on formula complexity can fail to assign a stabilizing ordinal to some perfectly grounded formulas. Without a guarantee that ρ_f(φ) is defined exactly for those formulas whose truth values stabilize, the key prefix/postfix properties collapse and the quasi-inductive fixed-point argument breaks down."
      },
      {
        "Problem": "Incomplete induction in Lemma 31 – cannot ensure excluded sentences stay excluded",
        "Location": "Lemma 31",
        "Explanation": "Lemma 31 aims to show that if ρ_g(φ) is undefined then φ never enters any stage of the iterated jump over the minimal evaluation g. The provided proof uses a secondary induction on formula complexity, but it overlooks how the liminf rule at limit ordinals interacts with the non-monotone clauses for ¬T and K. In particular, sentences involving ¬Tt or Kψ may re-enter at a limit stage despite having no stabilizing ordinal, invalidating the claimed prefix property of g and undermining the entire fixed-point construction."
      },
      {
        "Problem": "Frame supremum ξ_F may not exist as an ordinal",
        "Location": "Definition 15",
        "Explanation": "ξ_F is defined as the supremum of all stabilizing ordinals ρ_f(φ) ranging over every evaluation f ∈ Val_F and every sentence φ. But Val_F (the set of all evaluation functions) and Sent_{ℒ^k} (the set of all sentences) can be proper-class-sized in a semantically closed language with a full syntax theory. Taking the supremum over a proper class of ordinals may exceed ON and fail to be an ordinal, so the cap of the quasi-inductive iteration is not well-defined, rupturing the transfinite iteration central to the existence proof."
      },
      {
        "Problem": "Faulty impossibility proof for subjunctive semantics – case analysis ignores partial truth-assignments across worlds",
        "Location": "Lemma 12",
        "Explanation": "The proof that no fixed point exists for the counterfactual Kripke jump relies on a diagonal sentence λ := T⟨λ⟩  ⊥ and splits into two cases: either ¬∃v M,v⊩T⟨λ⟩ or its negation. However, it assumes that for a putative fixed f, either λ∉f(w) for all worlds or λ∈f(w) for all worlds, ignoring evaluation functions that make λ true at some accessible worlds and false at others. Since the semantics is world-relative, partial assignments of λ could satisfy f = C(f) without falling into the two considered extremes, so the argument fails to rule out all possible fixed points."
      },
      {
        "Problem": "Undefined similarity relation in K-teller example – closure operator ill-defined",
        "Location": "Example 29 (Step 2)",
        "Explanation": "To construct a ‘K-teller’ fixed point the paper defines Sim(Y) := { v∈W | ∃w∈W (w≈_w v) }, but the relation ‘≈_w’ is never introduced. The only ordering provided is ≤_w (a partial preorder), and no symmetric or equivalence relation ≈_w is defined. Without a clear definition of ≈_w, the operator F := Sim∘TC is ill-defined, so the purported construction of a world-specific fixed point for a self-referential K-sentence collapses."
      }
    ],
    "think_summary": [
      "**Identifying Errors Clearly**\n\nI think it's better to focus on the more obvious errors in the text. First, there's a mistake in the numbering of clauses (5) and (6) where T and ¬T misalign, which is minor. Then, the requirement to define when t is a code needs clarity since I(t,w) and I(w,t) are used ambiguously. The notation I(t,w) ∈ f(w) is incorrect because the argument order is swapped. However, it seems that some issues may be minor in nature. I’ll prioritize the more serious flaws instead.",
      "**Identifying Issues in Proofs**\n\nFor Entry 4, I see a problem in the impossibility proof for fixed points under subjunctive semantics (Lemma 12). It overlooks assignments where T⟨λ⟩ holds at some accessible world, which makes the case analysis incomplete. The proof splits based on whether ¬∃v M,v⊩T⟨λ⟩, but it fails to consider evaluation functions where T-teller λ is true only in certain accessible worlds. \n\nEntry 5 points out that the K-teller fixed point example (Example 29) uses an undefined similarity relation, leaving the closure operator ill-defined. The definition relies on a relation that isn't properly defined, which affects the demonstration of the K-teller fixed point. I’ll make sure to refine these issues thoroughly."
    ],
    "token_usage": {
      "input": 27133,
      "thinking": 3520,
      "output": 873
    }
  },
  {
    "entry_id": 2,
    "retraction_id": "1602.07129v2",
    "paper_id": "1602.07129v1",
    "retraction_comment": "this paper has been withdrawn due to minor error in the calculation of dielectric constant",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Contradictory interpretation of electron diffraction data undermines structural conclusion",
        "Location": "Page 3–4, Results and Discussion, electron diffraction (Fig. 2)",
        "Explanation": "The text first states that a superlattice peak at [½,½,½] is observed in the &#x3c;-101&#x3e; SAD pattern, then immediately claims its absence confirms disorder. This internal inconsistency casts doubt on whether the sample is truly disordered or partially ordered, invalidating the claim that STMO adopts a fully disordered Pm-3m structure."
      },
      {
        "Problem": "Insufficient evidence for complete B-site disorder from XRD alone",
        "Location": "Page 4, Rietveld analysis of XRD (Fig. 1)",
        "Explanation": "Rietveld refinements with Fm-3m and Pm-3m both fit equally well, and the absence of the (111) superlattice reflection in XRD is inconclusive given the similar scattering factors of Ti4+ and Mn4+. Without complementary neutron diffraction to distinguish B-site occupancies, the conclusion of total antisite disorder is unsound."
      },
      {
        "Problem": "Flawed lattice subtraction and circular heat-capacity fitting invalidate magnetic entropy results",
        "Location": "Page 5–6, Specific heat analysis (Eqs. (1)–(3), Debye fit, Fig. 4)",
        "Explanation": "The initial polynomial fit to Cp(T) between 20–40 K yields an unphysically low Smag=0.5 J/mol·K. The subsequent Debye fit is adjusted (θD=545 K) specifically to recover the theoretical entropy (11.5 J/mol·K), a circular approach. Without an independent lattice model, the extracted magnetic entropy and related conclusions about moment suppression or short-range order are unreliable."
      },
      {
        "Problem": "Unsupported claim of first-order magneto-structural transition",
        "Location": "Page 4, Magnetization hysteresis discussion (Fig. 3(a))",
        "Explanation": "The presence of a small thermal hysteresis in M–T under FCC/FCW is attributed to a first-order magneto-structural transition without any low-temperature structural measurements (e.g., variable-T XRD or neutron diffraction). Hysteresis in SQUID magnetometry alone can arise from instrumental or domain-wall effects, so the claim lacks direct structural evidence."
      },
      {
        "Problem": "Unsubstantiated magnetodielectric coupling",
        "Location": "Page 7–8, Dielectric permittivity vs. temperature (Fig. 5)",
        "Explanation": "Dielectric anomalies near 14 K are presented as evidence of magnetodielectric coupling, yet no dielectric measurements under applied magnetic field are reported. Possible Maxwell–Wagner effects, electrode polarization, or extrinsic inhomogeneities are not ruled out, leaving the coupling unproven."
      }
    ],
    "think_summary": [
      "**Analyzing cation ordering and methods**\n\nI see that the presence or absence of cation ordering is crucial here. The Rietveld refinement methods seem good, but they depend on seeing (111) superlattice absence, and since Ti and Mn scatter similarly, XRD might miss the order—suggesting neutron diffraction could be necessary. Using only x-ray and electron diffraction may miss weak ordering due to their sensitivity limits. \n\nThe entropy analysis looks questionable when isolating Cmag. The polynomial fit appears arbitrary, likely underestimating lattice effects and leading to an inaccurate Smag. Their approach seems circular when selecting models and parameters, and the field dependence claims about downward shifts might need a critical review for antiferromagnetic behavior.",
      "**Identifying structural issues**\n\nI'm analyzing the claim of magneto-structural coupling, starting with their interpretation of ZFC-FC irreversibility and hysteresis at low temperatures. It seems questionable since they lack structural data, making their assertion unsupported. \n\nFor the magnetodielectric coupling, they identify a dielectric anomaly around 14K but haven’t controlled for potential contributions affecting their results. Their correlation between magnetization and permittivity also lacks statistical backing. I'll prepare entries to address the top five problems, with details on locations and explanations to include in a structured format."
    ],
    "token_usage": {
      "input": 15508,
      "thinking": 1152,
      "output": 676
    }
  },
  {
    "entry_id": 3,
    "retraction_id": "2006.00175v4",
    "paper_id": "2006.00175v3",
    "retraction_comment": "In eq 38, misses a d^2 phi term, breaking down the results of the paper",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "The so-called “quantum symmetry’’ is just the Schwinger–Dyson identity, not a genuine new symmetry",
        "Location": "Section I.B (eqs. 21–24)",
        "Explanation": "The author treats any field variation whose action change can be written as ∫f(φ)dL/dψ (or similarly for dL/dφ) as a symmetry, deriving a conserved ‘current’ via the path integral. But these insertions are exactly the Schwinger–Dyson equations (i.e. ⟨δS/δψ⟩=0) which hold for any variation of integration variables, not a reflection of a true symmetry of the theory. Consequently, the purported Ward identities impose no new constraints beyond the usual equations of motion in correlators, so there is no new boson–fermion symmetry."
      },
      {
        "Problem": "Failure to account for non-trivial Jacobian under derivative-dependent transformations",
        "Location": "Section I (pp. 5–6), derivation of eqs. (18–24)",
        "Explanation": "The transformations δψ = iσ^μ ε̄ ∂_μφ, etc., involve field derivatives. Such transformations do not have unit Jacobian in the path integral measure unless one checks and compensates for the resulting functional determinant. The paper simply assumes measure invariance, invalidating the derivation of any exact Ward identity or conserved current."
      },
      {
        "Problem": "Incorrect handling of δ(∂φ) and δ(∂ψ) terms in verifying invariance",
        "Location": "Section II (eqs. 37–41) and Section IV (eqs. 58–62)",
        "Explanation": "When computing δL under φ→φ+εψ and ψ→ψ+iσ^μ ε̄ ∂_μφ, the author drops or mis-arranges contributions coming from variations of the derivative terms (e.g. δ∂_μφ, δ∂_μψ). The claimed rearrangement into f dL/dψ + total derivatives only holds if δψ contains no derivatives, in contradiction with their own ansatz. As a result the claimed exact quantum invariance of the action is algebraically unsound."
      },
      {
        "Problem": "Use of non-holomorphic ‘superpotential’ W breaches gauge invariance without justification",
        "Location": "Sections II and IV (discussion around eqs. 42, 62)",
        "Explanation": "To maintain gauge invariance the function W(φ,φ*) must be a gauge singlet, yet the author allows arbitrary non-holomorphic functions and merely imposes 'dW/dφ T^a φ=0' by fiat. No construction of genuinely gauge-invariant non-holomorphic W is provided, rendering the gauge-theory examples internally inconsistent."
      },
      {
        "Problem": "Allowing unbounded or negative scalar potentials leads to unstable, non-unitary theories",
        "Location": "Sections II and VI (eqs. 43, 75)",
        "Explanation": "The new potential V=2|∂W/∂φ|^2−f(φ)−f(φ*) (and its gauge analog) can be made arbitrarily negative or unbounded below by choosing f. The paper does not address vacuum stability or unitarity in such pathological potentials, so the physical viability of the ‘new symmetry’ theories is fundamentally undermined."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 14878,
      "thinking": 1728,
      "output": 776
    }
  },
  {
    "entry_id": 4,
    "retraction_id": "2108.05829v5",
    "paper_id": "2108.05829v4",
    "retraction_comment": "There is a mistake in the proof. The second term of the last equation in Lemma 2.2 does not have the desired asymptotic behavior. I am grateful with [REDACTED-NAME] for pointing out this mistake",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Invalid deduction of nonnegativity in Lemma 2.2",
        "Location": "Lemma 2.2 (pages 3–5), construction of V",
        "Explanation": "The author claims ⟨v,∇_vV⟩=(1+o(1))‖v‖² uniformly in v and hence ≥0 on a small neighborhood. However, after patching the local fields V_p with a partition of unity, one picks up extra terms of order O(‖v‖·‖V_p‖)=o(1)·‖v‖, not o(1)·‖v‖². For small v these linear errors can dominate the quadratic term, so one cannot guarantee ⟨v,∇_vV⟩≥0 for all tangent vectors v as claimed."
      },
      {
        "Problem": "Unjustified coverage of W−V(U) by the σ-charts",
        "Location": "End of Lemma 2.2 (page 5)",
        "Explanation": "The proof that every point of W−V(U) lies in ⋃_{p∈H}σ(W_p−E) hinges on the claim that the σ-images of the blown-up charts cover a punctured neighborhood of x₀ in M. But σ is only known to be proper and an isomorphism off E; without an explicit argument one cannot exclude the existence of points in W−V(U) whose preimages lie near other components of E or accumulate outside the chosen charts."
      },
      {
        "Problem": "Flawed factorization in Lemma 2.1",
        "Location": "Lemma 2.1 (page 2), deduction that σ⁻¹(mx₀) and σ⁻¹(I_U) are each monomial from σ⁻¹(I)=σ⁻¹(mx₀·I_U)",
        "Explanation": "The author invokes Hironaka to monomialize the product ideal I=mx₀·I_U and then asserts that each factor inverse image must itself be monomial because their product is. In general, the product of two ideals can become monomial under blow-up without each factor separately becoming monomial; one needs invertibility or Cartier assumptions on both factors to deduce their monomiality."
      },
      {
        "Problem": "Non-quantified o(1) terms undermining uniform estimates",
        "Location": "Lemma 2.2 (page 4), asymptotic estimates “(1+o(1))‖v‖²”",
        "Explanation": "Throughout Lemma 2.2 the author writes relations like ⟨v,∇_vV⟩=(1+o(1))‖v‖² as x→x₀ without specifying the rate or uniformity in v. Such uncontrolled o(1) terms cannot be turned into a strict inequality over a punctured neighborhood without a precise smallness estimate, especially when combining multiple local pieces."
      },
      {
        "Problem": "Inconsistent use of energy sign in the final contradiction",
        "Location": "Proof of the conjecture (page 5)",
        "Explanation": "The region W∩{U≤E} is used to bound the kinetic energy Q(γ˙)=E+U(γ). For E<U(x₀)=0 this implies E+U(γ)≤2E<0, yielding no real solutions. The author ignores this sign inconsistency, effectively assuming the existence of motions in an empty region."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 5559,
      "thinking": 8256,
      "output": 803
    }
  },
  {
    "entry_id": 5,
    "retraction_id": "2403.19848v2",
    "paper_id": "2403.19848v1",
    "retraction_comment": "We're withdrawing our paper from arXiv due to a critical error in our review methodology, which excluded key studies on sustainable road freight transport. This oversight could mislead the scientific community. We plan to correct this, ensuring comprehensive study inclusion, and will resubmit our paper for a more accurate review",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Methodology lacks transparency and rigor",
        "Location": "Section 2 (Methodology)",
        "Explanation": "The paper claims a systematic review but provides no details on search databases, keywords, inclusion/exclusion criteria, or screening process. Without a reproducible protocol (e.g., PRISMA flow), selection bias is likely and the representativeness of included studies cannot be verified, undermining the validity of any conclusions drawn."
      },
      {
        "Problem": "Duplicate entries in literature table",
        "Location": "Table 1 (pages 4–6)",
        "Explanation": "The study by Holguín-Veras et al. (2016) is listed twice under different rows, indicating data handling errors. Such duplication inflates the apparent coverage of the review and suggests poor quality control in assembling the evidence base."
      },
      {
        "Problem": "Inconsistent classification of sustainability dimensions",
        "Location": "Table 1 and Sections 3–4",
        "Explanation": "Several studies are marked under the wrong sustainability pillars (e.g., emissions-focused papers omitted from the environmental column), and the criteria for assigning economic, environmental, or social labels are not defined. This inconsistency questions the reliability of the synthesis and any dimension-specific conclusions."
      },
      {
        "Problem": "Overgeneralized conclusions unsupported by data",
        "Location": "Section 4 (Conclusion)",
        "Explanation": "The authors assert global trends and policy implications based solely on a descriptive tally of heterogeneous studies, without quantitative synthesis or critical appraisal of study quality. Such broad generalizations exceed what the presented narrative review can substantiate."
      },
      {
        "Problem": "Omission of key seminal work from review despite citation",
        "Location": "References vs. Table 1",
        "Explanation": "Emrah Demir et al. (2014) is cited in the references but not included in Table 1's analysis, despite its recognized importance in green freight transport research. Excluding foundational studies compromises the completeness and credibility of the review."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 11449,
      "thinking": 512,
      "output": 465
    }
  },
  {
    "entry_id": 6,
    "retraction_id": "2210.14117v3",
    "paper_id": "2210.14117v2",
    "retraction_comment": "Error in formulation of Bronold-Fehske model. The plots shown are actually in terms of E' + chi, shifted incorrectly by a factor of the electron affinity. The apparent perfect reflection region is therefore nonphysical",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Artificially inflated and spatially varying collision frequency lacks physical justification and dominates sheath behavior",
        "Location": "Section II.C (Eq. 15)",
        "Explanation": "The collision frequency ν_sr(x) is arbitrarily boosted in the presheath and tapered off in the sheath via a sigmoid (Eq. 15) to force Maxwellianization. This ad hoc profile critically alters the electron distribution—causing non-physical cooling and a continuously rising emission gain—undermining the validity of the simulated sheath structures and their classification as classical or SCL."
      },
      {
        "Problem": "No grid-convergence demonstrated for full 1X2V simulations",
        "Location": "Section II.B (simulation setup) and Fig. 4",
        "Explanation": "Convergence tests in Fig. 4 are limited to 1X1V cases, yet all central results use 1X2V simulations. Without demonstrating spatial and velocity convergence in the angularly resolved (2V) setting, key features such as the angular integration of R(E,μ) and the resulting sheath profiles may be under-resolved or spurious."
      },
      {
        "Problem": "Initial condition does not satisfy full kinetic model",
        "Location": "Section II.B (Eqs. 11–13)",
        "Explanation": "The plasma is initialized via the simplified Robertson ODE sheath solution (Eqs. 11–13) which omits both collisions and wall emission. Since the subsequent VM-Fokker–Planck system includes these effects, the initial state is not a steady solution, leading to lengthy transients that can be misinterpreted as physical steady-state sheath structures."
      },
      {
        "Problem": "Universal application of wall-roughness parameter C=2 to different materials without validation",
        "Location": "Section II.A (Eq. 7)",
        "Explanation": "The fitting parameter C=2 in the Bronold–Fehske transmission model is calibrated to MgO but is assumed to apply equally to boron nitride. Given the strong sensitivity of reflection probability R(E,μ) to C, this unvalidated transfer can significantly misestimate emission gain γ and lead to incorrect predictions of sheath regimes."
      },
      {
        "Problem": "No true steady state achieved; emission gain γ drifts over time",
        "Location": "Section III (discussion around Fig. 9)",
        "Explanation": "Due to collision-driven cooling and continuous trapping of low-energy electrons, γ increases monotonically toward unity and never stabilizes. Consequently, the identification of a space-charge-limited sheath based on a transient γ>γ_c may not represent a persistent equilibrium state, calling into question the robustness of the sheath classifications."
      }
    ],
    "think_summary": [
      "**Identifying critical errors**\n\nThe user wants me to review an attached paper and identify up to five major critical errors that could undermine the conclusions. They'd like this presented as a JSON object with specific fields. I'm starting by examining the methodology, particularly looking for soundness issues. \n\nOne potential problem I've found is related to the use of artificially inflated collision frequencies. The method described appears arbitrary and may lead to non-steady state conditions which could significantly affect the results on sheath predictions and distributions.",
      "**Identifying methodology flaws**\n\nI’m examining several critical errors that may undermine the paper's conclusions regarding sheath physics. \n\nFirst, the use of artificially enhanced collision models might lead to unphysical outcomes. Additionally, the omission of rediffusing electrons overlooks a potentially significant effect, especially for dielectrics.\n\nAssuming universal reflection parameters can yield inaccurate results, while neglecting ion-induced secondary emissions could alter gamma values crucial for sheath types.\n\nLastly, restricting the simulation to a one-dimensional model disregards essential geometric effects and may not accurately reflect real sheath behavior.",
      "**Analyzing initialization and errors**\n\nI'm evaluating critical errors in the paper regarding electricity sheath theories. They begin with Robertson approximation profiles, but these assumptions might lead to non-steady states during simulations. The lack of magnetic fields in their conclusions could undermine applicability to magnetized systems. Important issues include the collision modeling influencing physics, neglecting ion-induced emissions, and using constant reflection parameters without justification.\n\nAdditionally, the modeling choices, like source injection profiles, could affect accuracy, highlighting the need for a careful evaluation of validity and convergence in their simulations.",
      "**Assessing critical issues**\n\nI'm analyzing the critical issues in the paper, particularly focusing on potential misinterpretations and inaccuracies. The artificial inflation of collision frequencies may lead to unphysical outcomes, and using a fixed wall roughness parameter like C=2 lacks necessary validation for different materials.\n\nNoteworthy omissions include the neglect of grid convergence in 1X2V simulations, which could undermine results. The initialization from approximated ODE solutions also overlooks key physical aspects, potentially invalidating initial conditions. Finally, the non-steady-state results may affect conclusions on SCL formation. Let's prepare the JSON entries with these insights."
    ],
    "token_usage": {
      "input": 17237,
      "thinking": 2432,
      "output": 614
    }
  },
  {
    "entry_id": 7,
    "retraction_id": "1902.09447v2",
    "paper_id": "1902.09447v1",
    "retraction_comment": "We have to change the simulations section since the authors of the RANA method do not agree that we did fair comparisons with their method",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Biased stochastic gradient estimate",
        "Location": "Section III, Eq. (10) and Algorithm 1 (lines 7–9)",
        "Explanation": "The block‐stochastic gradient d_{Γ(t)} is formed by summing only the terms over the random minibatch without any normalization. Hence E[d_{Γ(t)}]≠∇h, but rather (Q/(N R))·∇h, so the update direction is systematically scaled by Q and no longer unbiased. This violates the standard assumptions for convergence of stochastic gradient methods and undermines Theorem 1."
      },
      {
        "Problem": "Convergence theory applies only to L=1 yet experiments use L>1",
        "Location": "Theorem 1 (Section III) and throughout numerical experiments",
        "Explanation": "Theorem 1—and its proof in Appendix A—assumes L=1. No analogue is provided for the practically important case L>1, which is the regime used in most experiments. Therefore there is no theoretical guarantee that the algorithm converges (even locally) when multiple delay steps are used."
      },
      {
        "Problem": "Unjustified Lipschitz‐gradient bound",
        "Location": "Appendix B, Lemma 1, item 3 (inequality (28))",
        "Explanation": "The proof claims a uniform Lipschitz constant U for the Wirtinger gradient over the search set J, but the intermediate bounds introduce constants r_{k,p}, s_{k,p} that depend on z and μ. The paper never shows these can be bounded uniformly in μ (and hence over iterations), so the claimed global Lipschitz property—and hence the convergence argument—lacks justification."
      },
      {
        "Problem": "Initialization lacks basin‐of‐attraction guarantee",
        "Location": "Section IV (Algorithms 2 and 3)",
        "Explanation": "The proposed spectral initialization via alternating estimation of G_p and x_p has no theoretical bound on dist(x^{(0)},x). Yet Theorem 1’s local convergence requires the initial point to lie within a small neighborhood of the truth. Without any guarantee that the initialization achieves this, the overall recovery claim is unsupported."
      },
      {
        "Problem": "Annealing μ does not imply stationarity of original non‐smooth objective",
        "Location": "Algorithm 1 (lines 13–16) and Theorem 1",
        "Explanation": "Theorem 1 shows μ^{(t)}→0 and ∥∇h(x^{(t)},μ^{(t)})∥→0 for the smoothed cost. However, it does not establish that any limit point is a Clarke‐stationary point of the original non‐smooth amplitude‐based loss (μ=0). Without an outer‐semicontinuity argument or analogous result, convergence to a critical point of the true objective is not guaranteed."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 20483,
      "thinking": 2048,
      "output": 638
    }
  },
  {
    "entry_id": 8,
    "retraction_id": "1610.03889v2",
    "paper_id": "1610.03889v1",
    "retraction_comment": "There is a serious gap about the tangent space of the dimension 2 foliations induced by a linear pull-back. I do not know if the scheme is reduced and so it is possible that the dimension of the tangent space of this scheme is bigger than his topological dimension",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "First-order tangent space identification does not guarantee actual deformation equivalence or unobstructedness",
        "Location": "Section 2 (end) and Section 4 (proof of Theorem 1)",
        "Explanation": "The paper shows that T_Π Pois(P^n)=T_F Fol(P^n) at the given Poisson structure Π, but it never addresses higher-order obstructions. Identifying first‐order tangent spaces does not imply that the analytic (or formal) deformation functors coincide or that Π is a smooth (regular) point of the Poisson moduli. Without a vanishing of obstruction groups or a formal smoothness argument, one cannot conclude that every foliation deformation lifts to a Poisson deformation or that the irreducible component is smooth at Π."
      },
      {
        "Problem": "Misapplication of Poincaré linearization to global Zariski density",
        "Location": "Section 4, Theorem 4.1 and proof of Theorem 1",
        "Explanation": "Theorem 4.1 asserts that the integral curves of the local coordinate fields ∂/∂y_i in the Poincaré chart are Zariski dense in P^{n−1}. Those ∂/∂y_i are local linear models for Y and do not define global algebraic flows. The argument conflates these local flows with leaves of the global foliation by Y, and no global continuation or density result for actual Y–leaves is established. Hence the crucial claim that vanishing along a single dense leaf implies vanishing everywhere is unsupported."
      },
      {
        "Problem": "Invalid use of the identity principle on non-global analytic curves",
        "Location": "Lemma 4.2 (page 6) and its use in proof of Theorem 1",
        "Explanation": "Lemma 4.2 extends vanishing of an algebraic section from a Euclidean open in an analytic curve C to all of C by patching local tubular neighborhoods. But it presumes C is a globally immersed complex analytic submanifold of P^{n−1} without singularities or monodromy, which is not justified for leaves of an algebraic foliation. Moreover, the subsequent proof applies this lemma to a local coordinate axis (the ∂/∂y_1–curve) that does not extend globally, so the analytic continuation argument breaks down."
      },
      {
        "Problem": "Oversimplified description of foliation deformations via bivector fields",
        "Location": "Section 2, definition of T_F Fol(P^n) (page 3)",
        "Explanation": "The paper identifies the tangent space to the space of dimension-2 foliations Fol(P^n) at F with the subspace of bivector fields ξ satisfying [Π,ξ]=0 and Π∧ξ=0. However, the deformation theory of codimension-(n−2) foliations is governed by H^1 of the foliation tangent sheaf or ext groups of the conormal sequence, and involves quotienting by automorphisms. The manuscript does not justify the equivalence of these descriptions, nor does it account for possible reparameterizations of leaves or cohomological obstructions to foliation deformations in this bivector framework."
      },
      {
        "Problem": "Unverified applicability of the stability theorem for split tangent sheaf foliations",
        "Location": "Section 4 (end of proof of Theorem 1)",
        "Explanation": "The final step invokes [3] to assert that any small deformation F_ε of the foliation F with split tangent T_F=O(−1)⊕O(1) remains split. The cited stability theorem requires certain positivity or slope stability hypotheses on the splitting factors and control of singular set dimensions. The paper does not verify that its specific splitting (degrees −1 and +1) satisfies all hypotheses of [3], nor does it check that singularities of the deformed foliation behave as needed, so the application of the stability result is not fully justified."
      }
    ],
    "think_summary": [
      "**Evaluating the paper's soundness**\n\nI'm looking into potential critical errors in the paper regarding the irreducible component of Pois(P^n) with rank 2 Poisson structures. They aim to prove stability under certain deformations in P^n for n greater than or equal to 4. It seems they have a confusing definition of tangent spaces. They're using bivectors that might not align with the standard definitions of tangent spaces for foliations, which suggests there could be issues in their identification of these concepts. I need to consider this further.",
      "**Analyzing paper assumptions**\n\nI'm evaluating the assumptions made in the paper about Poisson tangent spaces. They relate TΠPois to the foliation tangent, but it seems they only show that TΠPois is a subset of that space rather than an equality. They assume first-order Poisson deformations are always foliation deformations, yet they don't account for deformations that could alter projection coordinates. Their arguments about tangential equality and stability need stronger justification, particularly in controlling higher-order deformations. It appears they missed a more comprehensive obstruction analysis and justification of regular points.",
      "**Examining leaf density assumptions**\n\nI'm delving into the paper's assumptions about the closure of a leaf of Y being part of P^{n-1}. There's a need for a proper identity principle regarding polynomials vanishing on the leaf. They claim it's Zariski dense, but don’t provide solid justification. The paper suggests using Poincaré charts to argue about vanishing conditions, yet the arguments rely on local rather than global properties. There’s inconsistency when they shift from using a vector field Y to a coordinate field ∂/∂y1, which doesn't align with their earlier claims. It seems they’re mixing local and global assumptions about the behavior of their vector fields."
    ],
    "token_usage": {
      "input": 5693,
      "thinking": 3584,
      "output": 892
    }
  },
  {
    "entry_id": 9,
    "retraction_id": "1208.1540v2",
    "paper_id": "1208.1540v1",
    "retraction_comment": "This paper has been withdrawn because there is a gap in the construction of the canonical quadratic refinement on a mapping torus",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Unjustified vanishing of spin bordism groups",
        "Location": "Appendix A.2, Lemma A.1",
        "Explanation": "The claim that Ωⁱᵖspin⁾_{4ℓ+3}(pt)=0 for ℓ=0,2 or odd relies on Theorem 2.2 of [25] and a parity argument about shifts of KO-generators, but no explicit calculation of πₙ(MSpin) in these degrees is given. If these groups do not in fact vanish, one cannot guarantee that every 4ℓ+3–dimensional spin manifold bounds, and the subsequent construction of relative Wu‐class lifts fails."
      },
      {
        "Problem": "Misapplication of the Freudenthal suspension theorem",
        "Location": "Appendix A.3, paragraph following equation (A.3)",
        "Explanation": "The map Σ⁸K(ℤ,2ℓ+2)→Σ^{2ℓ+2}HZ is asserted to be a (4ℓ+4)–equivalence, yet the necessary connectivity bounds (that Σ⁸K is (2ℓ+1)–connected so that Freudenthal gives isomorphism up to degree 2·(2ℓ+1)) are not checked. Without verifying these connectivity hypotheses, the vanishing of the generalized spin bordism group Ω̃_{4ℓ+3}^{spin}(K(ℤ,2ℓ+2)) is unjustified."
      },
      {
        "Problem": "Use of non-compact gluing to compare pairings",
        "Location": "Section 5.4, Lemma 5.6, proof",
        "Explanation": "The proof constructs a non-compact manifold T by gluing tubular neighborhoods and computes an integral pairing over T, assuming it is an integer. However, on non-compact manifolds Stokes’ theorem and integrality of cohomological pairings require careful control of decay or compact support, which is not provided. This gap undermines the claimed equality of pairings modulo 1."
      },
      {
        "Problem": "Unsubstantiated vanishing of the Wu class on W₃",
        "Location": "Section 5.1, proof of Proposition 5.4, after equation (5.4)",
        "Explanation": "It is asserted that the Wu class ν_{2ℓ+2}(W₃)=0 for W₃=M×P because Wu classes of factors vanish. But the Cartan formula for Wu classes over ℤ₂ only shows ν(W₃)=ν(M)∪ν(P); one must check all Stiefel–Whitney classes of P up to degree 2ℓ+2. No argument is given, so dropping the λ–term in (5.5) is unwarranted, threatening the quadratic refinement property."
      },
      {
        "Problem": "Non-canonical extension of torsion corrections",
        "Location": "Section 4.3, end of paragraph after equation (4.6) and Appendix A",
        "Explanation": "The construction of the canonical relative lift λ^c assumes that the torsion classes u₁,u₂ on M_φ extend to W so as to kill the characters on boundary cohomology. Appendix A only treats extension of arbitrary cohomology classes from a boundary to some W, not these specific torsion classes. Without a proof that u₁,u₂ extend canonically to the same W used in defining λ^c, the canonicity and even existence of λ^c (and hence Q^c) remain in doubt."
      }
    ],
    "think_summary": [
      "**Preparing output for user**\n\nThe user has specified they want up to 5 entries, so I need to ensure that our output is concise and meets that requirement. I'll format this as a valid JSON array containing those entries. It’s important for me to double-check that everything adheres to JSON standards, keeping it clear and organized for the user. Let’s get this ready!"
    ],
    "token_usage": {
      "input": 28207,
      "thinking": 2496,
      "output": 809
    }
  },
  {
    "entry_id": 10,
    "retraction_id": "1708.09822v3",
    "paper_id": "1708.09822v2",
    "retraction_comment": "incorrect example (Example 5)",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Incorrect definition of b0 in the proof of Theorem 4.5",
        "Location": "Section 4.1, proof of Theorem 4.5 (page 12–13)",
        "Explanation": "In the case b=0 the splitting field of x^3−c is L=Q(c^{1/3},ζ_3), but the authors set b0=√3·c and claim L=Q(b0,ζ_3).  This confuses the cube root c^{1/3} with the square root √3·c.  As stated, b0∉L in general, so the constructed nilpotent element does not lie in H_{λ,3}, invalidating the 'if' direction of Theorem 4.5."
      },
      {
        "Problem": "Unjustified case analysis in Lemma 4.4",
        "Location": "Section 4.1, proof of Lemma 4.4 (page 11)",
        "Explanation": "The proof splits into Case I with b0∈Q and Case II with b0∉Q, but L^{⟨τ⟩} is a degree-3 extension of Q, so aside from the trivial b0=0 the choice b0∈Q is vacuous.  The argument in Case I(ii) also treats a_1∈Q(α)\\Q but then concludes 3w^2d=0 without ruling out w=0 properly.  This undermines the necessity of the condition −bD∈Q^{×2}."
      },
      {
        "Problem": "Multiplication table in Proposition 4.8 lacks justification",
        "Location": "Section 4.2, Proposition 4.8 (page 16)",
        "Explanation": "The authors display a 4×4 multiplication table for the basis {1,X,Y,Z} of the component R in H_{λ,4} without derivation.  A direct computation in the group algebra shows that (σ−σ^3)^2=2σ^2−2 rather than −4, so the entries X^2 and similarly X·Y, X·Z etc. are incorrect.  This invalidates the criterion in Theorem 4.9 for when H_{λ,4}≅Mat_2(Q). "
      }
    ],
    "think_summary": [
      "**Identifying a critical error**\n\nI’m observing a significant mistake involving the confusion of cube roots and square roots. The correct third root should be expressed as \\( c^{1/3} \\), but it’s incorrectly stated as \\( \\sqrt{3} \\cdot c \\). Upon reviewing, I see that the splitting field for the cubic \\( x^3 - c \\) is \\( Q(c^{1/3}, \\zeta_3) \\), not \\( Q(\\sqrt{3} \\cdot c) \\). This misdefinition of \\( b_0 \\) invalidates part of Theorem 4.5 and points toward a serious misapplication of Galois theory.",
      "**Analyzing potential issues in theorems**\n\nI’m reviewing Theorem 4.9 for \\( D_4 \\) about nilpotent conditions, which might be correct. However, I notice they might overlook potential Hopf-Galois structures beyond the stated ones. The dihedral group \\( D_n \\) isn't simple, despite classifying it as such. In their work, results focus on classical and canonical structures without acknowledging others.\n\nThere’s also a concern with Proposition 4.8's multiplication table, where there's confusion over the calculation. Although they claim \\( X^2 = -4\\alpha^2 \\), my calculations suggest otherwise. Overall, the real critical error seems to stem from Theorem 4.5 and the misdefinition of \\( b_0 \\).",
      "**Summarizing key issues**\n\nThe user asked for up to five entries, and I'll provide three key issues. The first is the invalid definition of \\( b_0 \\) in Theorem 4.5 for the \\( b=0 \\) case. The second involves Lemma 4.4, where irreducibility and fields are misapplied. The third concerns Proposition 4.8, which contains an unsubstantiated multiplication table. I'll compile these findings in a JSON format for clarity and ease of reference while ensuring they align with the user's request."
    ],
    "token_usage": {
      "input": 20646,
      "thinking": 4544,
      "output": 533
    }
  },
  {
    "entry_id": 11,
    "retraction_id": "2011.04373v3",
    "paper_id": "2011.04373v2",
    "retraction_comment": "paper withdrawn since dimension reduction might not hold in the parabolic setting",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Unnatural and overly strong additional assumption (H5)",
        "Location": "Section 1.2, equation (1.2)",
        "Explanation": "Hypothesis (H5) requires the solution u to lie in L¹(0,ρ; L^∞_t; L²(S_rⁿ⁻¹)) for all radii r and to satisfy the radial integral inequality (1.2).  Neither standard existence theories for weak solutions nor the structural hypotheses (H1)–(H4) guarantee such spherical trace bounds or the integrability in r, so (H5) is ad hoc.  It effectively restricts the result to a special class of highly regular solutions and is neither derived nor verified for the usual weak‐solution spaces."
      },
      {
        "Problem": "Lack of existence/verification that (H5) holds for any nontrivial solution",
        "Location": "Discussion following Remark 1.1 and in the proof relying on (1.2)",
        "Explanation": "Nowhere do the authors prove that a weak solution of the double‐phase parabolic problem actually satisfies the extra integrability and spherical‐trace condition (H5).  Without an approximation or density argument to recover (1.2) from e.g. W^{1,p}–theory, the main theorem applies only if one assumes the conclusion as an additional hypothesis, voiding its generality."
      },
      {
        "Problem": "Application of the spherical Sobolev embedding (Lemma 2.4) without valid trace theory",
        "Location": "Section 3.1, derivation of (3.11)",
        "Explanation": "The use of Lemma 2.4 on level‐sets {u>k} requires that (u−k)+ restricted to almost every sphere S_r belongs to W^{1,q*}(S_r).  But standard interior regularity only gives ∇u∈L^p in the ball, which yields traces in W^{1−1/p,p}(S_r), not W^{1,q*}.  The paper invokes (H5) to control the zeroth‐order term but gives no argument that the tangential gradient ∇_T(u−k) also lies in L^{q*}(S_r) in r.  This is a serious gap in justifying the key estimate (3.11)."
      },
      {
        "Problem": "Unjustified uniform bound M_i ≤ C₃ in (4.7)",
        "Location": "Section 4, equation (4.7)",
        "Explanation": "The authors assert “it is easy to see” from (1.3) that M_i ≤ C₃ := Γ/σ^{(N+1)(1+κ)} for all i.  However, the definition M_i=Y_i+Y_iZ_i^κ+Z_i^{1+κ} involves integrals over the shrinking cylinders Q_i and sup‐norms on shrinking balls B_{ρ_i}.  No clear scaling or measure‐ratio argument is given to relate these directly back to the original quantity Γ, nor is the factor σ^{−(N+1)(1+κ)} derived.  Without this, the subsequent application of the fast‐geometric‐decay lemma (2.6) fails."
      },
      {
        "Problem": "Overall circularity and lack of verification in the iterative scheme constants",
        "Location": "Sections 3–4, in the definitions of A_k, τ̃, b₁ and the final choice of k",
        "Explanation": "Throughout Sections 3 and 4 the authors introduce numerous exponents (β,γ,κ), constants (A_k, Ã, b, b₁, τ̃), and require delicate inequalities among them.  These are patched together by ‘re-defining’ A_k and absorbing universal constants, but no uniform bookkeeping is provided.  In particular, it is never shown that one can choose k large enough simultaneously to satisfy all exponent inequalities and the requirement Y₀ ≤ C^{−1/α} b^{−1/α²}.  This lack of a consistent choice makes the final boundedness conclusion formally unsubstantiated."
      }
    ],
    "think_summary": [
      "**Checking critical errors**\n\nI need to review the attached paper carefully for critical errors that could invalidate its conclusions. I’m looking to identify up to five major issues, such as methodological flaws, inconsistencies in assumptions, logical gaps, or misuse of lemmas. One part that stands out is hypothesis H5's condition (1.2), which might be an unphysical assumption. There could be significant problems with Sobolev embedding on the sphere, especially regarding the functions being considered. I’ll have to ensure that all definitions are appropriately applied.",
      "**Questioning assumptions in H5**\n\nActually, they defined \\( u \\) in the interior, but assume for H5 that \\( u \\) is in \\( L^1(0,r; L^\\infty(-\\theta,\\theta; L^2(S_{N-1})) \\) along with inequality (1.2). This seems like an unnatural and overly strong requirement; justifying this condition feels necessary as it relates averages on spheres to averages on a ball. It could invalidate generality, limiting the theorem's applicability only to specific weak solutions. The requirement for pointwise traces on spheres could also imply more regularity than just \\( W^{1,p} \\) provides.",
      "**Identifying key issues**\n\nI’m analyzing the complexities related to the statement and proof, especially concerning \\( M_i \\) and the scaling factors. It appears there's some ambiguity around defining certain elements and deriving inequalities, particularly with reference to conditions like H5. There are noteworthy issues to address, such as the unnatural assumptions in H5, the existence of solutions, and unclear justifications in certain equations. I'll prioritize these problems, summarizing them in a structured format for clarity. This will help to detail the points precisely, as they pertain to various sections in the document."
    ],
    "token_usage": {
      "input": 10096,
      "thinking": 3584,
      "output": 925
    }
  },
  {
    "entry_id": 12,
    "retraction_id": "2401.02488v3",
    "paper_id": "2401.02488v2",
    "retraction_comment": "Lemma 3.4 on page 7 is incorrect. This is crucial to the argument. The problem that could not be fixed is if there are parts of hilden subgroup elements that contain parts of powers of the garside element",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Unjustified claim that the Garside element ∆ lies in the Hilden subgroup Hₙ",
        "Location": "Lemma 3.1 (page 5–6)",
        "Explanation": "The proof of Lemma 3.1 and the entire reduction rests on the assertion that ∆∈Hₙ.  However, no general algebraic argument is given, only a few low-strand pictures in B₄.  In fact, it is nontrivial (and likely false in general) that the half-twist ∆ acts trivially on plat closures.  If ∆∉Hₙ, then one cannot remove powers of ∆ from any β by Hilden moves, invalidating the key step that every double-coset class contains a positive braid."
      },
      {
        "Problem": "Ambiguous and inconsistent definition of the Hilden subgroup Hₙ",
        "Location": "Definition 2.2 (page 4) and subsequent usage",
        "Explanation": "Hₙ is first defined as depending on a particular braid β and its plat closure L, yet later treated as a fixed subgroup of B_{2n} with a known finite generating set.  The dependence on β (and hence on L) is never reconciled with the claim that Hₙ acts uniformly on all braids in B_{2n}, undermining statements about double cosets AβB with A,B∈Hₙ."
      },
      {
        "Problem": "Undecidability of lexicographic minimization over braid word representatives",
        "Location": "Definition 3.2 and Section 3.1 (page 6–7)",
        "Explanation": "The algorithm hinges on choosing, for each positive braid element δ, the lexicographically smallest positive word representing δ.  No effective method is given to determine this among infinitely many equivalent words, and lex order on braid words does not induce a computable total order on group elements without exhaustive (and infinite) enumeration of all presentations, so the proposed minimization step is not shown decidable."
      },
      {
        "Problem": "Implicit assumption that membership and enumeration in Hₙ are decidable",
        "Location": "Section 3.1 search for A,B∈Hₙ (page 7)",
        "Explanation": "The finite search over all A,B∈Hₙ of bounded length requires an effective enumeration of Hₙ and a membership test for arbitrary words in Hₙ.  The subgroup membership problem in braid groups is nontrivial and not addressed.  Without a procedure to list exactly the words in Hₙ up to a given length, the search cannot be carried out algorithmically."
      },
      {
        "Problem": "Conflation of braid groups Bₙ and B_{2n} throughout statements and proofs",
        "Location": "Theorem 2.5 (page 5) and everywhere else",
        "Explanation": "Plat closures and the Hilden subgroup live naturally in even-strand groups B_{2n}, yet the main theorems, lemmas, and decompositions are repeatedly stated in Bₙ (odd or arbitrary n).  This notational and conceptual confusion casts doubt on whether the Garside decompositions, subgroup actions, and algorithmic steps are being applied in the correct group context."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 17542,
      "thinking": 4672,
      "output": 743
    }
  },
  {
    "entry_id": 13,
    "retraction_id": "1208.2556v8",
    "paper_id": "1208.2556v7",
    "retraction_comment": "This paper has been withdrawn by the author because (k+1)n<k(n+2) is wrong",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Vague and ill-defined parameter z in Lemma 2.1",
        "Location": "Section 2, Lemma 2.1",
        "Explanation": "The proof introduces z as an unspecified natural number summarizing all additive contributions of the 3n+1 steps, but gives no precise definition or bounds. This makes the core equation 3^x m_j + z = 2^y m_j ambiguous and undermines all subsequent Diophantine manipulations."
      },
      {
        "Problem": "Unjustified assumption z1 = 2 z0 + n",
        "Location": "Section 3, immediately after deriving (2 z0 – z1) + k(3 z0 – 2 z1) = 0",
        "Explanation": "The proof splits into cases by positing z1 = 2 z0 ± n without any derivation or justification from earlier relations. This ad hoc parametrization is the sole basis for all later bounds on z0 and z1, so the argument collapses if that form does not exhaust all possibilities."
      },
      {
        "Problem": "Arbitrary and incomplete bounding of z0",
        "Location": "Section 3, final case analysis (choices z0 = 2n, 2n+2, 2n+1)",
        "Explanation": "The author tests only three discrete forms for z0 within an alleged narrow interval without proving that z0 must lie between 2n and 2n+2. There is no justification that z0 cannot take any other integer value, so infinitely many potential solutions are ignored."
      },
      {
        "Problem": "Unsupported deduction n > k and k | n from n = k(z0 – 2n)",
        "Location": "Section 3, equation (k+1)n = k(z0 – n) and subsequent paragraph",
        "Explanation": "From the relation n = k(z0 – 2n) the author asserts both n > k and that k divides n, yet offers no argument for z0 – 2n ≥ 1 or for positivity relations. These claims are essential to the following case splits but are themselves unproven."
      },
      {
        "Problem": "Conclusion 3^x + 1 = 2^y rests on unproven z0 = m0",
        "Location": "Section 3, last displayed equation before invoking Corollary 2.1",
        "Explanation": "The final reduction to the Catalan equation 3^x + 1 = 2^y requires z0 = 2k+1 = m0, but that identification is derived only via the flawed bounding and case analysis. Without a valid proof that z0 must equal m0, the application of Corollary 2.1 is unjustified."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 3394,
      "thinking": 5760,
      "output": 642
    }
  },
  {
    "entry_id": 14,
    "retraction_id": "1905.08897v3",
    "paper_id": "1905.08897v2",
    "retraction_comment": "Although the quasi-nodal surface has been identified in our preprint, there is a tiny band gap (millivolt-level) in the non high symmetry k-line (e.g.,K1-K5). In this regard, the proposed bct-C40 should be a nodal net semimetal based on the strict definition of topological semimetal. We would like to withdraw this work to avoid misleading the readers from Arxiv",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Misinterpretation of schematic nodal-net geometry as exact k-space straight lines",
        "Location": "Page 1, paragraph 2 (discussion of Fig. 3(c) in Ref. [1])",
        "Explanation": "The comment faults the original work for slight curvature of the 0–1 and 0–2 segments, treating the schematic boxed-asterisk as a literal straight-line trajectory. In topological nodal-net studies, such diagrams depict connectivity and topology rather than exact k-space straightness; small deviations do not negate the nodal-net topology."
      },
      {
        "Problem": "Conflation of discrete nodal points with a continuous 2D nodal surface",
        "Location": "Page 3, Fig. 2 and accompanying text",
        "Explanation": "The authors infer a full nodal surface solely by plotting isolated degenerate k-points on several slices. They do not demonstrate that band degeneracy persists continuously over a 2D manifold nor that it is symmetry-protected across the entire Brillouin zone, as required for a true nodal-surface semimetal."
      },
      {
        "Problem": "Insufficient k-point sampling and convergence testing",
        "Location": "Pages 2–3, methodology of band-structure reconstruction",
        "Explanation": "The identification of ‘wavy’ lines and putative nodal surfaces appears based on sparse interpolation of VASP outputs. Without systematic convergence tests or denser k-meshes, these features may be numerical artifacts rather than genuine band degeneracies spanning surfaces."
      },
      {
        "Problem": "Absence of symmetry analysis for Class-I nodal-surface classification",
        "Location": "Page 4, final paragraph (classification of nodal surfaces)",
        "Explanation": "The claim that bct-C40 hosts Class-I nodal surfaces relies on specific mirror or inversion symmetries, yet the comment does not analyze the space-group operations or verify the necessary symmetry eigenvalue arguments to substantiate symmetry-enforced surface degeneracy."
      },
      {
        "Problem": "Mischaracterization of internal Dirac point Dg as independent from the nodal net",
        "Location": "Page 3, paragraph starting “Additionally, the authors ignored…”",
        "Explanation": "The newly reported crossing Dg on the K1–K5 line may simply be part of the curved nodal-net connectivity when extended off high-symmetry paths. No topological invariant or connectivity mapping is provided to demonstrate that Dg represents a separate nodal feature invalidating the original net."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 4603,
      "thinking": 2112,
      "output": 564
    }
  },
  {
    "entry_id": 15,
    "retraction_id": "1712.07752v2",
    "paper_id": "1712.07752v1",
    "retraction_comment": "Based on the numerous reviews I have received, the figures illustrated in the paper are highly incorrect and vague. This might guide a novice reader towards a wrong direction and lead to improper understanding of the subject. Also, the paper covers a diverse range of topics but doesn't get into the details of any and hence the proposals remain pragmatically irrelevant",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Unrealistic economic superiority premise",
        "Location": "Page 5, Section “Need for an economically superior UIRB-AI”",
        "Explanation": "The paper claims the UN/UIRB-AI must become wealthier than any nation or MNC via an AI/robot tax to enforce global compliance. It provides no quantitative revenue estimates, ignores legal and jurisdictional hurdles in cross-border taxation, and overlooks state sovereignty—undermining the central feasibility of the proposed body."
      },
      {
        "Problem": "Flawed treaty analogy for AI governance",
        "Location": "Page 6, Section “Objectives of the UIRB-AI” (paragraph on NPT and IBC)",
        "Explanation": "The author analogizes AI to nuclear weapons and genetic technologies, proposing a similar treaty/committee model. Unlike scarce, physically controlled materials, AI is intangible, rapidly copyable, and diffuse, making such treaty-style enforcement ineffective and inapplicable to software governance."
      },
      {
        "Problem": "Correlation-causation fallacy in economic unity argument",
        "Location": "Page 4, Figure 2 and accompanying text",
        "Explanation": "Figure 2 purports that economic budget superiority ensures political cohesion of unions (e.g., League of Nations vs. EU vs. UN). This simplistic correlation ignores complex political, historical, and social factors, rendering the conclusion—that a superior budget guarantees unity—unsound."
      },
      {
        "Problem": "No enforcement mechanism for AI/robot taxation",
        "Location": "Page 5, Section “AI Tax and Robot Tax: Revenue generation for the UIRB-AI”",
        "Explanation": "While proposing an AI/robot tax, the paper fails to specify legal frameworks, collection agencies, or definitions to distinguish between human-augmenting and displacing AI. There is no plan for international agreement or compliance monitoring, invalidating the revenue model."
      },
      {
        "Problem": "Vague organizational design lacking authority",
        "Location": "Page 4, Section “Organizational structure of UIRB-AI”",
        "Explanation": "The structure describes parallel teams of scientists, ethicists, economists, and politicians without defining decision-making processes, enforcement powers, dispute resolution, or accountability. Such vagueness makes the proposed body incapable of coherent action or binding regulation."
      }
    ],
    "think_summary": [
      "**Evaluating AI governance assumptions**\n\nI find that the idea of making the UN “economically superior” through AI taxes is built on shaky premises. The analogy with nuclear weapons treaties falls flat due to AI's rapid spread and reproducibility, making control nearly impossible. The paper also treats AI as if it's a single entity, neglecting technical diversity and jurisdictional issues. The arguments for economic superiority in Figure 2 come from flawed correlation without clear units, misrepresenting union stability. Lastly, it lacks political feasibility, improperly addressing sovereignty and the complexities of agreement.",
      "**Analyzing faulty reasoning**\n\nI’m going to examine the paper for faulty reasoning, starting with the introduction, which urges the UN to oversee global AI development without solid evidence that it’s the right body. The “Need for UIRB-AI” section presents a security dilemma but assumes only the UN can address it. Countries can form their own agreements. The section on “World historical learnings” naively connects technology and empire lifespan.\n\nThe paper makes several flawed claims, like the lack of research in national AI partnerships. The suggested robot tax lacks a clear implementation mechanism. The Swiss multipolar governance model doesn't translate well to a global scale either. \n\nI’ll reframe my critiques for clarity. Entry 1 notes the unrealistic expectation that the UN can use AI taxes for global compliance. Entry 2 points out the flawed analogy between AI governance and nuclear non-proliferation, as AI is easily replicated. Entry 3 highlights a causation fallacy in the economic graphs regarding union stability."
    ],
    "token_usage": {
      "input": 10556,
      "thinking": 1792,
      "output": 535
    }
  },
  {
    "entry_id": 16,
    "retraction_id": "2110.09901v3",
    "paper_id": "2110.09901v2",
    "retraction_comment": "It relies on maximizing the distance over an intersection of balls to a given point. The used algorithm for this however, is not able to solve the class of problem the SSP generates",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Use of an idealized real‐arithmetic (BSS) model hides exponential bit–complexity",
        "Location": "Section 3.5 (Complexity) and throughout",
        "Explanation": "The algorithm assumes exact arithmetic over real numbers (e.g. solving LPs with irrational coefficients, manipulating √n/2 etc.) in polynomial time. In the standard Turing model, these operations incur unbounded bit–precision costs, and many subroutines (LP solving, bisection on R) become pseudopolynomial or worse. Without a careful bit–complexity analysis or restrictions to rationals of bounded encoding size, the claimed polynomial‐time FPTAS does not carry over to the usual discrete model."
      },
      {
        "Problem": "Unjustified assumption of a unique minimizer X★ρ",
        "Location": "Section 3.3, equation (18) and the paragraph immediately following (“We assume in this paper that X★ρ = {x★ρ}…”)  ",
        "Explanation": "The entire threshold‐finding and scaling argument hinges on X★ρ being a singleton. In general the convex problem min{hρ(x)−||x||² : hρ(x)≤1} may have multiple minimizers or a whole face of minimizers, especially in symmetric instances. No conditions are given to guarantee uniqueness, so the algorithm’s branching on the sign of hρ(x★ρ) and its subsequent bisection logic break down when argmin is not unique."
      },
      {
        "Problem": "Critical dependence on the minimizer lying strictly in the interior of P",
        "Location": "Section 3.4 (“We analyze the case hρ(x★ρ,C)<0… ∃B(x★ρ,C,ε)>0⊆P”) ",
        "Explanation": "The correctness and convergence of the scaling‐and‐retest procedure assume x★ρ,C∈int P so that a small ball around it still lies in P. If the minimizer sits on a facet of P (or P’s boundary), no such ε‐ball exists, the outer approximation Qρ may fail to separate correctly, and the entire argument for measuring R★ρ,C via containment in P collapses."
      },
      {
        "Problem": "Scaling argument and Lemma 3.5 require exact ‘congruence’ of two different intersection‐of‐balls polytopes",
        "Location": "Section 3.4, Lemma 3.5 and the preceding construction of Qρ̂ with α>1",
        "Explanation": "The paper asserts that by scaling each ball center by α>1 and recomputing a single R̂, one obtains Pρ,R²,C=Pρ̂,R̂²,Ĉ exactly. That relies on delicate equalities among many quadratic constants (equations (21),(26),(63),(118),(134)). In high dimensions this congruence is fragile: numerical roundoff or slight perturbations in S or β destroy it, and no stability or generic‐position argument is given. Without proof that arbitrary S satisfy these exact identities, the key Lemma fails."
      },
      {
        "Problem": "Outer‐approximation Qρ does not uniformly approximate P to the claimed precision",
        "Location": "Lemma 3.2 (and Appendix B)",
        "Explanation": "Lemma 3.2 aims to show P⊆Qρ⊆⋃_{x∈P}B(x,δ), but the proof only analyzes one half‐space facet at a time via a planar picture. It does not bound how large ρ must be (in terms of P’s geometry) to achieve a uniform δ‐cover across all facets or handle intersections of more than two constraints. No quantitative relation between δ and ρ is given, so one cannot guarantee Qρ→P as ρ→∞ at any controlled rate, invalidating the approximation‐and‐bisection strategy."
      }
    ],
    "think_summary": [
      "**Analyzing the paper for critical flaws**\n\nThe user is asking me to check a paper for major errors that could invalidate its conclusions. They want me to focus on significant issues rather than minor details like typos. I'm going to closely examine the paper's claims, particularly its assertion about providing a fully polynomial-time approximation scheme for the subset-sum problem. I suspect there may be fundamental flaws regarding its complexity when applied to real numbers and how it redefines the problem. I'll identify up to five critical problems in a JSON format.",
      "**Evaluating polynomial algorithms**\n\nI’m grappling with whether a polynomial algorithm truly exists for the subset sum problem. It’s NP-hard for real and rational inputs, but the proposed algorithm seems to overlook bit complexity and numerical precision. They assume a BSS model and claim to solve real inputs in constant time, which feels flawed. I see risky assumptions in their key lemma and issues with approximations, especially regarding boundary points and non-convex constraints. There’s a lot to unpack here!",
      "**Analyzing proof limitations**\n\nI’m noticing that the proof only addresses a specific directional case, which makes it inadequate for ensuring consistent approximation quality across all facets. This oversight also means it doesn't properly bound the worst-case error, which is a critical issue. Now, I’ll need to represent these findings clearly in a JSON format, outlining the main problem, its location, and a brief explanation for clarity. This should help convey the shortcomings effectively!"
    ],
    "token_usage": {
      "input": 35773,
      "thinking": 1664,
      "output": 867
    }
  },
  {
    "entry_id": 17,
    "retraction_id": "2001.09967v3",
    "paper_id": "2001.09967v2",
    "retraction_comment": "This paper is withdrawn because there is an error in the last section: the algebraic identities, in the limit n-> \\infty, all collapse to the first conservation law. One could wonder whether this can be fixed via a suitable renormalization scheme but at present, the argument is incomplete",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Unjustified continuum limit in the derivation of the conservation laws",
        "Location": "Section 3 (especially 3.0.5)",
        "Explanation": "All of the infinitely many conservation laws are obtained by taking limits of Vieta/Newton identities for fixed k as n→∞.  However the paper never controls the error terms in replacing the discrete elementary symmetric polynomials e_k(x₁,…,x_n) by the integrals ∫x^k u(0,x) dx, nor does it justify uniform convergence of those sums after t n differentiations.  Without any quantitative estimate on the rate of convergence or on how the roots of p_n^{(t n)} continue to sample a nice density u(t,x), the passage from the algebraic identities at finite n to the continuous conservation laws is not rigorous and may fail."
      },
      {
        "Problem": "Over-strong statement of support monotonicity from an asymptotic discrete bound",
        "Location": "Theorem 2 (Section 2.3)",
        "Explanation": "Theorem 2 asserts that |supp u(t,·)|/(1−t) is non-decreasing in t.  Its proof appeals to Sz-Nagy’s discrete span bound span(p^{(k)})/span(p) ≥ √((n−k)(n−k−1)/(n(n−1))) and then passes to the limit k/n→t.  But for each finite n the factor √((n−k)(n−k−1)/(n(n−1))) is strictly less than (1−t), and no uniform error estimate is given.  Hence one cannot conclude a genuine monotonicity in the continuum limit unless one shows that the o(1) gap vanishes uniformly in t, which is not addressed."
      },
      {
        "Problem": "Neglect of boundary terms in the Hilbert-transform conservation identities",
        "Location": "Section 2.2 (derivation of the first two Hilbert transform identities)",
        "Explanation": "The identities  (1/π)∫_ {u>0}x∂_x arctan(Hu/u) dx = ∫_ {u>0}x u dx  and its quadratic analogue are obtained by formally differentiating the conserved moments and replacing ∂_t u via the PDE.  But when one integrates expressions of the form x ∂_x (arctan(Hu/u)) over the moving support {u>0}, boundary terms x arctan(Hu/u)|_{∂{u>0}} do not vanish a priori (u→0 causes arctan (Hu/u)→±π/2).  The paper does not justify that these boundary contributions cancel or vanish, so the proposed Hilbert transform identities may fail."
      },
      {
        "Problem": "Unsupported termwise Hilbert-transform expansion in the Chebyshev argument",
        "Location": "Proposition in Section 4.1",
        "Explanation": "To linearize around the semicircle law the paper writes f(x)/√(1−x^2)=∑a_k T_k(x) and then asserts Hf=∑a_k U_{k−1}(x) term by term.  But no justification is given for interchanging the principal-value integral defining the Hilbert transform with the infinite Chebyshev expansion.  In particular, there is no estimate on the uniform convergence of the series or on the decay of a_k to ensure that one may apply the Hilbert‐transform identity 1/π∫T_k(y)/((x−y)√(1−y^2)) dy=U_{k−1}(x) for each term and sum termwise.  Without such control the linearization proof is unsound."
      },
      {
        "Problem": "Unproven regularity and simplicity of the evolving support",
        "Location": "Implicit throughout, but especially in Sections 1.2 and 2.2",
        "Explanation": "All of the PDE‐based arguments and conservation laws require that for each t∈(0,1) the root density u(t,·) exists as a single smooth density on one interval, with no new gaps opening or merging in the support.  Yet no mechanism is provided to guarantee that differentiation of a ‘nice’ initial root ensemble cannot create multiple disjoint clusters or singularities in the root distribution before t=1.  Without such a regularity result the very hypothesis that u(t,x) satisfies the proposed PDE (and hence the ensuing conservation and Hilbert‐transform identities) may fail."
      }
    ],
    "think_summary": [
      "**Analyzing conservation laws**\n\nI see that the approach focuses on proposing conservation laws and identities instead of fully deriving the partial differential equations (PDE). The main results include infinite conservation laws and non-decreasing monotonicity quantities. However, there's a concern about the assumptions made regarding limits and the distribution of roots. They don't seem to verify limits or smoothness for all values, which might be a significant error in establishing their results. I wonder if that undermines the overall findings.",
      "**Examining conservation laws**\n\nI notice that the statements are conditional: they require the densities to exist for the conservation laws to hold. They use Newton's identities to rewrite elementary symmetric polynomials but need to ensure uniform convergence for the asymptotic factor when k is fixed. Considering the monotonic quantity, they relate Sz-Nagy theorem to show non-decreasing behavior but seem to misrepresent bounds. Particularly, the discrete factor appears larger than expected under large n, suggesting closer scrutiny is needed for accurate assessments.",
      "**Reviewing mathematical propositions**\n\nThey start with a function exchange involving f(x) and u(x) but seem to miscalculate a result by introducing a factor of π incorrectly. Initially intending to show conservation laws, they adjusted integrals and utilized Newton's identities but lacked rigor regarding convergence, leading to conditions on the results. They've consistently applied normalization factors, which I initially overlooked, allowing their propositions to check out despite some potential issues in assumptions about density existence and support behavior. Careful attention is required for rigorous validation.",
      "**Evaluating conservation laws**\n\nThey claim that certain representations yield values solely in terms of π but face issues with convergence requirements. While the factor in the proposition checks out, the reliance on assumptions about root distributions and continuity creates potential flaws. Problems arise in the justification for convergence and the lack of rigorous treatment on boundary terms during derivations. The paper's conclusions assert many conservation laws, yet critical issues must be addressed to validate these statements convincingly. Overall, there's enough uncertainty to warrant caution."
    ],
    "token_usage": {
      "input": 13632,
      "thinking": 7552,
      "output": 1006
    }
  },
  {
    "entry_id": 18,
    "retraction_id": "2003.01493v2",
    "paper_id": "2003.01493v1",
    "retraction_comment": "Theorem 3.3 is not true in general. If it holds, for example, when n=2, we infer that all 2-cluster tilting subcategories are 2Z-cluster tilting, but it can't hold in general",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Invalid use of embedding into L2(A, G) to construct cokernels in Lemma 3.1",
        "Location": "Lemma 3.1 (page 12)",
        "Explanation": "The proof invokes an n-exact functor He: A → L2(A, G) (from [4]) to deduce exactness of Hom-sequences and hence the existence of a cokernel u in A. However, the embedding of an arbitrary n-abelian category into L2(A, G) requires A to be small and injectively cogenerated, hypotheses not assumed in Theorem 3.3. Without a justified embedding or additional finiteness conditions, the existence of the ordinary cokernel u in A is not established."
      },
      {
        "Problem": "Use of ordinary kernels in n-abelian category without justification",
        "Location": "Step 3 of Theorem 3.3 (pages 14–15)",
        "Explanation": "The argument chooses f₀ as a kernel of f₁ in A by citing the dual of [10, Proposition 3.7]. But an n-abelian category only guarantees the existence of n-kernels (chains of length n+1), not 1-kernels (ordinary kernels). There is no general result in Jasso’s axioms that ensures every weak kernel can be refined to an actual kernel, so the passage from weak kernel to kernel is unsubstantiated."
      },
      {
        "Problem": "Incomplete proof of Lemma 3.2 on m-fold concatenation of n-extensions",
        "Location": "Lemma 3.2 (page 12)",
        "Explanation": "The lemma claims an equivalence between E◦F being contractible and E or F factoring through a trivial extension for arbitrary fold numbers r, s. The proof rigorously treats only the case r = s = 1, then asserts an inductive argument for larger r+s without detailing how homotopy equivalences, n-pushouts, or ‘good’ decompositions extend to higher-fold concatenations. Key steps (choice of morphisms ϕ, ψ, and verification of contractibility) lack verification for general r, s."
      },
      {
        "Problem": "Altering sequences by direct sum may break n-exactness",
        "Location": "Step 5 of Theorem 3.3 (page 15)",
        "Explanation": "To force d₁ fₙ₋₁ to become a weak kernel of d₂, the authors replace Tₙ₋₁ with Tₙ₋₁⊕Y¹ and adjust the maps. However, they do not prove that this alteration preserves the n-exactness of the column or the overall diagram. In an n-abelian category, direct sums and splits do not automatically yield valid n-exact sequences, so the construction may invalidate the n-exactness required for the argument."
      },
      {
        "Problem": "Overreliance on Lemma 3.2 in Step 6 without proper generalization",
        "Location": "Step 6 of Theorem 3.3 (page 16)",
        "Explanation": "The exactness at nE^i(A, Xₙ₊₁) uses Lemma 3.2 to deduce that if [X]◦[S] is trivial then S factors as ψ·F. But Lemma 3.2 was not fully established for arbitrary i-fold extensions (it was only detailed for the 1-fold case). The jump to arbitrary i relies on the incomplete inductive proof in Lemma 3.2, so the crucial factorization step in the main theorem lacks a firm foundation."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 25705,
      "thinking": 2560,
      "output": 827
    }
  },
  {
    "entry_id": 19,
    "retraction_id": "2005.08379v2",
    "paper_id": "2005.08379v1",
    "retraction_comment": "Figure 1 is incorrect. Will be updated in the revision",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Unvalidated historical trend data source",
        "Location": "§2 (Data Collection and Methodology)",
        "Explanation": "The paper relies on a third-party service (Trendogate) and ad-hoc validation via the Wayback Machine without quantifying coverage or completeness. Missing or incomplete historical trends could systematically bias trend counts across countries and invalidate cross-country comparisons."
      },
      {
        "Problem": "Correlation treated as causation",
        "Location": "Abstract and §4 (Discussion and Conclusion)",
        "Explanation": "The authors observe that countries with lower COVID-19 spread exhibited higher Twitter engagement and infer that social media awareness influenced pandemic outcomes. No controls for confounding factors (e.g., testing rates, government policies, demographics, healthcare capacity) are applied, so causal claims are unsound."
      },
      {
        "Problem": "Sentiment analysis applied to multilingual data without appropriate handling",
        "Location": "§3.3 (Sentiment Analysis)",
        "Explanation": "The study uses an English-only sentiment tool (TextBlob) on tweets collected in multiple languages. There is no description of translation or language-specific sentiment models, so sentiment scores for non-English tweets are unreliable."
      },
      {
        "Problem": "Keyword‐based identification of COVID-19 trends and tweets lacks validation",
        "Location": "Algorithm 1 and §2.1 (Table 1)",
        "Explanation": "COVID-19–related trends and tweets are identified solely via string matching against a curated term list. No evaluation of precision, recall or false positives/negatives is provided, so trend/tweet counts may be substantially misclassified."
      },
      {
        "Problem": "Raw tweet and trend volumes are not normalized for Twitter usage or population",
        "Location": "§2.1 (Table 1) and §3.1 (Temporal Analysis)",
        "Explanation": "Comparisons use absolute counts of trends and tweets without accounting for differences in overall Twitter penetration, population size, or baseline activity. Countries with larger or more active Twitter user bases will naturally generate more content, confounding the proposed link between engagement and pandemic spread."
      }
    ],
    "think_summary": [
      "**Analyzing trend comparisons**\n\nI'm considering how different onset dates make comparing trend spikes arbitrary. Tweets might refer to global news rather than local events, and the trend counts vary based on how many people are using Twitter in each country. So, their conclusions could be invalid due to this. I want to merge this into a previous point about baseline differences in Twitter usage, emphasizing that not normalizing for user population skews the data. I'll list it as a critical problem."
    ],
    "token_usage": {
      "input": 19939,
      "thinking": 960,
      "output": 464
    }
  },
  {
    "entry_id": 20,
    "retraction_id": "2408.09172v3",
    "paper_id": "2408.09172v2",
    "retraction_comment": "The model diagram in Figure 1 on page 3 of the paper has significant ambiguities. It may lead readers to mistakenly believe that the experiments were conducted in a multi-turn dialogue format. Therefore, we request the withdrawal of this submission",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Conflation of sycophancy (social conformity) with epistemic uncertainty",
        "Location": "Section 3.1–3.2 (Uncertainty Tripartite Testing Paradigm)",
        "Explanation": "The method assumes that wavering behavior under wrong-label injection directly reflects the model’s lack of knowledge, but this perturbation primarily measures the model’s tendency to conform to user-provided labels (sycophancy) rather than its true epistemic uncertainty. This confounding undermines the interpretation of ‘uncertain’ instances and the validity of downstream analyses."
      },
      {
        "Problem": "Heavy reliance on a single prompt formulation without robustness checks",
        "Location": "Section 3.1 (Prompt design) & Limitations",
        "Explanation": "Unc-TTP depends critically on the exact wording of the interference prompts (e.g., 'do not change your stance so easily'), yet no ablation or sensitivity analysis is presented. Given known prompt-sensitivity of LLMs, this threatens replicability and calls into question whether the observed uncertainty classifications generalize beyond the chosen prompt template."
      },
      {
        "Problem": "Unfair comparison between one-example-for-all selection and per-instance retrieval baselines",
        "Location": "Section 5.1 (Example Selection Details) & Table 1",
        "Explanation": "Unc-TTP selects a single example for all test instances, whereas retrieval-based baselines (BM25, Similarity) choose different examples per instance. This mismatch in selection granularity gives Unc-TTP an artificial efficiency advantage but makes the performance comparison inequitable, conflating methodological benefits with experimental design choices."
      },
      {
        "Problem": "Risk of overfitting due to single arbitrary data split and extensive validation tuning",
        "Location": "Appendix A (Dataset Implementation Details)",
        "Explanation": "The authors create bespoke train/validation/test splits (e.g., 500/1500/200 for SH) and use validation set performance to choose the ‘most accurate’ uncertainty category, but report results on a small (n=200) test split without cross-validation or multiple random seeds for data splits. This procedure may overfit the uncertainty-category selection to the validation set, inflating test performance."
      },
      {
        "Problem": "Absence of statistical significance testing for reported improvements",
        "Location": "Tables 1, 5, 6 (ICL results & category distributions)",
        "Explanation": "Performance gains of 1–3 percentage points are reported alongside standard deviations but without any hypothesis tests or confidence intervals. Given the overlap between means and variances across methods, it is unclear whether the observed differences are robust or due to random variation."
      }
    ],
    "think_summary": [
      "**Evaluating methodological concerns**\n\nIt seems like they should calibrate the number of runs, yet they don't clarify if the number of queries is equal, which could impact fairness. Regarding statistical significance, they mention improvements without testing if they’re significant; small changes might just be by chance, particularly with the large standard deviations in Table 1. Plus, they rely on subjective classification tasks without covering general tasks, which seems limiting but not critical. Lastly, their data splits seem arbitrary and could lead to overfitting, raising concerns about evaluation robustness.",
      "**Identifying critical flaws**\n\nThe label injection likely measures sycophancy rather than true model uncertainty, which blurs their distinction. This assumption is misleading, as they also group six wavering patterns uniformly, without justification for treating differing behaviors the same. Their category selection appears ad-hoc, favoring those with the best validation performances rather than assessing true uncertainty. The key flaws include: conflating sycophancy with uncertainty, prompt sensitivity affecting replicability, unfair baseline comparisons, overfitting risks with small test sets, and the absence of significance testing."
    ],
    "token_usage": {
      "input": 23803,
      "thinking": 1280,
      "output": 615
    }
  },
  {
    "entry_id": 21,
    "retraction_id": "1207.2800v2",
    "paper_id": "1207.2800v1",
    "retraction_comment": "This paper has been withdrawn by the authors because the proof of Lemma 3.3 has a gap. More precisely, the claim \"If R has a pylonic vertex, v, incident with at least two cables, the pylonicity of v is destroyed by the splitting of any corner\", as stated, is unjustified and looks false in whole generality; the authors overlooked some cases",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Unjustified general bounds in Theorem 3.6",
        "Location": "Theorem 3.6, page 7",
        "Explanation": "The authors state explicit linear upper bounds σ(S–D)=326g–162 for orientable genus g≥3 and σ(N_k–D)=163k–162 for nonorientable genus k≥5, yet these rely on exact lists of irreducible triangulations only available for closed surfaces up to genus 4. No such lists exist for higher genus, so the claimed tighter bounds beyond those small cases are unsupported."
      },
      {
        "Problem": "Over-restriction of cable location in Lemma 3.4",
        "Location": "Lemma 3.4, pages 5–6",
        "Explanation": "The proof assumes that if the patched triangulation *T is not irreducible then all its cables lie entirely within the patched disk D∪∂D. However, cables could arise outside that region through nonlocal splitting sequences, and these are not ruled out by the given conditions, so some irreducible triangulations of S–D might fail to be covered by cases (i)–(iv)."
      },
      {
        "Problem": "Insufficient isomorphism invariants in the enumeration of Section 4",
        "Location": "Section 4, pages 9–10",
        "Explanation": "In distinguishing the 20 projective-plane triangulations with 6–8 vertices, the authors rely solely on comparing cable-subgraphs and vertex-degree sequences. Different non-isomorphic triangulations can share both of these invariants, so this criterion is not in general enough to guarantee pairwise non-isomorphism."
      },
      {
        "Problem": "Ambiguous splitting operation for pinched surfaces",
        "Location": "Section 6 (definition of sp⟨u,v,w⟩ on pseudosurface), page 12",
        "Explanation": "The paper allows splitting at the singular vertex v=s provided u and w lie in the same boundary component of St(s). Yet in the spherical model this choice can change which singular relative is re-identified, potentially altering the number or arrangement of singular points and hence the topological type, a case not fully controlled by the authors' description."
      }
    ],
    "think_summary": [
      "**Evaluating critical errors**\n\nI'm tasked with identifying major errors in the paper about irreducible triangulations of punctured or pinched surfaces. They claim finiteness, bounds, enumeration of small triangulations, and findings related to the Möbius band and pinched torus. \n\nTo check for flaws, I notice an aspect regarding patch addition or removal that seems questionable. They claim certain cables must be within a specific region, but I wonder if there might be cables outside that area. I need to closely examine this argument regarding irreducibility.",
      "**Reviewing triangulation findings**\n\nThey find one irreducible triangulation at vertex count six and two at seven. However, it seems there are only two irreducible ones. I'm trying to clarify the count of triangulations with seven vertices, which they say equals three, including P1 (irreducible), P3, and P4. It appears they correctly enumerate these but must confirm symmetry definitions.\n\nMoving on to Section 5 about irreducible Möbius band triangulations, their findings likely align with known results. Section 6 discusses pinched torus triangulations, where they claim two irreducible ones. There might be a gap in the assumption regarding edge classifications. I need to consider how this affects their classification overall.",
      "**Analyzing triangulations of pinched torus**\n\nI'm calculating minimal triangulations of a pinched torus, and it looks like they found exactly two. In their proof, they describe an irreducible triangulation represented in a spherical model. They discuss edges in a specific cycle but may have misrepresented that cycle, as it's actually a union of two disjoint cycles. Their argument about connectivity seems plausible, leading to a basic skeleton.\n\nHowever, I wonder if they've overlooked non-isomorphic cases while concluding the irreducibility. Theorems 3.5 and 3.6 raise some concerns about assumptions they made regarding finiteness and boundaries, and I need to explore those further.",
      "**Creating a JSON summary**\n\nI'm breaking down various problems identified in the analysis:\n\n1. **Theorem 3.6:** The bounds for general cases are unjustified as they rely on triangulation lists known only up to genus 4, yet claim bounds for arbitrary genus without valid data.\n   \n2. **Lemma 3.4:** The argument doesn’t consider that cables may exist outside the defined patch region, which can emerge through multiple splitting sequences.\n\n3. **Section 4 Enumeration:** This section assumes non-isomorphic triangulations will always have different cable-subgraphs or degree sequences, which might not be true for more nuanced graphs.\n\n4. **Lemma 6.1:** The proof overlooks degree 2 vertices in the spherical model, invalidating that all unaffected vertices must have a degree of at least 3.\n\n5. **Section 6 Splitting:** The criteria for splitting corners in triangulations are too loose, allowing for alterations in the singular structure, which could lead to invalid results.\n\nI'll structure these issues into a JSON array format now."
    ],
    "token_usage": {
      "input": 19856,
      "thinking": 5184,
      "output": 498
    }
  },
  {
    "entry_id": 22,
    "retraction_id": "2212.04565v2",
    "paper_id": "2212.04565v1",
    "retraction_comment": "There is an error. Some cases were not considered in the proof of Theorem 1",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Unjustified projection to Grundy sequences in G and H",
        "Location": "Proof of Conjecture 1.1 (page 4)",
        "Explanation": "The argument crucially assumes that, for any closed‐neighborhood sequence S = ((g_i,h_i)) in G⊠H, the projected sequence (g_1,…,g_k) is itself a closed‐neighborhood sequence in G (and similarly for (h_1,…,h_k) in H). No justification is given for this. In fact, a new neighborhood in G⊠H can arise purely from the H‐coordinate, so the G‐projection need not exhibit the Grundy property, and one cannot bound the number of distinct g_i by γ_gr(G)."
      },
      {
        "Problem": "Invalid bound on the number of distinct coordinates",
        "Location": "Proof of Conjecture 1.1 (page 4)",
        "Explanation": "The conclusion k ≤ γ_gr(G)·γ_gr(H) rests on the statement “at most γ_gr(G) unique G‐vertices and at most γ_gr(H) unique H‐vertices may appear,” hence at most their product many pairs. But without showing the projections avoid repeats only by exhausting Grundy neighborhoods, there is no barrier to having more than γ_gr(G) distinct g_i's (or similarly for h_i)."
      },
      {
        "Problem": "Circular reasoning in applying Corollary 2.3",
        "Location": "Corollary 2.3 and its use in the proof (pages 3–4)",
        "Explanation": "Corollary 2.3 gives an upper bound |N[(g_k,h_k)]\\…| ≤ |N[g_k]\\…|·|N[h_k]| + |N[h_k]\\…|·|N[g_k]|. The proof then asserts that vanishing of both projection differences forces termination at k ≤ γ_gr(G) and k ≤ γ_gr(H). But showing |N[g_k]\\∪_{i<k}N[g_i]|=0 only when k exceeds γ_gr(G) assumes exactly the same projection‐sequence property that the paper has yet to prove. This is circular."
      },
      {
        "Problem": "Overlooked interdependence of G‐ and H‐novelty",
        "Location": "Proof of Conjecture 1.1 (page 4)",
        "Explanation": "The proof treats novelty in the G‐coordinate and the H‐coordinate as if they were independent resources until exhaustion, then simply multiplies their maxima. In reality, a step that fails to introduce new G‐neighbors may still introduce new H‐neighbors (and vice versa), so the dynamics cannot be split cleanly and bounded by a plain product."
      },
      {
        "Problem": "No treatment of degenerate cases",
        "Location": "Corollary 2.3 and Proof of Conjecture 1.1",
        "Explanation": "The argument assumes |N[g_k]|>0 and |N[h_k]|>0 to deduce that vanishing products imply vanishing factors. While closed neighborhoods always contain the vertex itself, the proof does not note whether isolated vertices or graphs with trivial Grundy number cause any exceptions to the counting, nor how these edge cases are handled."
      }
    ],
    "think_summary": [
      "**Clarifying combinatorial reasoning**\n\nI’m realizing that the previous counting isn’t necessary here. It’s about combinatorial choices: select at most m vertices from G and n from H, leading to a maximum of m*n pairs. They also mention conditions about showing m ≤ γgr(G) and n ≤ γgr(H), which gets tricky. The confusion seems to lie in how they relate elements of S to projections on G. Their assertion of equality at γgr(G) needs clarification, especially since zeros at step k imply coverage from previous neighborhoods.",
      "**Analyzing closed neighborhoods**\n\nI see that S is a closed neighborhood sequence in G⊠H, meaning N[(gk,hk)] minus previous neighborhoods is non-empty for all k within |S|. They argue that A or B being non-zero matters but really they should be showing when N[(gk,hk)] is non-empty. To keep the sequence going, either A or B must be greater than zero, which means S stops when both A and B hit zero. So, the maximum length is likely the sum of lengths, m + n, not the product. The product bound is a weaker constraint when m and n are greater than 1.",
      "**Examining projection flaws**\n\nThey don't claim equality, so that simplifies things! I see that |N[gk]| must be greater than zero, implying gk is always present in N, making the neighborhood non-empty. I need to outline the major issues here: \n\n1. They assume projections form closed neighborhood sequences without justification, undermining the bounds they set with γgr(G).  \n2. The proof doesn't show that projections respect closed neighborhoods, invalidating distinct count bounds.  \n3. They oversimplify maximum length, wrongly tying it to γgr(G) and γgr(H) without considering adjacency.  \n4. They overlook sequences that bounce between novelty in G and H, potentially extending their length beyond the proposed bounds.",
      "**Identifying proof flaws**\n\nI see that there’s no proof connecting the lower bound γgr(G⊠H) to γgr(G)γgr(H) because the upper bound fails to consider interactions between the summands. This means the proof is fundamentally flawed due to incorrect bounding arguments, so I need to outline some key problems. \n\nI'll summarize three main issues with citations for clarity:  \n\n1. Projection arguments lack justification;  \n2. Misuse of bounds on projections;  \n3. Faulty cardinality leading to incorrect limits.  \n\nThis should be sufficient to conclude my assessment."
    ],
    "token_usage": {
      "input": 5477,
      "thinking": 4480,
      "output": 739
    }
  },
  {
    "entry_id": 23,
    "retraction_id": "2209.08741v2",
    "paper_id": "2209.08741v1",
    "retraction_comment": "This manuscript has been withdrawn by the authors. Condition (B) needs to be modified for the proofs to work",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Invalid application of the removable singularity theorem across an analytic hypersurface in several complex variables",
        "Location": "Proposition 4.1 (Section 4, p. 16) and proof of Theorem 1.4 (Section 2, p. 9–10)",
        "Explanation": "The authors extend the bounded holomorphic functions w_j(z) and hence T(z) across the zero set A_p of K(·,p) by invoking the Riemann removable singularity theorem. In C^n (n>1), A_p is typically a hypersurface (codimension 1), and there is no general removable singularity theorem for bounded holomorphic functions on the complement of a hypersurface. The classical Riemann theorem applies only to isolated points in one complex variable, and Hartogs’ phenomenon requires codimension at least two. Thus the claimed holomorphic extension of T across A_p is unjustified."
      },
      {
        "Problem": "Unjustified construction and pseudoconvexity of the enlarged domain Ω̃",
        "Location": "Proof of Theorem 1.4 (Section 2, p. 9–10)",
        "Explanation": "To handle boundary points where limsup K(z,z)<∞, the authors glue in pluripolar neighborhoods U_j to form Ω̃ and assert that Ω̃ remains pseudoconvex and carries the same Bergman metric. However, filling in pluripolar 'holes' can alter pseudoconvexity or the domain of holomorphy, and no justification is given that the Bergman kernel or metric extend holomorphically (or even continuously) across these sets. The step that 'domains Ω̃ and Ω have the same Bergman metric' is therefore unsupported."
      },
      {
        "Problem": "Use of Theorem 2.3 without verifying necessary hypotheses",
        "Location": "Theorem 2.3 (Section 2, p. 8) and its application in the proof of Theorem 1.4 (Section 2, p. 9)",
        "Explanation": "Theorem 2.3 equates blow-up of the Calabi diastasis at ∂Ω with Ω being biholomorphic to a ball (and a specific relation between curvature and dimension). The original proof in [28] relied on completeness of the metric or additional regularity, but these hypotheses are neither restated nor checked here. Without verifying that the diastasis blow-up alone suffices under the weaker assumptions, the conclusion that Ω̃ is a ball may fail."
      },
      {
        "Problem": "Failure to control the Bergman kernel ratio near accumulated zeros",
        "Location": "Proposition 4.1 (Section 4, p. 16)",
        "Explanation": "Proposition 4.1 argues boundedness of w_j(z) from the estimate sup_{ζ∈U}|K(ζ,z)| ≤ C|K(p,z)|. If the zero set A_p of K(·,p) accumulates at the boundary or inside U, the ratio |K(ζ,z)|/|K(p,z)| can blow up near those zeros. Since no isolation or non-accumulation of zeros is assumed, the boundedness of w_j(z) (and hence T(z)) is not guaranteed."
      },
      {
        "Problem": "Reliance on Condition (B), which is not biholomorphically invariant",
        "Location": "Definition 1.3 and Theorem 1.4 (Section 1, p. 4–5)",
        "Explanation": "Condition (B) is used to control derivatives of K(z,p), but as shown in Remark 3.4, this condition fails to be preserved under biholomorphic change of coordinates. Since the conclusions aim to characterize Ω up to biholomorphism, depending on a non-invariant condition at a specific point undermines the intrinsic nature of the results."
      }
    ],
    "think_summary": [
      "**Identifying critical problems**\n\nThe user asked me to check a paper for critical errors and problems that could invalidate the conclusions. I'm focusing on significant mathematical issues while ignoring minor ones.\n\nI see three potential problems: \n1. Condition (B) not being a biholomorphic invariant may affect overarching statements.\n2. There’s a concern about whether the proof of Theorem 1.4 being circular since it depends on Theorem 2.3, which isn’t fully complete.\n3. The use of removable singularity in Proposition 4.1 raises questions about boundedness near the zeros of K(z,p). \n\nI need to structure these findings into a JSON format.",
      "**Identifying critical issues**\n\nI'm examining a mathematical paper, focusing on specific problems that could invalidate its conclusions. It seems there's an arbitrary boundary point chosen, and the authors need to clarify their assertions about continuity and how they extend certain functions.\n\nI've identified several critical issues:\n1. The removal of singularities in Proposition 4.1 and Theorem 1.4 may not be justified.\n2. The use of Theorem 2.3 could be too strong since completeness was removed.\n3. Other equivalence issues in Theorem 2.3 appear problematic.\n4. Condition (B) is not invariant, which could complicate assumptions. \n5. The definition of Ω˜ needs verification to ensure it meets domain criteria.",
      "**Summarizing critical issues**\n\nThe authors claim that Ω˜ is a valid domain for extending a certain function, but this might not be true. I’ve identified three main issues:\n\n1. The application of the Riemann removable singularity theorem is incorrect for functions of several complex variables, which affects the extension across an analytic variety.\n2. There's faulty reasoning in constructing the domain Ω˜, particularly concerning pseudoconvexity and the Bergman metric.\n3. The use of Theorem 2.3 lacks proper verification of necessary conditions, potentially weakening their conclusions.\n\nI’ll format this into structured entries for clarity.",
      "**Identifying further critical issues**\n\nIn Proposition 4.1, I noticed that the Cauchy estimate assumes boundedness of a function near the zeros of K(z,p). However, if these zeros accumulate at the boundary, this boundedness might not hold. \n\nAdditionally, Condition (B) is not an invariant under biholomorphic changes, meaning conclusions drawn from it could be problematic if coordinates are altered. \n\nNow, I'll structure these observations into entries for clarity and to effectively summarize the critical problems."
    ],
    "token_usage": {
      "input": 18255,
      "thinking": 3136,
      "output": 871
    }
  },
  {
    "entry_id": 24,
    "retraction_id": "1603.02871v2",
    "paper_id": "1603.02871v1",
    "retraction_comment": "Problems in the proof....specifically maximal pivotality. The estimates on the number of pivotal edges is fine",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Use of a fixed edge e₀ in the pivotal‐edge construction (Lemma 2) is unjustified",
        "Location": "Section 2, Lemma 2 (equations (2)–(3))",
        "Explanation": "The proof asserts that with positive probability one can locally modify inside a large ball B_M so that the single edge e₀ between 0 and (1,0,…,0) is pivotal for the global event {N=1}.  However, EM only guarantees two distinct infinite clusters outside B_M, and no argument is given to build inside B_M two disjoint open paths from the endpoints of e₀ to those exterior clusters.  Without a detailed deterministic template and finite‐energy estimate showing that e₀ actually lies on the unique connection of the two infinite clusters, one cannot conclude P_p(e₀ is pivotal)>0."
      },
      {
        "Problem": "Independence claim for maximally pivotal set (Lemma 4(ii)) is invalid",
        "Location": "Section 2, Lemma 4(ii)",
        "Explanation": "The paper claims that the event {P_e={e₁,…,e_k}} depends only on the states of edges outside {e₁,…,e_k}, hence is independent of their joint state.  In fact, maximality of P_e involves checking that adding any other edge destroys the pivotal property, which in turn depends on the states of those other edges.  Thus {P_e={…}} is not a cylinder event measurable only on the complement of {e₁,…,e_k}, and the asserted product‐form factorization P({P_e=…}∩∧ᵢL_{eᵢ})=P(P_e=…)\nP(∧ᵢL_{eᵢ}) fails."
      },
      {
        "Problem": "Sketchy use of the Burton–Keane tripartition argument without ergodicity",
        "Location": "Section 2, immediately after Proposition 1",
        "Explanation": "The paper appeals to the 'tripartition argument' following Grimmett’s Lemma 8.5 to conclude P(N≥3)=0 from the existence of trifurcation points with positive probability.  However, the standard Burton–Keane proof uses both translation invariance and ergodicity to bound the expected number of trifurcations in a large box by its boundary size.  Without ergodicity or a precise substitute, the necessary contradiction (that interior trifurcations grow like volume) is not rigorously established."
      },
      {
        "Problem": "Neglect of boundary‐edge dependencies in variance estimate of X_n",
        "Location": "Section 2, Lemma 3 (equations (10)–(23))",
        "Explanation": "The variance bound for the sum X_n of pivotal‐edge indicators over the box B_n treats edges near ∂B_n as if their pivotalness events depend only on disjoint sets of other edges.  In reality, an edge near the boundary can be pivotal only via influence of edges just outside B_n, so the claimed decomposition into independent contributions (in particular the bound I_{2,1}≤(3M)^d n^d and the splitting around B'_{3M}(e₁)) omits long‐range dependencies across the boundary, invalidating the Chebyshev‐type concentration step."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 5609,
      "thinking": 5824,
      "output": 759
    }
  },
  {
    "entry_id": 25,
    "retraction_id": "1110.2623v5",
    "paper_id": "1110.2623v4",
    "retraction_comment": "This paper has been withdrawn since a necessary condition for the existence of an asymptotically cylindrical Calabi-Yau metric on W_1 is in fact not satisified",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "D is not an anticanonical divisor for p≥3",
        "Location": "Appendix A (erratum)",
        "Explanation": "The construction in Section 6 asserts L(D)=K_W^{-1}, but one checks L(D)^k=K_W^{-l} with k/l=2/p≠1 for p≥3. Hence D∉|−K_W| and (W,D) fails the admissibility condition of Theorem 2.3, invalidating the core connected‐sum construction for all prime orders p>2."
      },
      {
        "Problem": "New admissible‐pair construction is incomplete",
        "Location": "Appendix B (erratum)",
        "Explanation": "The proposed remedy via successive Zn–branched covers and crepant resolutions omits proofs that the resulting W′ is simply connected, that a crepant toroidal resolution exists globally, and that the normal bundle of D′ remains trivial. Without these, (W′,D′) may still violate the hypotheses of Kovalev’s theorem."
      },
      {
        "Problem": "Matching condition for p=3 relies on an unjustified density argument",
        "Location": "Theorem 7.7",
        "Explanation": "The existence of v_2+iv_3 in the open dense subset K3′(L_1)∩Eig(ρ,ζ_3) is claimed by analogy with the p=2 case, but no argument shows that a one‐parameter real family of real parts of eigenvectors intersects the complement of countably many walls. The density argument from [16] fails for the non‐open analytic locus D^p_ρ."
      },
      {
        "Problem": "Chen–Ruan age‐shifting formula applied without Gorenstein check",
        "Location": "Section 5",
        "Explanation": "The orbifold cohomology calculation assumes all stabilizers G_x⊂SL(3,C), but for the quotient (S×P^1)/(ρ×ψ) the local actions at isolated fixed points are not verified to lie in SL(3). If det(g)≠1, the age need not be integral and Yasuda’s theorem may not apply, invalidating the Hodge‐number computations."
      },
      {
        "Problem": "Non‐existence of matchings for p≥5 is asserted too rigidly",
        "Location": "Remark 7.8",
        "Explanation": "The paper argues no matching exists when p≥5 because real parts of eigenvectors cannot be integral multiples of a primitive class in a one‐dimensional lattice. This overlooks the possibility of more general primitive embeddings of L_1⊕L_2 into the K3 lattice that are not orthogonal sum decompositions, so the non‐existence claim is unsubstantiated."
      }
    ],
    "think_summary": [
      "**Reviewing a paper for errors**\n\nThe user has asked me to check the attached paper for critical errors that could invalidate the conclusions. I need to focus on significant issues while ignoring minor ones like typos and formatting. The document consists of a main text and an erratum starting after page 25, which runs from pages 26-40. I’ll identify up to five critical problems and prepare them in a JSON format as specified. My goal is to ensure a thorough examination.",
      "**Identifying critical errors**\n\nA major critical issue arises in Appendix A regarding the claim that D is an anti-canonical divisor. They acknowledge this is only true when p=2; for p≥3, the main theorem's construction fails. Although they've proposed a remedy in Appendix B, I'm unsure if it restores the conclusions effectively. Moreover, there are issues with matching conditions for p≥5 and unstated significant group actions impacting simply connectedness. Calculating Hodge numbers could yield errors too. Thus, I should summarize these top problems.",
      "**Identifying matching condition issues**\n\nThe matching condition for the case p=3 depends on finding a positive eigenvector that meets specific divisibility requirements. However, they overlook that certain conditions must be outside a countable union of hyperplanes, which might cause generic failures. For p≥5, they state no matchings exist due to real part constraints but don’t explore other possibilities. Additionally, while calculating Hodge numbers, they ignore the effects of potential non-Gorenstein singularities. I’ll list these issues in a structured JSON format as requested."
    ],
    "token_usage": {
      "input": 25904,
      "thinking": 832,
      "output": 609
    }
  },
  {
    "entry_id": 26,
    "retraction_id": "1307.5957v4",
    "paper_id": "1307.5957v3",
    "retraction_comment": "This paper has been withdrawn by the author due to a crucial error in equation (5)",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Misapplication of Euler–Lagrange formalism in Section 3 leading to Proposition 3.1",
        "Location": "Section 3 (eqns 3.1–3.4)",
        "Explanation": "The Lagrangian L(u)=∫(½|∇u|²–¼|u|⁴)dx is a functional of the field u, not of a particle coordinate x(t). Treating x(t) as a generalized coordinate and applying the Euler–Lagrange equations to L(u) is nonsensical. Consequently the derived equation ∂ₜₜx‖u(x)‖²_{L²}=–¼∫|u|⁴dx has no valid basis."
      },
      {
        "Problem": "Incorrect and ill-posed statement of the local smoothing estimate",
        "Location": "Section 2.1, Theorem 2.2 (eq 2.8)",
        "Explanation": "The inequality ∫Im(uₓu)(t,0)dt ≤ C‖∇u‖_{L²ₓ(R)} is dimensionally inconsistent, omits specification of the time interval and norm on the left, and does not match any known local smoothing result. No proof or reference supports this malformed estimate."
      },
      {
        "Problem": "Flawed definition of the stress–energy tensor and conservation laws",
        "Location": "Section 2, eqns (2.1)–(2.4)",
        "Explanation": "The components F_{μν} are written without complex conjugation (e.g. Im(uₓu) instead of Im(ūuₓ)), the index placements and signs do not agree with the standard T^{μν} for NLS, and the derived divergences ∂_μF^{μν}=0 are not justified. This undermines all subsequent conservation-law–based arguments."
      },
      {
        "Problem": "Inconsistent setup of the Cauchy problem for NLS",
        "Location": "Introduction, eq (1.1) and surrounding text",
        "Explanation": "The Laplacian is denoted Δ_{R^{d+1}} although u:ℝ×ℝ^d→ℂ, the initial data space is given as H^s_x(ℝ^2) regardless of d, and the notation Pd_i=1∂²_{x_i} is garbled. These contradictions render the problem statement ill-posed."
      },
      {
        "Problem": "Invalid use of the Poincaré inequality on the whole real line",
        "Location": "Section 2.1, Proof of Theorem 2.2",
        "Explanation": "The Poincaré inequality requires a bounded domain or zero boundary conditions. Applying it to estimate ‖u‖_{L²(ℝ)} by ‖∇u‖_{L²(ℝ)} is false, since constant (nonzero) functions on ℝ violate any nontrivial Poincaré estimate."
      }
    ],
    "think_summary": [
      "**Reviewing paper errors**\n\nThe user provided a short paper titled \"The nonlinear Schrödinger equation and conservation laws\" and wants me to identify critical errors that could invalidate the conclusions. I should focus on the most significant issues, disregarding minor ones. I’ll look through the content, paying special attention to the abstract and the formulation around the Cauchy problem for cubic NLS. I’ll outline problems with specific locations and explanations to help clarify any major mistakes I've found."
    ],
    "token_usage": {
      "input": 3139,
      "thinking": 2112,
      "output": 690
    }
  },
  {
    "entry_id": 27,
    "retraction_id": "2211.05302v2",
    "paper_id": "2211.05302v1",
    "retraction_comment": "We just noted the explanation on phase retardation was incorrect and accordingly, the inhibition mechanism of zeroth-order light was not properly elucidated. We will submit a revised version soon",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Continuous-phase model ignores discrete pixelation and fill-factor effects",
        "Location": "Equations (3)–(7), Pages 1–2",
        "Explanation": "The theoretical analysis and numerical simulations assume a spatially continuous phase modulation (via eqs. (3)–(7)) and omit the SLM’s discrete pixel grid and finite fill factor. In reality, pixel gaps and dead zones produce additional diffraction orders and residual zero-order light that cannot be suppressed by voltage tuning alone. This oversimplification undermines the validity of the predicted zero-order suppression and modulation-efficiency improvements."
      },
      {
        "Problem": "Method still relies on blazed-grating separation contradicting claim of shield-free operation",
        "Location": "Page 2, paragraph beginning “Here, the phase maps… accompanied with blazed grating.”",
        "Explanation": "Although the paper claims elimination of zeroth-order light “without the requirement of shielding,” all experimental phase maps incorporate a blazed grating to spatially separate diffraction orders (as noted on page 2). This spatial separation and downstream aperture (iris diaphragm) remain essential for zero-order suppression, confounding the attribution of suppression effects solely to voltage optimization."
      },
      {
        "Problem": "Assumption of constant light-power loss (P_loss) across voltage settings is invalid",
        "Location": "Equation (7) and discussion preceding Equation (9), Page 2",
        "Explanation": "Modulation efficiency δ (eqs. (6)–(8)) and its relative change δ_r (eq. (9)) are computed under the assumption that the total optical loss P_loss remains constant when V0 and Vc vary. However, changing drive voltages alters liquid-crystal orientation, reflectivity, and polarization leakage, so P_loss is voltage-dependent. This invalidates direct comparisons of δ and δ_r and calls into question the reported efficiency gains."
      },
      {
        "Problem": "Experimental validation limited to two beam types at a single wavelength",
        "Location": "Figures 2 and 3, Page 3",
        "Explanation": "The method is demonstrated only for perfect vortex and Airy beams at 639 nm. No data are provided for other beam shapes, wavelengths, or broader operating conditions. Without broader experimental validation, the claimed general applicability of voltage optimization for arbitrary SLM holograms remains unproven."
      },
      {
        "Problem": "Inconsistent baseline values and arithmetic in reported modulation-efficiency improvement",
        "Location": "Page 4, paragraph describing δ rising “from 77.5% to 92.6% … δ_r = 19.5%” (Figure 3)",
        "Explanation": "The abstract cites an efficiency increase from 76% to 92.6% (≈21.8% absolute), but the experiment reports a baseline δ = 77.5% (not 76%), with δ_r calculated as 19.5%. The mismatch in baseline values and percentage-improvement calculation suggests inconsistent data reporting or arithmetic errors, undermining confidence in the quantitative results."
      }
    ],
    "think_summary": [
      "**Identifying critical problems**\n\nI see that there are some critical issues here. The user is asking for up to five significant problems in a specific format. \n\nFirst, there's the oversimplified model that neglects pixelation and fill factor, found in specified equations. This leads to an underestimation of key simulation aspects. \n\nNext, I notice inconsistency in the experimental methods when claiming no shielding, particularly regarding the use of a blazed grating, which contradicts the initial assertion. \n\nFinally, the assumption of constant power loss across all voltage settings needs addressing, given its placement in certain equations.",
      "**Highlighting critical model issues**\n\nThe model assumes constant power loss despite variations in voltage affecting liquid crystal properties, which complicates comparisons of modulation efficiency. There's also limited experimental validation, verifying only two beam types at one wavelength, raising questions about general applicability. Furthermore, discrepancies in reported modulation efficiency improvements suggest potential calculation errors, undermining credibility. Finally, using a refractive index model without accounting for real liquid crystal behavior introduces critical errors, as certain assumptions made about parameters remain unquantified and could invalidate their findings.",
      "**Clarifying critical issues**\n\nI need to highlight alternate critical issues, such as ignoring wavelength dependence and variations in LC cell thickness. The equations use fixed values, but manufacturers block different parts, causing discrepancies. The first three critical issues remain solid, and I should include calculation errors as the fifth. I’ll replace the general limitation with the pixelation effect, which isn’t accurately modeled. Finally, I’ll address the assumption of linearity in the phase shift versus voltage, but need to keep the problem list within five entries while crafting it in the required JSON format."
    ],
    "token_usage": {
      "input": 10233,
      "thinking": 2752,
      "output": 676
    }
  },
  {
    "entry_id": 28,
    "retraction_id": "1910.05809v3",
    "paper_id": "1910.05809v2",
    "retraction_comment": "Need to fix some error in the paper. In the last step of the proof, the hypersurface of the minimal principle curvature equal to zero may be tangent to the boundary of the domain",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Unjustified analyticity of the foliation hypersurfaces Γ_t",
        "Location": "§3 (paragraphs 1–2), eqns (3.4)–(3.5)",
        "Explanation": "The proof requires that each inner boundary Γ_t be real‐analytic so that u_t can be extended past Γ_t by the Cauchy–Kowalewski theorem. But starting from only C^{2+α} data on Γ, the mean‐curvature flow surfaces Γ_t are at best C^∞ for t>0, not analytic, and there is no justification that the level‐set function w solving (3.4)–(3.5) is real‐analytic. Without analytic Γ_t one cannot invoke Cauchy–Kowalewski to extend u_t across the boundary and hence cannot guarantee the smoothness of the smallest principal curvature κ₁ near a boundary point P."
      },
      {
        "Problem": "Misuse of the Cauchy–Kowalewski theorem to extend u_t across Γ_t",
        "Location": "§3 (middle of proof of Theorem 3.1)",
        "Explanation": "The authors assert that because Γ_t is analytic they can apply Cauchy–Kowalewski to the quasilinear PDE (1.1) and extend u_t to a neighborhood of a boundary point P. But Cauchy–Kowalewski requires analyticity of both the PDE coefficients and boundary data. Even if Γ_t were analytic, the constant‐mean‐curvature PDE coefficients depend on Du and so are not known a priori to be analytic in a neighborhood across the boundary. This step is therefore unjustified."
      },
      {
        "Problem": "Application of the Hopf boundary‐point lemma to the degenerate operator Δ_Σ",
        "Location": "§3, just before Claim 1 and in the final contradiction (use of Hopf on κ₁)",
        "Explanation": "The superharmonicity arguments for 1/W and for κ₁ rely on applying the Hopf lemma to the Laplace–Beltrami operator Δ_Σ, which is degenerate at points where Du may become large and whose boundary regularity is only C^2 up to Γ. No verification is given that Δ_Σ is uniformly elliptic up to the boundary nor that the geometric boundary of the graph Σ meets the hypotheses of the classical Hopf lemma. Without uniform boundary ellipticity one cannot conclude the strict normal‐derivative inequalities used in the convexity argument."
      },
      {
        "Problem": "Gap in the continuity‐method existence proof: lack of C^{2,α} a priori estimates",
        "Location": "§2 (proof of Theorem 2.1)",
        "Explanation": "The authors claim that deriving only C¹ bounds for u_t suffices for the continuity method, but closing the continuity argument for a quasilinear elliptic PDE like (1.1) requires uniform C^{2,α} estimates (or Evans–Krylov‐type second‐derivative bounds). No C² or Schauder estimates are produced, so the openness/closedness steps of the continuity method are unsupported."
      },
      {
        "Problem": "Undemonstrated applicability of the Bian–Guan constant‐rank theorem",
        "Location": "§3 (application after Claim 1)",
        "Explanation": "The constant‐rank theorem of Bian and Guan requires the PDE to be concave (or convex) in the Hessian D²u and to satisfy certain structural conditions. Although the mean‐curvature operator in (1.1) is linear in D²u with gradient‐dependent coefficients, no verification is given that it meets the exact hypotheses of Corollary 1.3 in [1]. In particular, the dependence on Du may violate the needed uniform structural concavity in D²u along the homotopy of domains."
      }
    ],
    "think_summary": [
      "**Identifying critical errors**\n\nI need to focus on finding critical errors in the paper, which could undermine its conclusions. This involves highlighting up to five significant issues while ignoring minor concerns.\n\nThe paper discusses strict convexity of constant mean curvature graphs in a specific context and relies on continuity methods for proof. One potential issue arises from their assumption of an analytic foliation in the level set flow of mean curvature flow. This raises questions about regularity, especially since the foliation could break down at singularities, challenging their claim of analyticity.",
      "**Assessing PDE analyticity and theorems**\n\nI need to analyze equations 3.4 and 3.5 concerning the quasilinear PDE. They claim that w is analytic, but the PDE isn't elliptic unless the gradient is non-zero. I wonder if classical theory confirms real-analytic solutions here because it’s nonlinear yet has analytic coefficients. \n\nNext, regarding the constant rank theorem's applicability, I see it requires convexity in the Hessian, which raises questions about whether the mean curvature operator meets this condition. \n\nLastly, I need to check their application of Simons' identity to the graph.",
      "**Evaluating Simons identity and boundary issues**\n\nI need to determine if Simons' identity applies to minimal or constant mean curvature surfaces. They present a version of it, but I'm not sure if their transformations maintain sign consistency.\n\nIn their proof, they use the Hopf lemma, which raises questions about its applicability to variable metric Laplacians; it's crucial to ensure the conditions are met. Moreover, they may need to manage coordinate frames and curvature carefully in their calculations.\n\nFinally, they discuss a mean curvature flow, but I need to verify if their assumptions about curvature and timing are consistent across the defined domain.",
      "**Identifying key issues in analysis**\n\nI need to pinpoint several critical problems in the paper regarding existence and regularity of solutions. They only derive C^1 estimates, yet C^{2,α} is essential for applying continuity methods, as it ensures the necessary bounds. \n\nI see some specific issues: First, there’s an unjustified assumption about analyticity in foliation and Cauchy-Kowalewski extension. Second, the validity of mean curvature flow foliation is questionable, given only C^{2+α} leads to C^∞ surfaces. \n\nThe Hopf lemma’s application seems misapplied for their operator, and I need to clarify the constant rank theorem's requirements."
    ],
    "token_usage": {
      "input": 7320,
      "thinking": 3968,
      "output": 875
    }
  },
  {
    "entry_id": 29,
    "retraction_id": "1108.1348v2",
    "paper_id": "1108.1348v1",
    "retraction_comment": "Withdrawn because of a crucial error in eq.(15)",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Use of the projectable version with global Hamiltonian constraint",
        "Location": "Sec. 2 (after Eq. 1)",
        "Explanation": "By imposing N = N(t) (projectability) the model enforces a single global Hamiltonian constraint and forbids sourcing a local Newtonian potential. The failure to recover Newton’s law in the IR is a known pathology of projectable Hořava–Lifshitz gravity, independent of λ running. Thus attributing the non-recovery to λ≠1 conflates two separate issues and invalidates the conclusion that λ must be fixed to unity."
      },
      {
        "Problem": "Ad hoc regularization of divergent Hamiltonian integrals",
        "Location": "Eqs. (14)–(24)",
        "Explanation": "The global Hamiltonian constraint integral over r of 1/r–weighted oscillatory functions diverges. The authors introduce an arbitrary cutoff L to ‘regularize’ these integrals and force the oscillation amplitudes a, b to zero. This procedure lacks any physical or mathematical justification in the context of GR or HL gravity, so the vanishing of a,b — and therefore the exclusion of nontrivial solutions — is unjustified."
      },
      {
        "Problem": "Underspecified potential term V",
        "Location": "Sec. 2 (after Eq. 2) & Eq. (11)",
        "Explanation": "The paper refers to a generic renormalizable potential V containing all marginal operators up to z=3 but does not specify its couplings or explicit form. The omitted terms (denoted OV in Eq. (11)) could contribute nontrivially to the equations of motion and to the Hamiltonian constraint, potentially admitting solutions that the authors discard. Without fixing or at least constraining these couplings, the negative results about the Newtonian limit are not general."
      },
      {
        "Problem": "Asymptotic solutions not shown to satisfy global constraints",
        "Location": "Sec. 3.3 (after Eq. (30))",
        "Explanation": "The authors derive oscillatory asymptotic forms for n(r) and A(r) at large r but do not verify that these asymptotics can be extended to full solutions obeying the exact equations of motion and the global Hamiltonian constraint. Drawing conclusions about incompatibility with Newton’s law from unchecked asymptotics is speculative."
      },
      {
        "Problem": "Neglect of known strong-coupling instabilities for λ≠1",
        "Location": "Introduction (paragraphs 2–3) & Sec. 4",
        "Explanation": "Although the U(1) extension is claimed to eliminate the extra scalar mode even for λ≠1, prior studies (e.g. da Silva et al.) show that strong-coupling and instability issues persist in the matter sector when λ≠1. Ignoring these obstructions undermines the premise that a smooth IR limit with λ≠1 is physically viable, calling into question the analysis of λ running altogether."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 8341,
      "thinking": 1536,
      "output": 687
    }
  },
  {
    "entry_id": 30,
    "retraction_id": "1505.02494v2",
    "paper_id": "1505.02494v1",
    "retraction_comment": "This paper has been withdrawn by the author due to crucial sign errors in Theorem 5 and equation (10)",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Algorithm 2’s convergence is only proved under an unverified \"local convexity\" assumption and no mechanism is given to enforce it",
        "Location": "Section 4 (Algorithm 2, Theorems 7–9)",
        "Explanation": "The monotonic ascent proof requires that each new iterate x^{k+1} lie in a neighborhood Σ(x^k) on which the shifted objective is convex.  However, the paper does not show how to choose the step or shift α to guarantee x^{k+1}∈Σ(x^k), and in the implementation they omit the standard corrective line‐search or shift‐increase strategy.  Without enforcing this, the ascent property and hence convergence claim may fail in practice."
      },
      {
        "Problem": "The reformulation in Theorem 4 is only shown to be one–way (KKT ⇒ complementarity) and not fully equivalent",
        "Location": "Section 3, Theorem 4",
        "Explanation": "The authors prove that any KKT point x of the nonlinear program (P)_J with λ(x)>0 satisfies the tensor complementarity conditions, but they do not show the converse—that every solution of the symmetric GEiCP‐T_J corresponds to a KKT point of (P)_J.  Thus the asserted ‘‘equivalence’’ is incomplete and it is unclear whether solving (P)_J will find all complementarity solutions."
      },
      {
        "Problem": "Inconsistent or incorrect gradient and Hessian formulas may invalidate the KKT analysis and the algorithmic updates",
        "Location": "Section 3 (proof of Theorem 4) vs. Section 4 (Eqs. (9),(10),(11),(12))",
        "Explanation": "The paper states two different expressions for ∇λ(x)—one with an overall factor 2·B x^m and no denominator, and later the standard m/(B x^m) form—and gives a complicated Hessian in (10) without derivation.  These inconsistencies cast doubt on both the KKT derivation in Theorem 4 and the correctness of the ascent direction used in Algorithm 2."
      },
      {
        "Problem": "The NP‐hardness reduction via tensor symmetrization glosses over a nontrivial scaling factor",
        "Location": "Section 3, Theorem 6",
        "Explanation": "To show NP‐hardness the authors symmetrize a specially constructed tensor so that A x^m equals y^T A y.  In fact, the symmetrization introduces a constant factor 1/(k!(m−k)!), so A x^m = (1/(k!(m−k)!))·y^T A y.  While this does not change the sign test, the paper fails to mention or justify this scaling, leaving the reduction informal."
      }
    ],
    "think_summary": [
      "**Checking critical errors**\n\nThe user wants me to identify critical errors and unsoundness in a paper titled \"Generalized Eigenvalue Complementarity Problem for Tensors,\" while skipping typos and formatting issues. I’m tasked to scan through the paper and focus on Section 2, examining its propositions and lemmas for correctness. For example, Proposition 1 appears sound. However, I'll need to keep searching for up to five major problems, organized neatly in a JSON format as specified.",
      "**Identifying gradient errors**\n\nI’m analyzing the expression for the gradient in Theorem 4 related to certain tensors. I’ve calculated the gradient and found discrepancies in dimensions and factors. The original statement suggests a mismatch, as it involves a confusing arrangement of variables. I notice that while they define it, they may have intended a different formulation. It’s crucial to recognize that the gradient should maintain consistent dimensions across variables, but their notation seems flawed unless a specific case holds true. I need to clarify this.",
      "**Identifying critical issues**\n\nI'm reviewing weaker bounding real solutions and selecting the four most significant issues: algorithm convergence, the direction of Theorem 4, gradient inconsistencies, and the bounds on real solutions. Minor issues like constant gradient errors and NP-hard scaling can be overlooked. My entries include: 1. A gap in algorithm convergence due to an unverified assumption. 2. Theorem 4's one-directional claim lacks proof for KKT condition satisfaction. 3. Gradient and Hessian formulas may disrupt the algorithm. 4. NP-hardness reduction may overlook a critical scaling factor. Now I'll prepare this as a JSON list."
    ],
    "token_usage": {
      "input": 18760,
      "thinking": 7360,
      "output": 619
    }
  },
  {
    "entry_id": 31,
    "retraction_id": "1504.07542v2",
    "paper_id": "1504.07542v1",
    "retraction_comment": "This paper has been withdrawn by the author due to a crucial sign error in equation 9",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Defining a critical temperature in the presence of an explicit external pair potential is unphysical",
        "Location": "Page 1, Introduction and Eq.(3–4)",
        "Explanation": "Introducing the external pair potential υ breaks U(1) symmetry explicitly, ensuring Δ≠0 at all temperatures. A system with an explicit symmetry-breaking field has no true thermodynamic phase transition, so the notion of Tc* where part of the gap goes to zero is artificial and does not correspond to any singularity in the free energy."
      },
      {
        "Problem": "Unjustified piecewise constant gap ansatz ∆(ξ)",
        "Location": "Page 2, Eq.(10–11)",
        "Explanation": "The assumption of two separate constant gap amplitudes (∆ph for |ξ|<ω and ∆cl for ω<|ξ|<εF) is physically unfounded. Above the phonon cutoff ω there is no pairing interaction to sustain ∆cl, and the Coulomb pseudopotential only renormalizes the net coupling rather than creating a distinct gap region."
      },
      {
        "Problem": "Non-analytic linear term in the Ginzburg–Landau functional violates Landau theory prerequisites",
        "Location": "Page 4, Eq.(21–22)",
        "Explanation": "The free energy includes a term proportional to |Ψ|, which is non-analytic at Ψ=0 and breaks U(1) gauge invariance. Landau expansions require analyticity in powers of |Ψ|^2 near the transition. The ad hoc addition of 2u|Ψ| via the “small increments theorem” lacks microscopic justification."
      },
      {
        "Problem": "Physically impossible predictions of infinite Hc2 and vanishing Hc1",
        "Location": "Page 5, Eq.(31–32)",
        "Explanation": "The model yields Hc1→0 and Hc2→∞ near Tc*, implying superconductivity survives arbitrarily high magnetic fields—contradicting fundamental limits. These pathologies stem from the flawed free-energy functional with an external pair field rather than any realistic superconducting behavior."
      },
      {
        "Problem": "Singular divergence of Tc* and effective coupling in the nonadiabatic limit",
        "Location": "Page 3, Eq.(18–19)",
        "Explanation": "In the limit μ*→μ the denominator in geff=(g−μ*)μ/(μ−μ*) vanishes, giving geff→∞ and Tc*→∞. This mathematical singularity reflects an improper application of the Coulomb pseudopotential renormalization rather than any physical mechanism for an unbounded critical temperature."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 8372,
      "thinking": 2432,
      "output": 606
    }
  },
  {
    "entry_id": 32,
    "retraction_id": "2012.10766v4",
    "paper_id": "2012.10766v3",
    "retraction_comment": "Propositions 3 and 4 are incorrect, and so is their proof. The proof requires the shifted convolution problem for the coefficients of the cusp forms, which is not presented here. The definition of the mollifier M(f,s) is wrong. It needs to be defined as a convolution inverse of the Fourier coefficients of the automorphic forms",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Inadequate justification of the twisted mean‐square for GL(2) L–functions (Proposition 4)",
        "Location": "Section 7, Proposition 4",
        "Explanation": "The paper’s key inversion step relies on a ‘twisted’ second‐moment estimate for L(f,σ0+it) against the mollifier M(f,s).  The author merely sketches a transfer of Hughes–Young’s zeta‐function methods to holomorphic cusp‐form L–functions, omitting all the detailed analysis of gamma‐factors, contour shifts, diagonal/off‐diagonal separation, and error bounds.  Without a fully rigorous derivation of the main term ∼T and o(T) error, the crucial claim L(f,σ0+it)M(f,σ0+it)=1+o(1) is unsupported, undermining the entire mollification argument and thus the CLT proof."
      },
      {
        "Problem": "Neglect of cross‐moment contributions in joint moments (Lemmas 10 & 11)",
        "Location": "Section 8, Lemma 10; Section 9, Lemma 11",
        "Explanation": "When computing mixed moments of the prime sums P_{a1,a2,0}(s), the author drops all terms involving λ_{f1}(p)λ_{f2}(p) for distinct cusp forms f1≠f2, asserting they are lower order.  However, no citation or proof is given of the Rankin–Selberg bound ∑_{p≤X}λ_{f1}(p)λ_{f2}(p)/p=O(1).  Without demonstrating that these cross‐terms contribute at most O((log log T)^{k−1+ε}), the claimed factorization of moments and the resulting independence conclusions are unjustified."
      },
      {
        "Problem": "Unchecked contour integral in the |L(f,s)|^2 identity (Lemma 6)",
        "Location": "Section 7, Lemma 6",
        "Explanation": "Lemma 6 expresses |L(f,s)|^2 via contour integrals I(f,s) and I(¯f,1−s) over vertical lines.  The paper does not verify the necessary decay of the integrand at large imaginary parts, the absence of poles off the critical strip, or that the gamma‐factor ratio G(f,z+s)/G(f,s)G(¯f,z+¯s)/G(¯f,¯s) is holomorphic and bounded.  As this identity underpins the approximate functional equation and the mean‐square analysis, its unproven status casts doubt on all subsequent mean‐value estimates."
      },
      {
        "Problem": "Lack of simultaneous verification of parameter constraints (Propositions 2–3)",
        "Location": "Section 3 (parameter setup), Section 5–6 (Propositions 2–3)",
        "Explanation": "The proof introduces multiple scales X, Y, σ0 and truncation levels for the Dirichlet polynomials, each requiring inequalities like X^{k+ℓ}≪T, Y^{100 log log T}X^{100 log log log T}<T^ε, and AW≪√{log log T}.  The paper never checks that all these conditions can be met simultaneously as T→∞.  Hidden conflicts among these parameter choices could invalidate the error‐term estimates in Propositions 2 and 3, disrupting the moment computations that drive the CLT."
      },
      {
        "Problem": "Informal application of Cramér–Wold for multivariate normality",
        "Location": "Section 8–9 (proofs of Theorems 4 & 5, final steps of Theorems 2 & 3)",
        "Explanation": "To conclude joint Gaussianity of (log |L(fj,½+it)|)_{j=1}^n, the author invokes a lemma stating that if every linear combination converges to a normal law then the vector is multivariate normal.  However, there is no discussion of tightness or exchange of the limit over continuous parameters t and choices of coefficients.  Without controlling the full characteristic function or verifying Lindeberg‐type conditions, the Cramér–Wold argument remains heuristic rather than a rigorous proof of joint convergence."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 25108,
      "thinking": 2752,
      "output": 973
    }
  },
  {
    "entry_id": 33,
    "retraction_id": "1703.02535v2",
    "paper_id": "1703.02535v1",
    "retraction_comment": "Error in the diffusion constant of Theorem 2.2, equation (2.4) due to a delicate issue in the method of the proof which is not trivial to correct",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Lack of rigorous propagation of chaos at multiple hierarchical scales",
        "Location": "Section 3.2; Theorems 2.2 and 2.6",
        "Explanation": "The derivation of the limiting SDEs for block communities relies on heuristic telescoping arguments and an informal separation of time scales, with no estimates controlling the error terms or proving propagation of chaos across scales. Without a full, rigorous proof that the block averages indeed converge to autonomous Kuramoto‐type diffusions, the main conclusions (Theorems 2.2 and 2.6) remain unestablished."
      },
      {
        "Problem": "Reliance on numerically verified inequality for general disorder",
        "Location": "Lemma 4.9; Section 4.3; Appendix B.2",
        "Explanation": "Lemma 4.9, which provides the key lower bound V_μ(λ)≥V_δ₀(I_μ λ), is justified only by numerical checks for three specific distributions. The subsequent classification of universality classes in Theorems 2.7–2.8 assumes this inequality holds for all symmetric unimodal μ, but no general proof is given. Thus the results for the disordered system lack validity beyond the tested examples."
      },
      {
        "Problem": "Unproven concavity of V_μ(λ) for general disorder measures",
        "Location": "Remark 4.1",
        "Explanation": "The analysis of the renormalization map, including uniqueness of synchronized states and monotonicity properties, depends on the strict concavity of λ↦V_μ(λ) for symmetric unimodal μ. However, this concavity is only conjectured (and proved under narrow‐disorder assumptions) but not established in general. Without it, the structure and iteration of the renormalization transformation are not guaranteed."
      },
      {
        "Problem": "Neglect of higher‐level interactions without error estimates",
        "Location": "Section 3.2, equations (3.20)–(3.34)",
        "Explanation": "In passing to the limit N→∞ the authors drop all interaction terms beyond the immediately higher hierarchical level (`≥k+2) as negligible. No quantitative bounds or uniform estimates are provided to justify that these infinitely many weaker couplings do not cumulatively affect the limit. This gap undermines the derivation of the reduced SDE at each scale."
      },
      {
        "Problem": "Uncontrolled asymptotic expansion for large λ in Appendix A.2",
        "Location": "Appendix A.2",
        "Explanation": "The asymptotic 1−V_μ(λ)~1/(2λ) as λ→∞ is derived via a saddle‐point argument with several nonrigorous steps (e.g. neglecting O(λ ε⁴) terms and exchanging limits) and without verifying uniformity in the disorder variable ω. This weakens the validity of the large‐λ bounds used in Lemma 4.9 and hence the synchronization criteria for strong coupling."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 37008,
      "thinking": 1408,
      "output": 691
    }
  },
  {
    "entry_id": 34,
    "retraction_id": "1611.01102v3",
    "paper_id": "1611.01102v2",
    "retraction_comment": "The paper fails to appreciate that the necessitation rule is tacitly presupposed in the formulation of the Girle-Priest tableau rules. If those presuppositions were made explicit, the proofs of this paper would depend on a use of necessitation, contra what was claimed",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "No mechanism for self-referential fixed point in pure modal logic",
        "Location": "Section 2, (Def) on page 3",
        "Explanation": "The argument assumes the existence of a sentence q such that □(q ↔ ¬□q). But standard modal logic (operator view) has no built-in diagonal or truth-predicate machinery to construct a formula that directly refers to its own necessity. Without a fixed-point or diagonal lemma, (Def) is unjustified in the object language."
      },
      {
        "Problem": "Conflating tableau meta-rules with object-level derivations",
        "Location": "Section 2, rules □-Elim and ◇-Elim on pages 2–3",
        "Explanation": "The proof treats tableau proof-search rules—especially ◇-Elim which introduces a new world k and an atom nAk—as if they were inferential rules in the object language. Standard axiomatic modal systems do not include formulas like nAk, so the derivation illegitimately mixes semantic machinery into an object-level inconsistency proof."
      },
      {
        "Problem": "Assuming □(q ↔ ¬□q) as an axiom rather than deriving it",
        "Location": "Section 2, (Def) on page 3",
        "Explanation": "The paradoxical inconsistency arises only because the system is extended with the axiom schema □(p ↔ ¬□p). Since system T (and stronger) does not prove this schema, the result shows only that adding a self-referential axiom yields inconsistency, not that T itself is inconsistent."
      },
      {
        "Problem": "Mischaracterizing modal soundness proofs",
        "Location": "Section 3, page 5",
        "Explanation": "The paper claims that standard soundness proofs for T presuppose that each atomic proposition is consistent, and that an inconsistent q undermines those proofs. In fact, soundness proofs verify that axioms and rules preserve validity in all models; they do not assume consistency of arbitrary formulas, so established soundness results remain intact."
      },
      {
        "Problem": "Conflating object-level and meta-level notions of truth and necessity",
        "Location": "Section 4, page 6",
        "Explanation": "The discussion of semantic definitions (ii) and (iii) treats meta-language truth and necessity conditions as if they were object-language principles responsible for the paradox. However, these are external semantic descriptions not used in the formal derivation, so they do not show any flaw in pure modal logic."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 2838,
      "thinking": 2944,
      "output": 595
    }
  },
  {
    "entry_id": 35,
    "retraction_id": "2205.15802v2",
    "paper_id": "2205.15802v1",
    "retraction_comment": "The proof of Theorem 3 is wrong: in the display equation below Equation (22), bottom of page 15, the gradient of $\\phi_{t+1}$ is missing a factor $1/(\\alpha\\eta_t)$",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Misapplication of the black-box reduction to KT-OCO in the parameter-free extension",
        "Location": "Section 3, Algorithm 1 and Theorem 5",
        "Explanation": "The KT-OCO base algorithm requires that at each round the scalar ‘gradient’ st lies in [–1,1] almost surely. In the paper the authors only show that E[|st|]≤1 (or E[st^2]≤1), not that |st|≤1 for every realization. Since st=√N/L·⟨g_t, X˜ₜ(iₜ,:)⟩ and Schatten-1 constraints do not bound the Euclidean norm of X˜ₜ(iₜ,:), |st| can exceed 1, invalidating the regret guarantee of the reduction and hence Theorem 5."
      },
      {
        "Problem": "No control of per-task predictor Euclidean norms under the Schatten-1 constraint",
        "Location": "Section 3, proof of Theorem 5 (equations 23–24) and Algorithm 1",
        "Explanation": "The argument bounding |⟨g_t, X˜ₜ(iₜ,:)⟩| uses only ‖X˜ₜ‖_{S(1)}≤1, but this does not imply any bound on the Euclidean norm of individual rows X˜ₜ(i,:), which can be arbitrarily large. Without a uniform row-norm bound on X˜ₜ, the scalar gradients fed to KT-OCO are unbounded, breaking the key assumption of the parameter-free reduction."
      },
      {
        "Problem": "Unsupported online-to-batch conversion in the multitask setting",
        "Location": "Section 3, Theorem 6",
        "Explanation": "Theorem 6 applies a standard online-to-batch conversion to obtain excess risk bounds, but the online algorithm is run on a mixture of stochastic task activations and (potentially adversarial) losses. The conversion theorem cited assumes i.i.d. losses on each round, whereas here the sampling over tasks and samples is heterogeneous. No justification is given that the required i.i.d. conditions or bounded‐range conditions on the multitask loss are satisfied, so the excess‐risk conclusion is unsupported."
      },
      {
        "Problem": "Incorrect domain for the strong-convexity proof of the A-update",
        "Location": "Section 3, Proposition 2",
        "Explanation": "The proof of φ(X,·) being strongly convex invokes Juditsky & Nemirovski’s result on symmetric matrices over an open spectral interval (0,δ). However φ is defined on general (non-symmetric) A∈S₊₊ⁿ, and the mapping to the symmetric-matrix setting is not detailed. Moreover, strong convexity on an open spectral set lacks the boundary control typically required for FTRL analyses. This gap calls into question the validity of the A-update curvature bounds used in Theorem 3."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 30843,
      "thinking": 3968,
      "output": 661
    }
  },
  {
    "entry_id": 36,
    "retraction_id": "2006.05804v2",
    "paper_id": "2006.05804v1",
    "retraction_comment": "Lemma 2.2 is incorrect",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Invalid angular localization estimate",
        "Location": "Lemma 2.2 (page 8)",
        "Explanation": "The stated estimate ||P_{N,L} H_l u||_{L^2_{t,x}} ≤ N^{−1/2} L^{1/2} l ||P_{N,L} H_l u||_{L^2_{t,x}} is dimensionally inconsistent and contradicts known refined Strichartz/ω‐localized estimates.  In fact one expects a decay in l (e.g. l^{−1/2}) rather than growth, so as written the lemma either gives a trivial bound (if N^{−1/2} L^{1/2} l ≥ 1 always) or an incorrect loss/gain.  All subsequent bilinear estimates relying on this Lemma are invalidated."
      },
      {
        "Problem": "Divergent summation over angular frequencies",
        "Location": "Section 4.1 proof of (4.1) (pages 12–13)",
        "Explanation": "When applying Lemma 2.2 twice the argument produces factors l₁·l₂ and then sums over dyadic l₁≪l₂ (or vice versa).  Even with an angular weight σ≥1, the double sum ∑_{l₁<l₂} l₁^σ·l₁·l₂^σ diverges.  There is no mechanism in the paper to control these divergent angular sums, so the claimed bilinear bounds fail to close."
      },
      {
        "Problem": "Over‐optimistic recovery of derivative loss via Bernstein",
        "Location": "Section 5 (proof of (5.2)) (page 16)",
        "Explanation": "The use of Bernstein’s inequality to absorb one derivative in the trilinear term [∂φ, [A,φ]] loses too much regularity (going to L^∞ in space) and the subsequent ‘gain’ from Lemma 2.2 overcounts the frequency interactions.  The net effect is that the trilinear estimate cannot be bounded at the claimed regularity, invalidating the higher‐order term estimates."
      },
      {
        "Problem": "Insufficient control of non‐null bilinear form [φ,∂φ] at critical regularity",
        "Location": "Remark 4.1 (page 13)",
        "Explanation": "The term [φ,∂φ] lacks any null structure.  Direct application of standard bilinear estimates in X^{s,b} spaces only yields well‐posedness in H^1, and the proposed angular regularity fix (σ≥1) does not restore the missing 1/2 derivative in the critical s=1/2 regime.  The paper does not provide a valid mechanism to bound this term at scaling‐critical regularity."
      },
      {
        "Problem": "Oversimplified reduction of non‐abelian coupling",
        "Location": "Appendix 6.1 (page 18)",
        "Explanation": "The reduction from general su(n,C) to the n=1 case via the Fierz identity ignores the interplay of nontrivial structure constants in commutators like [A,ψγψ].  In fact these constants can affect multilinear estimates (through size and combinatorics of index sums), and treating all fields as scalar-valued glosses over potential growth in the number of components when n>1, undermining the non‐abelian well‐posedness claim."
      }
    ],
    "think_summary": [
      "**Identifying critical paper issues**\n\nThe user wants to examine the paper for critical errors that might invalidate its conclusions. I need to find up to five major problems, detailing each one with a \"Problem,\" \"Location,\" and \"Explanation.\" The paper under review is about the scaling critical regularity for certain systems, and I’m focusing on its use of function spaces with angular regularity. One significant question arises around the validity of specific lemmas concerning estimates, and whether the dependencies on certain factors are correct. This needs careful inspection.",
      "**Identifying critical flaws**\n\nThe user has pointed out assumptions regarding sigma finiteness and the consequences of summing over angular frequencies, which highlights a major flaw in the paper. They argue that incorrect estimates in Lemma 2.2 lead to divergence, invalidating key bilinear estimates. I'll produce a JSON list summarizing five critical problems, including their problem description, location, and explanations. Entries will cover issues such as misapplications of estimates, projections, and regularity concerns that ultimately affect the overall conclusions of the paper.",
      "**Highlighting significant issues**\n\nThe user is identifying a critical flaw regarding the non-null form [φ, ∂φ], noting that it cannot be accurately bounded with the given sigma weighting at s=1/2+. Precise estimates require s to be at least 1 or need additional structure, which implies local well-posedness below energy fails. Another significant concern is that reducing non-abelian structure simplifies coupling too much, ignoring essential contributions from non-trivial structure constants, which may invalidate estimates for cases where n>1."
    ],
    "token_usage": {
      "input": 21774,
      "thinking": 2432,
      "output": 758
    }
  },
  {
    "entry_id": 37,
    "retraction_id": "1708.02698v3",
    "paper_id": "1708.02698v2",
    "retraction_comment": "It turns out that there is an error in the argument for the curve case in Lemma 2.5 which makes the main result (Theorem 2.4) partially incorrect. We will post a modified version of the paper in which this is fixed. Meanwhile we withdraw the paper",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Application of Bertini’s irreducibility theorem to a non-proper map",
        "Location": "Lemma 2.5 (pp. 14–16)",
        "Explanation": "The proof of Lemma 2.5 invokes Jouanolou’s Bertini irreducibility (Thm 6.3(4)) to assert that for a general hyperplane H⊂P^N the inverse image φ⁻¹(H) is irreducible.  However φ: X^ss→P^N is obtained by restricting a proper map to the open semistable locus X^ss, so φ need not be proper or projective.  Standard Bertini irreducibility requires the domain to be projective (or at least φ to be proper), and without that hypothesis the general hyperplane section of a quasi-projective variety need not remain irreducible.  This gap undermines the construction of infinitely many height-one primes p."
      },
      {
        "Problem": "Unjustified identification of the final algebra as a semigroup algebra",
        "Location": "Theorem 2.4, conclusion (b') (pp. 16–17)",
        "Explanation": "The authors conclude that the final graded domain R^(d) is k[S] for some finitely generated semigroup S⊂N×Z^d simply because Proj R^(d) carries a torus action with a dense open orbit.  But having an open torus orbit does not by itself force the multigraded ring to be generated by one-dimensional weight spaces (i.e. monomials).  No argument is given that each combined (degree, weight) piece of R^(d) is one-dimensional, which is essential to present it as the semigroup algebra k[S]."
      },
      {
        "Problem": "Use of GIT semistability on Proj(R) without verifying O(1) is a line bundle",
        "Location": "Section 2.1 (pp. 10–12)",
        "Explanation": "The definitions of X^ss and X^s, and the construction of the GIT quotient X//G, assume a G-linearized ample line bundle.  But on X=Proj(R) the Serre sheaf O_X(1) need not be a line bundle unless R is generated in degree 1.  Although later one Veronese-re-embeds (R→R[m]) to remedy this, Lemma 2.5 and the initial steps do not explicitly justify passing to a genuine linearization.  Without ensuring that O_X(1) (or some O_X(m)) is very ample and G-linearized, standard GIT stability notions may fail."
      },
      {
        "Problem": "Possible triviality of the invariant ring R^G invalidates Lemma 2.5’s setup",
        "Location": "Lemma 2.5 hypothesis (p. 14)",
        "Explanation": "Lemma 2.5 requires the existence of infinitely many nonconstant homogeneous invariants f∈R^G to produce principal G-stable hypersurfaces.  But in many torus actions one can have R^G=k (no nonconstant invariants), in which case X//G is a point and there is no family of hyperplanes in Proj(R^G).  The paper does not rule out at each step the possibility that R(i)^T_i=k, so Lemma 2.5 may become vacuous and the induction cannot proceed."
      },
      {
        "Problem": "The DVR assumption in Theorem 1.6 is not guaranteed for the primes produced later",
        "Location": "Theorem 1.6 (p. 8) and its use in Section 2.2",
        "Explanation": "Theorem 1.6 requires that the chosen height-one prime p satisfy that the localization R_p is a discrete valuation ring.  In the main construction p is one of the minimal primes over the pullback of a general invariant hyperplane, but Lemma 2.5 does not ensure that any of these p have R_p normal (hence DVR).  Without that, the Rees algebra A_v(R) need not be finitely generated and the flat degeneration step fails."
      }
    ],
    "think_summary": [
      "**Analyzing prime height**\n\nI need to ensure that p has a height of one. Lemma 2.5 provides the p radical of a principal ideal. In a Noetherian domain, the p radical of a principal ideal implies height one if the ring is integrally closed. While A is a finitely generated domain, it may not be normal. If height(p) is necessary for valuation, they assume height 1 for p, despite not checking it directly. Given that minimal primes over fA have height 1, along with the principal ideal theorem, I conclude height(p) equals 1.",
      "**Evaluating proper morphisms**\n\nIt seems the composition of proper and open immersion is not necessarily proper. When I restrict φ to Xss, the proper morphism's restriction to an open subset remains non-proper. The map from Xss to P^N is indeed not proper. Jouanolou's theorem appears to require properness, but I need to confirm whether generalized Bertini requires it too. The implications of applying Bertini need careful consideration, especially regarding the hyperplane intersections and the stable locus being nonempty in the context of moment polytopes. There seem to be complexities with prime avoidance and the behaviour of functions at intersections as well.",
      "**Examining prime ideals and valuation**\n\nThey've chosen f to avoid minimal primes in A, which is a graded domain, ensuring f is a nonzerodivisor on A/(πe#(g)). This leads to each Yi intersecting Xf. However, the assumption that infinitely many prime ideals exist might be a major issue. We actually need at least d primes for d steps. The lemma won’t constrain p to height 1, though minimal primes over (f) will be height 1. There's a subtlety in the methodology of using Rees algebras to ensure that the final algebra remains finitely generated.",
      "**Addressing GIT Linearization**\n\nI’m looking at GIT linearization issues in Section 2.1, specifically regarding RG being trivial and the preamble for Lemma 2.5. In addition, Theorem 1.6 relies on the DVR assumption presented in Section 1.3. I should carefully review these sections to ensure everything aligns properly. It's essential to prepare the JSON format accurately given these connections! I wonder if I’m missing any details that could affect the overall structure. Let's make sure everything is comprehensively organized."
    ],
    "token_usage": {
      "input": 15453,
      "thinking": 8640,
      "output": 929
    }
  },
  {
    "entry_id": 38,
    "retraction_id": "1201.3873v3",
    "paper_id": "1201.3873v2",
    "retraction_comment": "This paper has been withdrawn by the authors due to a crucial gap in the estimates for m>=4",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Restriction to two‐variable base polynomials without justification",
        "Location": "Sections 2–4 (construction of Pₘ as (P₂)^{m/2})",
        "Explanation": "All lower bounds Dₘ ≥ fₘ(...) are obtained by taking Pₘ(z)=P₂(z₁,z₂)^{m/2}. There is no proof that no other m-homogeneous polynomial in higher dimensions can produce a larger ratio, so it is unjustified to assume this two‐variable family is extremal. If a different family yields higher lower bounds, the claimed exponential growth and conclusions about lim Dₘ/Dₘ₋₁ would be invalid."
      },
      {
        "Problem": "Incomplete exploration of norm formula branches in Proposition 1",
        "Location": "Proposition 1, Section 2",
        "Explanation": "When optimizing f₂(a,b,c) the authors only consider the case ab<0 (second branch of the norm formula) and ignore the region ab≥0 (first branch). Without checking that the first branch cannot produce a larger f₂, the reported maximum f₂≈1.1066 may be suboptimal, invalidating the lower bound D₂≥1.1066."
      },
      {
        "Problem": "Unjustified reliance on an external norm formula",
        "Location": "Proposition 1",
        "Explanation": "The formula for ∥a z₁²+b z₂²+c z₁z₂∥_{ℓ_∞^2} is taken from [1] without full proof or verification of applicability to complex variables. A misapplication or error in that formula propagates through all subsequent lower‐bound computations, undermining all numeric estimates for Dₘ."
      },
      {
        "Problem": "No rigorous verification that numerically found extrema are global maxima",
        "Location": "Sections 2–4 (optimizations of f₂, g₄, g₆)",
        "Explanation": "The paper states ‘a straightforward examination shows’ the reported maxima but provides no detail, error bounds, or proof of global optimality. Without a rigorous argument or certification, the numeric lower bounds could be incorrect or only local maxima."
      },
      {
        "Problem": "Unproved extrapolation from finite m to m→∞ behaviour",
        "Location": "Section 5",
        "Explanation": "The key conclusion that lim Dₘ/Dₘ₋₁≠1 is based on lower estimates computed only up to m=14 and a heuristic graph. No analytic argument shows these exponential lower bounds persist for all m, so the claim that no hypercontractivity sequence with ratio→1 exists is not rigorously established."
      }
    ],
    "think_summary": [
      "**Analyzing mathematical constant**\n\nI'm diving into an equation that relates to Dm and its behavior as m grows larger. For large m, it seems Dm approximates to e multiplied by a factor involving square roots and powers of two. However, there's confusion around the constant C, which is mentioned to be 1.8529, despite my findings suggesting other values. The derivation looks at upper bounds for small m and combines those with results for larger m to find a maximum. It appears m=5 yields the highest supremum, aligning with the value stated.",
      "**Examining mathematical bounds**\n\nI've established that at m=5, C equals 1.8529, which seems consistent with equation (1.3) holding true across certain values. But I'm questioning the assumptions made about variables a and b, particularly their signs. They restricted a and b to be negative symmetric but didn’t explore what might happen if both were positive. This oversight could lead to missed optimum values. I need to check both regions thoroughly, as ignoring the first branch could be a critical gap in their analysis."
    ],
    "token_usage": {
      "input": 10268,
      "thinking": 3968,
      "output": 620
    }
  },
  {
    "entry_id": 39,
    "retraction_id": "1006.2189v2",
    "paper_id": "1006.2189v1",
    "retraction_comment": "This paper has been withdrawn by the authors. The proof of the verification of axiom 1 for the smoothing functor that is given in the paper is false, since it would violate what is known in dimension 4. If U is a subset of V and both are diffeomorphic to R^4, then the restriction map of smoothing spaces sm(V) -> sm(U) need not be one-to-one on path components. Thus axiom 1 is violated in dimension 4. The verification of axiom 1 in higher dimensions is probably a consequence of the product structure theorem",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [],
    "think_summary": [],
    "token_usage": {
      "input": 9239,
      "thinking": 4416,
      "output": 7
    }
  },
  {
    "entry_id": 40,
    "retraction_id": "1207.1467v2",
    "paper_id": "1207.1467v1",
    "retraction_comment": "This paper has been withdrawn due to a logica error in equation 1",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Incorrect dismissal of the Bird–Lightfoot–Stewart (BLS) nonlinear Stefan-flow model as spurious “enhanced diffusion”",
        "Location": "Section 3.1",
        "Explanation": "The authors assert that neglecting the inert‐species velocity in BLS leads to a false diffusion enhancement and that the BLS frame of reference is “unachievable.” In fact, BLS’s nonlinear correction (Stefan flow) is a physically valid consequence of mass conservation in a binary mixture and can be analyzed in any inertial frame. The criticism conflates coordinate choice with physical validity and thus misinterprets the classical model."
      },
      {
        "Problem": "Flawed momentum closure yielding invalid reduction to linear diffusion",
        "Location": "Section 3.1, Equation (3.12)",
        "Explanation": "Equation (3.12) purports to express the dry‐air velocity v_ga in terms of ∇ρ_gv and the bulk velocity vg using a simplistic momentum balance (C_gv v_gv,g + C_ga v_ga,g = 0). This misuses definitions of diffusive vs. absolute velocities and neglects the proper species momentum equations, leading to an incorrect removal of the Stefan‐flow nonlinearity and an unwarranted return to a purely linear diffusion model."
      },
      {
        "Problem": "Unphysical thermal‐gradient model lacking energy‐mass coupling",
        "Location": "Section 3.3",
        "Explanation": "The imposed temperature field T(ξ) is prescribed a priori without solving the energy equation or accounting for latent‐heat effects at the evaporating interface. This decoupling of heat and mass transfer renders the predicted “thermal enhancement/dampening” of diffusion ungrounded, since real capillary‐tube problems exhibit strong coupling between interfacial evaporation and temperature."
      },
      {
        "Problem": "Erroneous neglect of natural convection via mis‐scaled momentum equation",
        "Location": "Section 4 (Equations (4.14)–(4.17))",
        "Explanation": "The dimensionless groups in the simplified Navier–Stokes analysis are computed without reference to the Rayleigh or Grashof numbers, and viscous and buoyancy forces are dismissed based on incorrect order‐of‐magnitude estimates. This leads to the unsound conclusion that natural convection in the capillary tube is always negligible, ignoring regimes where density‐driven flows can be important."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 15841,
      "thinking": 2112,
      "output": 554
    }
  },
  {
    "entry_id": 41,
    "retraction_id": "1503.03121v2",
    "paper_id": "1503.03121v1",
    "retraction_comment": "This paper has been withdrawn by the author because the duality is indefensible speculation",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "No physical mechanism for jettisoning matter and entropy at turnaround",
        "Location": "Section 1 (CBE assumption)",
        "Explanation": "The model assumes that all matter (including black holes) and associated entropy can be ‘jettisoned’ into the extroverse at turnaround simply by accelerated expansion. However, the cosmological event horizon is not a physical membrane that expels matter; matter crossing the horizon remains part of the global spacetime and its entropy remains in the universe. Without a concrete mechanism to physically separate or remove entropy-carrying matter, the key CBE assumption is unjustified."
      },
      {
        "Problem": "Ad hoc redefinition and matching of the scale factor without dynamical justification",
        "Location": "Section 2 (Eqs. (17)–(21))",
        "Explanation": "The contraction scale factor is defined by ˆa(t)=f(t_T) a(t) at turnaround and then matched to the expansion solution at t_m by choosing t_T to satisfy ˆa(t_m)=a(t_m). No derivation from the Einstein–Friedmann equations is provided, nor is continuity of the Hubble parameter or its derivative ensured. This arbitrary rescaling lacks dynamical underpinning and fine-tunes the cyclic period, undermining the model's consistency."
      },
      {
        "Problem": "Unjustified assumption of radiation-dominated contraction dynamics",
        "Location": "Section 2 (Eq. (20))",
        "Explanation": "The paper assumes that during contraction the introverse is radiation-dominated with a(t)∝t^{1/2}, ignoring the effects of curvature and any residual dark energy component. In a contracting FRW universe, curvature and vacuum energy scale differently and typically dominate at small scale factors, altering the dynamics. Without solving the full Friedmann equations with the stated energy content, the assumed contraction law is unsupported."
      },
      {
        "Problem": "Neglect of entropy production during contraction phase",
        "Location": "Section 2 (Eqs. (20)–(21))",
        "Explanation": "Even if matter is removed, the retained radiation will blue-shift during contraction, increasing its entropy density and total entropy in the causal patch. The model does not address how this growing radiation entropy is eliminated before the bounce, contradicting the claim of a low-entropy introverse in each cycle and violating the second law unless an additional mechanism is introduced."
      },
      {
        "Problem": "Absence of bounce or turnaround physics",
        "Location": "Sections 2–4 (throughout model description)",
        "Explanation": "The transition from contraction back to expansion (the bounce) is assumed to occur smoothly but no mechanism—classical, quantum, or otherwise—is provided to resolve the singularity or reverse the collapse. Without a specified bounce theory, infinite cyclicity cannot be demonstrated, and the model's core claim of a perpetual cycle remains conjectural."
      }
    ],
    "think_summary": [
      "**Identifying key problems**\n\nI'm identifying some critical issues related to the current Cosmological Bounce model. Firstly, there's an absence of a physical mechanism for removing matter and entropy during turnaround, which is discussed in section 1. Secondly, the model benefits from a generalized assumption of scale factors without properly resolving the Friedmann equations—again in section 2. Thirdly, ignoring entropy production during contraction raises concerns about violating the second law of thermodynamics. Finally, the use of a cosmic event horizon as a boundary to discard entropy is a flawed approach discussed in section 1. This highlights significant gaps in the study."
    ],
    "token_usage": {
      "input": 4669,
      "thinking": 1920,
      "output": 662
    }
  },
  {
    "entry_id": 42,
    "retraction_id": "2405.12710v3",
    "paper_id": "2405.12710v2",
    "retraction_comment": "The author has withdrawn this paper due to a critical definitional error in concept learning for global/local-interaction learning during training. This error led to an alignment issue with the definition of the text-video retrieval task, causing an unfair comparison with state-of-the-art (SOTA) methods. Consequently, this hindered the accurate evaluation of the paper's contributions",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Misleading computational cost comparison",
        "Location": "Fig. 1, Table I, Sec. I (pp. 1–2), Sec. IV-B (pp. 5–6)",
        "Explanation": "The paper claims a ~220× reduction in FLOPs by comparing only the similarity-head computations across methods. This ignores the cost of the proposed Local Interaction Module (LIM) and Global Interaction Module (GIM) (i.e., the transformer blocks, attention operations, and query processing), as well as any overhead for computing their outputs during inference. Measuring only the final dot-product head severely underestimates true model complexity and leads to an invalid efficiency claim."
      },
      {
        "Problem": "Incomplete complexity analysis of the proposed method",
        "Location": "Sec. I (last paragraph, p. 2), Sec. V (Complexity discussion), Table V (p. 8)",
        "Explanation": "Although the authors derive O(NtNv(1+Nq)) for retrieval scoring, they omit the cost of applying LIM’s transformer blocks to each text and video sample (i.e., O(Nq · Nf) and O(Nq · Nw) per sample). This non-negligible overhead is neither reported in FLOPs nor in inference-time measurements, invalidating the claimed overall retrieval efficiency."
      },
      {
        "Problem": "Lack of ablation for the global interaction module (GIM)",
        "Location": "Sec. III-B (p. 4), Sec. IV-C (Ablation Study, p. 7)",
        "Explanation": "The paper introduces GIM as a key component for coarse alignment but does not include an experiment that removes or replaces GIM to quantify its standalone impact. Without a 'no-GIM' baseline, it is unclear whether GIM materially contributes to retrieval accuracy or if most gains derive from LIM and the added losses, undermining the claimed novelty and effectiveness of GIM."
      },
      {
        "Problem": "Inconsistent use of normalized and unnormalized representations in loss terms",
        "Location": "Sec. III-D (Eq. 6–8, p. 4)",
        "Explanation": "The inter-consistency loss mixes an ℓ2 distance on raw concept vectors cᵥ, cₜ (unnormalized) with a squared hinge on their raw dot product, while retrieval scoring uses cosine similarities of ℓ2-normalized cᵥ, cₜ. This mismatch can lead to conflicting optimization objectives (e.g., scaling magnitudes vs. aligning directions), yet no normalization or balancing strategy is discussed, calling into question the stability and validity of the training objectives."
      },
      {
        "Problem": "Insufficient validation of generalization across varied video datasets",
        "Location": "Sec. IV-B(c) (pp. 6–7), Table II (p. 6)",
        "Explanation": "The method fixes hyperparameters (e.g., number of queries Nq, loss weights α/β) across five diverse benchmarks but exhibits weak or negative gains on ActivityNet, a dataset with long, complex videos. This one-size-fits-all setup suggests overfitting to MSR-VTT’s characteristics and undermines the paper’s claim of broad effectiveness and efficiency."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 27939,
      "thinking": 2752,
      "output": 748
    }
  },
  {
    "entry_id": 43,
    "retraction_id": "1305.5284v3",
    "paper_id": "1305.5284v2",
    "retraction_comment": "This paper has been withdrawn by the author due to missing phase-space factor in eq(1)/(2), thus the whole structure is wrong",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Flawed thermodynamic argument for pure gluon dominance",
        "Location": "Page 2, Eqs. (1)&(2) and subsequent paragraphs",
        "Explanation": "The author uses the infrared divergence of the Bose-Einstein distribution at E/T→0 to argue that gluons vastly outnumber quarks at high temperature, implying a pure gluon equilibrium phase. In reality, the gluon distribution infrared divergence is regulated by interactions and collective effects, and thermal quark–antiquark pair production at high T is copious. A thermalized system at high temperature contains significant quark content, so one cannot sustain a pure-gluon plasma as proposed."
      },
      {
        "Problem": "Assumption of zero photon/dilepton emission from 'Glasma' phase",
        "Location": "Page 2, paragraph beginning “Now let’s high temperature…”",
        "Explanation": "The claim that a gluon-only plasma emits no photons or dileptons ignores processes such as gg→gγ via quark loops and rapid quark–antiquark pair production in a hot QCD plasma. Even a gluon-dominated system produces quarks on short timescales, leading to nonzero electromagnetic emission. Thus the proposed complete suppression of EM signals is unphysical."
      },
      {
        "Problem": "Invalid application of hydrodynamics to non-equilibrium Glasma",
        "Location": "Page 3, section “how to modeling the system now?”",
        "Explanation": "The paper treats the Glasma as if it were an equilibrated fluid and applies hydrodynamics with an equation of state from lattice QCD. However, the Glasma is a far-from-equilibrium classical color field state in the CGC framework, not a thermalized medium. Using hydrodynamics to describe flow build-up and EM emission fractions in this phase is unjustified and undermines the conclusions about enhanced photon v2."
      },
      {
        "Problem": "Unsupported extrapolation to cosmic dark matter and dark energy",
        "Location": "Pages 3–4, paragraphs on astrophysics and cosmology",
        "Explanation": "The extension of a heavy-ion collision Glasma to a cosmic-scale core providing dark matter and driving accelerated expansion lacks any theoretical or observational basis. A hot, strongly interacting QCD plasma would not behave as collisionless dark matter, nor produce the negative pressure required for dark energy. This speculative leap invalidates the cosmological claims."
      },
      {
        "Problem": "Incorrect critique of existing photon/dilepton emission rates",
        "Location": "Page 3, paragraph on emission rates and classical approximation",
        "Explanation": "The author asserts that current thermal emission rates overestimate high-T emission by using a classical exp(−E/T) approximation, but state-of-the-art rates (e.g. AMY) include full Fermi–Dirac and Bose–Einstein factors, medium effects, and LPM suppression. The claimed large overestimate is not supported, so the suggested missing emission in Glasma cannot be remedied simply by replacing distributions."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 4198,
      "thinking": 1024,
      "output": 678
    }
  },
  {
    "entry_id": 44,
    "retraction_id": "2307.11176v2",
    "paper_id": "2307.11176v1",
    "retraction_comment": "There is an irrecoverable error in Lemma 2.5. There are counterexamples even in case R=Q[x]. The lemma is crucial for the rest of the paper and it does not work unless strong assumptions are made (like: the modules are graded)",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Unstated and potentially false noetherian hypotheses on R[w] and R[w, w^{-1}]",
        "Location": "Introduction / Lemma 6.3",
        "Explanation": "The paper assumes only that R is noetherian, but in the noncommutative case does not specify left‐noetherian or that the indeterminate w is central. Many finiteness arguments (e.g. that R[w] and its localization are noetherian, Lemma 2.5, Lemma 6.1, Lemma 6.3) rely on polynomial and Laurent extensions preserving (left) noetherianity. Without explicit hypotheses that R is left noetherian and w central, these extensions need not be noetherian, invalidating the finiteness and torsion arguments."
      },
      {
        "Problem": "Unjustified use of the Artin–Rees lemma in the noncommutative setting",
        "Location": "Lemma 6.3",
        "Explanation": "The proof invokes Artin–Rees for the ideal (w) in R[w] without a reference that this holds when R is noncommutative but (w) central. The standard Artin–Rees requires commutativity; extending it to central ideals in a noncommutative noetherian ring is nontrivial and not established, so the key step w F_iK = F_{i+1}K for large i may fail."
      },
      {
        "Problem": "Gap in splitting exact sequence 0→F₁M→F₀M→N→0 as R-modules",
        "Location": "Lemma 3.5 (proof of Proposition 3.2)",
        "Explanation": "The argument asserts a decomposition F₀K = A⊕B mapping into summands of R[w]^r and thereby constructs an R-linear splitting N→F₀M. However, no justification is given that the submodule F₀K of the direct sum R[w]^r actually splits as a direct sum of its intersections with the two summands. This unproven split is essential to produce the filtered isomorphism M≅gr(M), so its failure would undermine Proposition 3.2 and all subsequent results depending on M≅gr(M)."
      },
      {
        "Problem": "Implicit assumption that w is central",
        "Location": "Definition 1.2 and throughout",
        "Explanation": "The constructions R[w] and R[w,w^{-1}] as polynomial and Laurent extensions implicitly require w to commute with R. This centrality is used critically for localization, flatness (R[w,w^{-1}] over R[w]), and Artin–Rees. Yet the paper never states that w is central, leaving the ring structure and many arguments ambiguous or invalid in a genuinely noncommutative setting."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 13298,
      "thinking": 8256,
      "output": 640
    }
  },
  {
    "entry_id": 45,
    "retraction_id": "0804.4876v3",
    "paper_id": "0804.4876v2",
    "retraction_comment": "A counterexample to Theorem 1.2 has been pointed out to the author (x^2+3 reduced modulo 2). The mistake cannot be corrected at this time",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [],
    "think_summary": [
      "**Evaluating critical problems**\n\nI'm tasked with identifying up to five critical issues that could invalidate the conclusions in the paper, while ignoring minor concerns. I need to thoroughly scan the paper, focusing on the main contributions, which include several significant theorems and propositions. For instance, I’ll dig into Theorem 1.2, checking the proof in Section 4. It heavily relies on group actions and known facts, and I must assess if the justification for the bijection from R to H\\G is solid.",
      "**Analyzing conditions for theorems**\n\nI see that the map sending α*g to coset Hg is known, but I wonder if there are hidden conditions, like the polynomial being monic. It's assumed that OK is the ring of integers in K, and that α is integral in OK. They don't require OK = Ok[α], which simplifies things. However, I need to check if the reduction map R -> R bar is defined, especially given the assumptions about factorization mod p. \n\nI notice the theorem states the factorization of c modulo p has distinct irreducible factors, but it seems they've removed any unramified conditions. While that's interesting, I wonder if the implications stand without those assumptions about p being unramified in K.",
      "**Checking factorization patterns**\n\nI need to confirm if the factorization patterns presented are complete. For A5, the cycle types seem to match, including various forms like 1-cycles and transpositions. For Hol(Z5) (F20), the cycle structure involves elements acting on five objects, and I need to verify details about automorphisms. \n\nThe patterns for D5 and A5 also appear correct, including reflections and transpositions. Theorem statements and the concept of “division” as related to cyclic subgroups seem well-founded too. Now, I’ll check any assumptions tied to unramified primes and their implications on residual degrees."
    ],
    "token_usage": {
      "input": 10981,
      "thinking": 14272,
      "output": 7
    }
  },
  {
    "entry_id": 46,
    "retraction_id": "2105.09970v2",
    "paper_id": "2105.09970v1",
    "retraction_comment": "The proof of the main Lemma (3.11, section 3.4) is incomplete: in the middle of page 22, the fact that $\\gamma$ is weakly distributive is not sufficient to justify the chain of two inclusions used to invoke Proposition 2.1",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Unjustified division of arbitrary weakly distributive forest algebras into linear ones",
        "Location": "Proposition 3.3",
        "Explanation": "The authors claim that every weakly distributive forest algebra G “divides” the linear distributive algebra [τ]^(sticks(G)), and from that deduce that G recognizes only leaf-path languages. However, they never exhibit the required subalgebra and quotient homomorphism witnessing this division, nor do they check that [τ]^(sticks(G)) really satisfies the forest-algebra distributive axioms (as opposed to the spa axioms). Without a concrete division construction, the equivalence between weak distributivity and recognition of leaf-path languages is unproven."
      },
      {
        "Problem": "Flawed spa→forest­-algebra conversion undermining the variety framework",
        "Location": "Section 3.1 (spa to forest­-algebra paragraphs)",
        "Explanation": "To recover a forest­-algebra variety for weak distributivity the paper converts each spa H into a forest algebra H₀ by ‘adding’ the empty forest and trivial context. This ad hoc construction destroys closure under context-quotients and inverse homomorphisms (since H₀ can no longer recognize the empty forest and its vertical monoid gains no identity), yet the authors proceed as if the resulting H₀’s still form a bona fide variety. This gap invalidates every subsequent argument that relies on standard variety-theoretic tools."
      },
      {
        "Problem": "Over­-simplified recursion for the ideals µₙ may yield incorrect Mᵅₙ",
        "Location": "Section 3.5, Equation (6)",
        "Explanation": "The formula\n\n    µₙ(a x δₙ₋₁(s)) = I[ α(aΔ) ( µₙ(x δₙ₋₁(s)) ∩ νⁿ₋₁(s) )]\n\nis supposed to characterize all G-ideals arising from extending a stick x by a new letter a.  But the derivation ignores subtle interactions between the relabeling δₙ₋₁(·) and the choice of embedding forest u:  some u with α(u) in the intersection may nonetheless carry extra structural information that changes δₙ₋₁(u) and thus should not contribute.  As stated, (6) over-approximates the true set of ideals, so the algorithm could compute the wrong Mᵅₙ and hence fail to detect membership correctly."
      },
      {
        "Problem": "Nonconstructive, possibly invalid factorization of α in the Simulation Lemma",
        "Location": "Lemma 3.11",
        "Explanation": "The key step is to assume that any given α: AΔ→G factors as sequential compositions of weakly‐distributive homomorphisms matching some wreath-product decomposition of G∈*D, but the paper never shows how to extract such a factorization from α itself.  Existence of a wreath factorization of G does not automatically give a decomposition of the specific α under study.  Without a concrete procedure to factor α, the inductive argument that ‘G divides Dᵅₙ’ is unsound."
      },
      {
        "Problem": "Unsupported identification of spa- and forest-algebra wreath products",
        "Location": "Section 3.1, paragraph ‘Wreath products of spas’",
        "Explanation": "The authors assert without proof that the forest algebra attached to the wreath product of two spas recognizes exactly the same forest languages as the wreath of the two corresponding forest algebras.  Their brief sketch matches only the relabeling maps but omits any verification that the horizontal monoids and vertical actions coincide or even that the resulting homomorphisms line up on ±0.  This missing isomorphism undermines the foundational step equating the spa-based variety *D with the forest-algebra variety they build."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 29142,
      "thinking": 6784,
      "output": 907
    }
  },
  {
    "entry_id": 47,
    "retraction_id": "1508.06018v3",
    "paper_id": "1508.06018v2",
    "retraction_comment": "This paper has been withdrawn by the authors due the fact that the main results Proposition 4.1 and Theorem 4.8 are not correct. Anonymous reviewers notice, that In the former the set B\\A is not necessarily nonempty while, in the latter, the formulation and proof are unclear",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Use of divergence theorem/density propagation without ensuring boundary terms vanish",
        "Location": "Assumption 4.5 and the proof of Theorem 4.6 (Section 6.2)",
        "Explanation": "The argument that ∫_{X(t,Z,u)} div(ρf) ≥ 0 ⇒ ∫_{X(t,Z,u)} Q(x) dx = 0 implicitly uses the divergence theorem on a non-compact, time-varying domain without verifying that ρ vanishes on the boundary or decays at infinity. Since supp ρ ⊃ D is not assumed compact and trajectories may reach ∂D, boundary terms need not vanish, invalidating the conclusion that the ‘bad’ set Z has zero Lebesgue measure."
      },
      {
        "Problem": "Unverifiable existence of the D-path satisfying Assumption 3.1",
        "Location": "Assumption 3.1 (Section 3)",
        "Explanation": "The main small-gain–density combination hinges on the existence of a continuous, strictly increasing mapping σ:[a,b]→[M, M¯] satisfying the strict inequality Γ_η(σ(r))≺σ(r) and derivative bounds in (3.2). No general constructive conditions or algorithms are provided to find such σ, and no checkable criteria are given, making the core assumption effectively vacuous in practice."
      },
      {
        "Problem": "Insufficient justification for the small-input bound δ_u in Proposition 4.1",
        "Location": "Proof of Proposition 4.1 (end of Section 6.1)",
        "Explanation": "The proof invokes existence of δ_u>0 so that V(x)≥max_i φ⁻¹(γ_{iu}(|u|)) implies the desired derivative negativity, citing positive invariance and local attractivity. However, the transition from global attractivity of A to a uniform input bound δ_u is only sketched and lacks a rigorous argument that (4.2a) holds uniformly for all u with |u|_∞<δ_u."
      },
      {
        "Problem": "Use of discontinuous, non-class-K∞ gain functions in the illustrative example",
        "Location": "Section 5, definitions (5.1) and Claims 5.1–5.2",
        "Explanation": "The example constructs gains I_p and I_g that are discontinuous at zero (and only strictly increasing elsewhere), then patches them into class-K∞ functions by hand. This violates the standard ISS-gain requirement of continuity at zero and calls into question the validity of applying small-gain and density arguments with such irregular gains."
      },
      {
        "Problem": "Nonconstructive state-space cover in Theorem 4.8",
        "Location": "Section 4.1, Theorem 4.8 and preceding definitions of A_j, B_j, S_j",
        "Explanation": "Theorem 4.8 requires partitioning the state space into countably many regions where either the small-gain or density conditions hold, with intricate inclusions S_j⊂cl D_j. No systematic method is offered to identify these regions or verify the cover has no gaps, rendering the main global stability result non-implementable."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 32082,
      "thinking": 1856,
      "output": 733
    }
  },
  {
    "entry_id": 48,
    "retraction_id": "1310.8403v3",
    "paper_id": "1310.8403v2",
    "retraction_comment": "This paper has been withdrawn as a bug has been discovered in the proof of Claim 5",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Non-existence of a minimizer Pn",
        "Location": "Section 1.2, paragraph defining “MIN-MAX point set” (page 2)",
        "Explanation": "The authors posit Pn = arg minP A(P) over an uncountable domain without proving that A(P) attains its infimum. Since A(P) is defined via a supremum over packings, continuity or compactness arguments are needed to ensure a minimizer exists, but none are given. The extremal-set argument thus lacks a valid base case."
      },
      {
        "Problem": "Invalid WLOG assumption that every point has its own rectangle",
        "Location": "Section 1.2, last paragraph (page 2)",
        "Explanation": "They assert that if a point pi has no anchored rectangle in R(Pn), one can split a neighboring rectangle to introduce one at pi without reducing total area. No proof is offered that such a split always preserves area or disjointness, making the claim unsound and undermining the structure of R(Pn)."
      },
      {
        "Problem": "Unjustified local modification in Claim 3",
        "Location": "Lemma 2 proof and Claim 3 (pages 3–5)",
        "Explanation": "The argument moves a single point pi to pi' and assumes all other rectangles Rj (j≠i) remain valid in the new optimal packing for Qn. There is no justification that the global optimum for Qn differs only in Ri; other rectangles could reconfigure, so the inequality A(Qn)<A(Pn) is not rigorously established."
      },
      {
        "Problem": "Unsupported distinct-coordinate assumption",
        "Location": "Section 2.2, opening sentence (page 3)",
        "Explanation": "The proof assumes without justification that in a MIN-MAX set no two points share an x- or y-coordinate. Degenerate cases with shared coordinates could be extremal, but the authors neither eliminate nor handle them, leaving a gap in the characterization of minimizers."
      },
      {
        "Problem": "Flawed perturbation in Lemma 4",
        "Location": "Lemma 4 proof (page 6)",
        "Explanation": "To force the same bounding point for V(p1) and H(p1), the authors propose moving two points to pi', pj' so that only V and H change area. They ignore possible cascading effects on other rectangles in the packing. Without controlling these global interactions, the claimed strict decrease A(Qn)<A(Pn) is unsubstantiated."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 5309,
      "thinking": 1536,
      "output": 584
    }
  },
  {
    "entry_id": 49,
    "retraction_id": "2003.05237v3",
    "paper_id": "2003.05237v2",
    "retraction_comment": "There is an issue in the proof of Lemma 3.7: evaluation of the involved classes gives back the same values for the coinvariants and not necessarily for the coefficients themselves. At the moment I do not see how to fix it. The lemma is needed in the proof of the main Theorem",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [],
    "think_summary": [],
    "token_usage": {
      "input": 25280,
      "thinking": 2560,
      "output": 7
    }
  },
  {
    "entry_id": 50,
    "retraction_id": "2203.03600v2",
    "paper_id": "2203.03600v1",
    "retraction_comment": "The main technical result, Lemma 4, has a major error in the proof: The claim in the proof \"... we could decompose $y^i$, and therefore $y$ into at least two sign-compatible, non-zero cycles of $\\mc A$ ...\" is NOT true. This claim is based on our claim in Lemma 3 that the decomposition of cycles $y^i$ into bricks $y^{i^j}$ yields cycles $y^{i^j}$ of the N-fold matrix $\\mc A$. This is not true",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Invalid decomposition in Lemma 3: zeroing Graver vectors breaks the kernel property",
        "Location": "Lemma 3 (Pages 6–7)",
        "Explanation": "The proof of Lemma 3 decomposes each brick y_i^(j) into Graver basis elements g_k of B(j), then defines y_i^(j)_k by zeroing all coordinates outside the support of partition P_i. However, zeroing out entries of an integer cycle of B(j) generally destroys the relation B(j)·y=0, so y_i^(j)_k need not lie in ker B(j). This invalidates the construction of the vectors v_ℓ (and hence the use of Steinitz’s Lemma) and thus the claimed ℓ₁‐norm bound on Graver basis elements of the full N-fold matrix."
      },
      {
        "Problem": "Misestimated partition parameter p_B for scheduling with release times/deadlines",
        "Location": "Section 5.1.2, Corollary 4 (Page 9)",
        "Explanation": "When adding constraints for job start times and release‐time/deadline ordering, each B(i) block acquires O(d) rows whose supports overlap on many x‐ and s‐variables. The paper continues to assume p_B=1 (as in the basic Q||C_max formulation), but these overlapping supports force p_B=Ω(d). As a result, the bound g₁(A)=S_A·(p_A·p_B·Δ)^{O(p_A·p_B)} and the consequent runtime blow up by factors exponential in d, invalidating the stated (p_max)^O(d)·m·log m·log^2 n bound."
      },
      {
        "Problem": "Unjustified application of halving augmentation to separable convex objectives",
        "Location": "Section 5.1.1, Corollary 2 (Page 8)",
        "Explanation": "The paper extends the linear‐objective halving augmentation framework of Eisenbrand et al. to the separable convex cost function f(x,z) for Q||∑w_jC_j by citing [6, Theorem 77], but that result only treats linear objectives (or requires additional subroutines for convex costs). No adaptation or verification is given that the halving step can be computed and analyzed in the same time for general separable convex f, so the claimed runtime for the weighted completion‐time objective is unsupported."
      },
      {
        "Problem": "Corollaries omit exponential factors in the partition parameters, understating complexity",
        "Location": "Corollaries 1–6 (Pages 8–11)",
        "Explanation": "The general runtime from Theorem 1 is nt·log(nt)·L·(S_A)^{O(r+s)}·(p_A·p_B·Δ)^{O(r p_A p_B + s p_A p_B)}, which for scheduling yields factors like d^{Θ(d)} or 2^{Θ(d log d)}. Yet in all corollaries these are collapsed to (p_max)^O(d)·poly(m,n), with no explicit mention of the hidden partition‐dependent overhead. This significantly overstates the practical improvement in the parameter d and obscures the true double‐exponential‐type behavior in d."
      },
      {
        "Problem": "Incorrect partitioning of B-matrices in the Minimum Sum Coloring formulation",
        "Location": "Section 5.3, Corollary 6 (Page 11)",
        "Explanation": "In the N-fold IP for Minimum Sum Coloring, each B(i) block encodes constraints x^α_i + x^α_j ≤ 1 for all edges {i,j} in the type graph, leading to many rows that share columns across multiple constraints. The paper asserts p_B = k (the number of twin types), but a correct column-independent partition must group any two rows sharing a color‐index column into the same part, forcing p_B = Ω(k^2). This error invalidates the claimed g₁(A) bound and the k^{O(k^3)}·|V|·log^3 |V| runtime."
      }
    ],
    "think_summary": [
      "**Analyzing paper for critical errors**\n\nThe user has requested that I check the attached paper for significant errors and issues that could invalidate its conclusions. It's important to focus on critical problems and compile up to five major points in a JSON format, noting the \"Problem,\" its \"Location,\" and an \"Explanation.\" The paper discusses refined Graver bases and algorithmic aspects, so I need to pinpoint any logical mistakes, flawed proofs, or incorrect assumptions, starting with examining Lemma 2 regarding the partition validity.",
      "**Evaluating Lemma 3 flaws**\n\nI'm considering some coordinate manipulations and their effects on the validity of Lemma 3 regarding the decomposition of \\( y_i^{(j)} \\) into cycles of \\( B(j) \\). If coordinates are zeroed, it alters \\( B(j)y \\), which means the decomposition might be incorrect. Instead of needing cycles, all they really need is the sum of \\( R_i^{(j)} y_i^{(j)} \\). There’s confusion over bounding norms because they mistakenly apply bounds relevant to \\( A(i) \\) rather than addressing \\( R_i \\), which combines rows from both A and B matrices.",
      "**Identifying critical errors**\n\nI'm analyzing the paper to identify five critical problems that could invalidate its conclusions. Firstly, there’s a flawed decomposition step in Lemma 3 because zeroing Graver vectors disrupts the kernel property. Secondly, the estimation of partition parameters for generalized scheduling seems incorrect, which inflates runtime beyond claimed bounds. Third, unsound assumptions regarding separable convex objectives risk invalid conclusions. Fourth, omitted exponential dependencies in runtime expressions underreport complexity. Lastly, incorrect partitioning in Minimum Sum Coloring may underestimate parameters, leading to erroneous runtime calculations. I'll structure these findings as JSON entries."
    ],
    "token_usage": {
      "input": 16180,
      "thinking": 6080,
      "output": 941
    }
  },
  {
    "entry_id": 51,
    "retraction_id": "2001.10983v3",
    "paper_id": "2001.10983v2",
    "retraction_comment": "A case in the proof of Proposition 2.8 was overlooked (thanks to [REDACTED-NAME] for pointing out this) and I withdraw the paper until that gap is filled",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Invalid application of Kodaira/Akizuki–Nakano vanishing to anti-ample twist",
        "Location": "Section 2.3, between Eqs. (2.3)–(2.5)",
        "Explanation": "The author claims H^q(Ω^p_{T×X}⊗O_{T×X}(−1,−1))=0 for p+q<n by invoking Kodaira vanishing, but O_{T×X}(−1,−1) is anti-ample, not ample, so neither Kodaira nor Akizuki–Nakano vanishing applies. This breaks the key identification (2.5) of H^0(Ω^{n−1}_{XT}) and invalidates the Hodge number comparison that underpins Theorem 2.13."
      },
      {
        "Problem": "Unjustified generation of W(E6) from arbitrary root generators",
        "Location": "Proposition 1.17 and Lemma 1.19",
        "Explanation": "The proof that any set {v_i} of minimal-norm roots spanning the E6 lattice yields a generating set of reflections for the full Weyl group relies on a combinatorial 'good sequence' argument. This argument tacitly assumes that any root can be written as a sum of the v_i with all partial sums again roots, a property known only for simple systems, not arbitrary generating sets. Without a rigorous verification or reference, the conclusion that the monodromy reflections generate W(E6) remains unsound."
      },
      {
        "Problem": "Sketchy dimension count in existence of smooth total space",
        "Location": "Proposition 2.8",
        "Explanation": "To produce a curve T_0 inside a positive-dimensional fiber of Φ_0 along which the total space X_T is smooth, the author compares dimensions of families of tangent directions to the dual variety. However, the bound dim {lines tangent to X* at I_α}≤n−1 and the subsequent conclusion that a general curve avoids all tangencies in a fiber of dimension 1 are asserted without a rigorous argument. This gap undermines the proof that one can find a smooth family X_T, which is essential for the Hodge-theoretic contradiction."
      },
      {
        "Problem": "Overstatement of codimension of indeterminacy locus for Φ",
        "Location": "Section 2.1, before Eq. (2.1)",
        "Explanation": "The extension of the rational map Φ:P^{n+1*}–X*→M_{d,n−1} to a morphism Φ_0 by removing an indeterminacy locus I of ‘codimension ≥2’ is taken for granted but not justified. In the GIT setting the indeterminacy may lie along unstable or strictly semistable hyperplanes, and its actual codimension and behavior can affect generic smoothness and dimension-count arguments downstream."
      },
      {
        "Problem": "Étaleness of Aut_{P1*}→P1* not uniformly valid",
        "Location": "Proposition 2.6",
        "Explanation": "The claim that Aut_{P1*}→P1* is finite étale uses H^0(X_H, T_{X_H})=0 to rule out infinitesimal automorphisms. But this vanishing holds only for a general hyperplane section; special fibers in the Lefschetz pencil may admit nontrivial infinitesimal automorphisms, so the morphism need not be unramified everywhere. Failure of étaleness would compromise arguments about sections and extendable automorphisms in Section 2.1–2.2."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 15787,
      "thinking": 5440,
      "output": 823
    }
  },
  {
    "entry_id": 52,
    "retraction_id": "2307.05226v3",
    "paper_id": "2307.05226v2",
    "retraction_comment": "The paper has been withdrawn because of the basic mistake: the map $f$ has in general rank q, not p < q (Section 3)",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Unjustified finiteness of the restricted map ψ in the isolated singularity argument",
        "Location": "Section 5, proof of Proposition 4.2 (page 14–15), definition of ψ before Lemma 5.1",
        "Explanation": "The key contradiction relies on applying Lemma 5.1 to the map ψ: (C^p,0)→(C^p,0) defined by ψ_i(y_2,…,y_{p+1})=φ_i(0,y_2,…,y_{p+1},a,0,…,0). However, it is not established that this ψ is a finite holomorphic map germ, since restricting φ to the affine subspace y_1=0,y_{p+2}=a,… need not preserve finiteness. Without ψ finite, Lemma 5.1 does not apply and the contradiction collapses."
      },
      {
        "Problem": "Non-rigorous continuity argument for level-set biholomorphisms in Proposition 3.3",
        "Location": "Section 3, Proposition 3.3 (pages 9–10)",
        "Explanation": "The proof asserts that for c near 0, the restricted projections π_c: Y_c∩(C^p×Y^ns_0)→Y^ns_0 are bijections and hence biholomorphisms by a ‘continuity of zero-dimensional cycles’ argument invoking Rouché’s lemma. The necessary analytic-cycle convergence and uniformity estimates are neither stated nor proved, leaving a substantial gap in establishing that the foliation G is non-singular around Y_0."
      },
      {
        "Problem": "Unproven preservation of singularity under generic hyperplane intersection",
        "Location": "Section 4, dimension-reduction step (page 11, paragraph beginning “Fix a linear hyperplane…”)",
        "Explanation": "The reduction to lower dimension asserts that for a generic affine translate σ(H_0), each analytic level set X_c remains singular upon intersection with σ(H_0). While invoking Rückert’s lemma for a single germ, no justification is given that this singularity property persists uniformly for all nearby X_c simultaneously, which is crucial for the inductive argument."
      },
      {
        "Problem": "Reliance on an unstated complex-analytic analogue of Kleiman’s theorem",
        "Location": "Section 1, Theorem 1.5 and Remark 1.6 (pages 3–4)",
        "Explanation": "The paper employs a ‘local complex analytic version’ of Kleiman’s transversality theorem for finite maps of analytic spaces without proof, citing only heuristics via Sard’s theorem. The passage from the algebraic to the analytic category is non-trivial and its validity is critical for the dimension-reduction argument; yet no conditions or references ensure the theorem holds in the needed generality."
      },
      {
        "Problem": "Insufficient justification for vanishing of H^2(U\\Sing(F_X),ℤ)",
        "Location": "Section 3, construction of ω_X via Cech cocycles (page 8, after Corollary 1.4)",
        "Explanation": "To show H^1(U\\Sing(F_X),𝒪^*)=0 one needs both H^1(U\\Sing(F_X),𝒪)=0 and H^2(U\\Sing(F_X),ℤ)=0. The vanishing of the topological group H^2(U\\Sing(F_X),ℤ) is asserted without argument. While Sing(F_X) has complex codimension ≥3, the requisite topological cohomology vanishing for the complement is non-obvious and unproven."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 10185,
      "thinking": 3712,
      "output": 834
    }
  },
  {
    "entry_id": 53,
    "retraction_id": "1702.07688v4",
    "paper_id": "1702.07688v3",
    "retraction_comment": "I no longer believe that the conclusions are supported by the calculations done in this manuscript. The paper tried to determine what will limit the precision in practice. I incorrectly pointed to small 1 qubit (precision) errors happening everywhere in the circuits",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Misrepresentation of threshold theorem assumptions regarding measurement precision",
        "Location": "Page 1–2, Introduction",
        "Explanation": "The paper claims that fault-tolerance theorems assume infinite precision in measurement direction. In fact, those theorems accommodate arbitrary small local errors—including small coherent misalignments of measurement axes—by modelling them as bounded noise channels. Finite measurement precision simply contributes to the overall error rate input to the threshold theorem and does not invalidate it."
      },
      {
        "Problem": "Erroneous assertion that measurement‐axis misalignment is uncorrectable",
        "Location": "Page 3, paragraph 2 (\"the unphysical assumption behind this theorem…\")",
        "Explanation": "The author argues that unknown measurement‐axis rotations cannot be corrected because syndrome measurements yield only ±1 outcomes. In reality, a misaligned measurement is equivalent to a correct‐basis measurement preceded by a small unitary rotation, which can be treated as a standard gate error in the fault-tolerant framework. Such coherent errors are within the scope of existing threshold proofs once appropriately bounded."
      },
      {
        "Problem": "Use of non–fault-tolerant extraction circuits in simulations",
        "Location": "Page 4–5, Figure 2 and accompanying text",
        "Explanation": "The numerical experiments omit fully fault-tolerant syndrome extraction (no ancilla verification or repeated measurement). The paper asserts that including these procedures \"does not help,\" but without simulating verified, repetitive measurements and error‐suppression gadgets, the conclusion about performance under finite precision is not representative of established fault-tolerant architectures."
      },
      {
        "Problem": "Neglect of techniques for mitigating coherent errors (e.g., randomized compiling)",
        "Location": "Page 5, final paragraph before Conclusion",
        "Explanation": "The paper treats all continuous misalignment errors as irreducible leakage reservoirs. However, modern fault-tolerance approaches employ randomized compiling and Pauli twirling to convert coherent misalignment into stochastic Pauli errors—again covered by threshold theorems. Ignoring these techniques overstates the detrimental impact of finite precision."
      },
      {
        "Problem": "Overgeneralization from small‐code simulations to all fault‐tolerant schemes",
        "Location": "Page 5, Conclusion",
        "Explanation": "Results from 5-, 7-, 9-, and 13-qubit codes with simplified circuits are extrapolated to claim that no quantum error correction can surpass single‐qubit precision. This leap ignores larger concatenated codes and topological surface-code implementations, where logical precision scales favorably even with imperfect measurements."
      }
    ],
    "think_summary": [
      "**Evaluating critical errors**\n\nThe user wants me to analyze the attached paper for significant errors or weaknesses that could undermine its conclusions. They’ve asked me to ignore minor issues and emphasize up to five critical problems, formatted as a JSON list. I need to specifically examine Waintal's claims about an implicit assumption regarding measurement precision and how that relates to fault tolerance. I’ll also look into whether fault tolerance theory sufficiently addresses measurement errors and noise models.",
      "**Analyzing measurement errors**\n\nI'm examining how continuous measurement errors can be managed through fault tolerance. It seems erroneous measurement bases can correspond to unitary rotations before and after measurement, which can be considered as faulty gates. Waintal’s claim that measurement misalignment leads to irreparable errors appears flawed. The threshold theorem can manage local noise and bounded errors, while claims about random measurement directions being uncorrectable don't hold up; calibration can fix measurement axes, and slight misalignments can be treated as coherent noise.",
      "**Identifying simulation flaws**\n\nI've noticed that numerical simulations in the paper omit fault tolerance by not incorporating ancilla verification and repeated measurements, even claiming this won't help, which is misleading. Fault-tolerant measurements are crucial for preventing error propagation. The claim regarding measurement error and coherent errors doesn’t consider the effectiveness of error correction circuits. I also see issues with how misalignment is treated and the assumption that these errors can’t be corrected. I’ll note these problems and format my findings accordingly."
    ],
    "token_usage": {
      "input": 10782,
      "thinking": 1152,
      "output": 600
    }
  },
  {
    "entry_id": 54,
    "retraction_id": "1408.2493v2",
    "paper_id": "1408.2493v1",
    "retraction_comment": "This paper has been withdrawn by the author. The proof of Theorem 6.2 is incorrect and the Theorem probably fails to be true",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Use of non‐constructive case distinction on reals in the proof of Heine–Borel ⇒ Fan Theorem",
        "Location": "Section 3.3, pages 7–9 (proof of (ii) ⇒ (i) in Theorem 3.3)",
        "Explanation": "The argument distinguishes two cases for an arbitrary real γ—namely that γ coincides with a binary rational or is positively apart from every binary rational—and proceeds by separate constructions in each case. While each branch can be carried out, the proof tacitly assumes the excluded‐middle principle for the predicate “γ is a binary rational,” which is not available in BIM. Hence the reduction of HB to FT′ is not valid constructively without further semi‐classical assumptions."
      },
      {
        "Problem": "Implicit appeal to full countable choice in the proof of IRT(1)",
        "Location": "Section 12.2, proof of Theorem 12.1 (case k=1)",
        "Explanation": "To extract a strictly increasing sequence η satisfying η(n)>n and hζ(η(n))i∈Dα, the proof invokes the Minimal Axiom of Countable Choice. However, BIM only guarantees choice for decidable numerical relations (Axiom 3(iii)), not for arbitrary Π₁ relations of the form ∀n∃m[R(n,m)], and here R(n,m) is itself a negative statement. This step thus exceeds the strength of BIM’s choice axioms."
      },
      {
        "Problem": "Circularity in the equivalence of AFT and OI(C) via Bar ⊆ WF",
        "Location": "Section 11, Theorem 11.2",
        "Explanation": "The proof that the Approximate‐Fan Theorem implies every bar in N is well‐founded under the Kleene‐Brouwer ordering relies on constructing a decidable bar B and then applying AFT in the form of OI(C) on Cantor space. But OI(C) itself was earlier shown equivalent to AFT, so this uses the very principle under investigation in a different guise, creating a hidden circularity."
      },
      {
        "Problem": "Use of classical double‐negation shift in arithmetical bar induction",
        "Location": "Section 14, Theorem 14.2",
        "Explanation": "The equivalence between ‘every enumerable set is nearly‐decidable’ and arithmetical bar induction (ABI) presupposes the validity of DNS₀ (¬¬∀n[P(n)]→∀n[P(n)] for arithmetical P), which is not derivable in BIM+MP₁ alone. Thus ABI is not shown to follow from BIM+MP₁ without adding Kuroda’s principle."
      },
      {
        "Problem": "Non‐decidable equality of real numbers used in OI([0,1]) ⇒ CMC",
        "Location": "Section 5.3, proof of Theorem 5.2, pages 15–16",
        "Explanation": "The proof scales a monotone sequence γ to δ=γ/M and then defines an open set Hα by testing δn>1. It implicitly uses the decidability of the predicate δn>1, but in BIM real‐number comparators yield only ¬¬(δn>1) or ¬(δn>1), not a direct decision. This may require Markov’s Principle or excluded middle for real inequality, undermining the claimed derivation of CMC from OI([0,1])."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 64286,
      "thinking": 2496,
      "output": 800
    }
  },
  {
    "entry_id": 55,
    "retraction_id": "2112.06228v2",
    "paper_id": "2112.06228v1",
    "retraction_comment": "Theorem 4.3 is missing the terms corresponding to the linear pentagon equation",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Ill-defined ‘virtual posets’ and formal inverses",
        "Location": "Section 3.1–3.2 (Definition of 0 and removal of a point, eq. (3.3))",
        "Explanation": "The construction of virtual posets relies on formally inverting combinatorial operations (disjoint union and concatenation) via differential and integral operators, without any concrete combinatorial or categorical model. In particular, the ‘removal of a point’ operator (eq. (3.3)) depends on an arbitrary choice of integration constant, so the resulting power series are not uniquely or canonically defined. This lack of a rigorous foundation calls into question all subsequent algebraic properties (e.g. that these series give well-defined shuffle algebra morphisms)."
      },
      {
        "Problem": "Divergence of integral representations for non-integer parameters",
        "Location": "Section 4.2 (analytic continuation of σ_z, paragraph claiming convergence for Re(z)>0)",
        "Explanation": "The family σ_z is defined by iterated integrals whose last differential form is (x^z/(1−x)) dx. Near x→1 this behaves like (1−x)^{−1} dx, whose integral diverges as log(1−x). Thus the integrals do not converge at the upper limit, so the assertion that σ_z ζ(...) defines a well-defined analytic function for Re(z)>0 is incorrect. Without convergence, the analytic path of associators φ_{KZ,z} cannot be constructed."
      },
      {
        "Problem": "Invalid shuffle product for truncated multiple zeta values",
        "Location": "Theorem 3.4 (Section 3.3)",
        "Explanation": "The proof that the truncated sums ζ_n(w) inherit the shuffle product uses Proposition 2.2 on iterated integrals. However, that proposition requires the first form to be dx/(1−x) and the last to be dx/x for convergence and cancellation of boundary terms. In the truncated-sum integrals the last form is (x^n/(1−x)) dx, violating these conditions. Therefore there is no guarantee that ζ_n respects the shuffle product, invalidating the key algebra-morphism property."
      },
      {
        "Problem": "Unchecked normalization in applying Furusho’s theorem",
        "Location": "Theorem 4.3 (Section 4.1)",
        "Explanation": "Theorem 4.2 ensures that a group-like series satisfying the pentagon equation yields a solution of the hexagon equations for some parameter µ. However, the paper never verifies that the constructed series φ_{KZ,m} or φ_{KZ,z} satisfy the required normalization (in particular the correct leading terms and value of µ). Without this check, the hexagon relations need not hold, so φ_{KZ,m} and φ_{KZ,z} may fail to be genuine Drinfeld associators."
      },
      {
        "Problem": "Illicit evaluation at x=1 of power series with poles",
        "Location": "Proposition 3.2 and Section 3.2 (identification of |0|=x/(1−x) and its powers at x=1)",
        "Explanation": "The paper repeatedly evaluates power series like |0|(x)=x/(1−x) and its iterated images at x=1 to recover zeta values. But x/(1−x) and its powers have poles at x=1, so these evaluations diverge. No regularization or limiting procedure is given. Thus the identification of series coefficients with multiple zeta values is not justified."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 16101,
      "thinking": 2880,
      "output": 807
    }
  },
  {
    "entry_id": 56,
    "retraction_id": "1311.7114v2",
    "paper_id": "1311.7114v1",
    "retraction_comment": "This paper has been withdrawn by the author due to an error in the derivation of equation 24 and 25",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Non‐Hermitian one‐electron local Hamiltonian",
        "Location": "Eq. 3, Sec. II A",
        "Explanation": "The one‐electron term in Hp is defined as θp(r) h(r) without symmetrizing the projector θp with the kinetic operator (which contains ∇²). Because θp is a discontinuous step function, θp h is not Hermitian unless one includes the surface‐term corrections arising from ∇²θp. Omitting these boundary contributions means the local kinetic energy operator is not self‐adjoint and its expectation values can be complex or ordering‐dependent, invalidating the rigor of the energy partitioning."
      },
      {
        "Problem": "Mathematical inconsistency in the dividing‐surface definition",
        "Location": "Eq. 29, Sec. III",
        "Explanation": "The partitioning function θ(x,y,z) uses a term (x–x₀)^a with a=–1.3, which is undefined (and singular) when x–x₀≤0 and yields complex values for negative arguments. No absolute‐value or smoothing is specified, nor is there justification for handling the resulting singularity. An ill‐defined partition boundary undermines all subsequent subsystem integral evaluations and thus the computed energy partitions."
      },
      {
        "Problem": "Incorrect formula for partitioned nuclear attraction integrals",
        "Location": "Eq. 23, Sec. II B",
        "Explanation": "Equation 23 gives V^(p)_{rs} with a prefactor 2(ζ/π)^{1/2} and omits the nuclear charges Zα and the correct sign, whereas the standard closed‐form integral is proportional to –Zα·(2π/ζ) F₀(ζ|R_P–R_α|²)·S_{rs}. The missing Zα factor and wrong numerical prefactor mean the computed one‐electron potential contributions to subsystem energies are incorrect, casting doubt on all numerical results."
      },
      {
        "Problem": "Coordinate‐frame dependence (lack of rotational invariance)",
        "Location": "Sec. II A & III",
        "Explanation": "The real‐space projector θp(r) is defined via an explicit Cartesian formula (Eq. 29). As a result, subsystem energies depend on how the molecule is oriented in the chosen coordinate system. Physical energy partitions must be invariant under rotations; this frame‐dependence makes the method’s predictions arbitrary unless an ad hoc orientation convention is imposed."
      }
    ],
    "think_summary": [
      "**Analyzing critical errors**\n\nThe user is looking to identify critical errors in a 5-page paper that might undermine its conclusions. I need to thoroughly read the text and understand their methods to pinpoint any major flaws, focusing on significant issues like theoretical inconsistencies, calculation problems, unjustified assumptions, and mathematical errors—while ignoring minor concerns. My goal is to summarize up to five critical problems in a structured format, highlighting the “Problem,” “Location,” and “Explanation” based on my findings.",
      "**Identifying critical issues**\n\nThe user is focused on assessing the A1N molecule study, which looks at partitioning anthracenyl and CH2-naphthyl subsystems. They're utilizing an arbitrary surface defined by a specific equation, which raises some concerns. There are potential issues highlighted, such as an arbitrarily defined dividing surface that might affect results, the use of the CIS method, which may limit accuracy, and the neglect of nuclear repulsion in their calculations. Additionally, the approach to deriving two-electron integrals seems to lack rigor as well, which could complicate the conclusions drawn."
    ],
    "token_usage": {
      "input": 9072,
      "thinking": 5888,
      "output": 554
    }
  },
  {
    "entry_id": 57,
    "retraction_id": "1206.3652v3",
    "paper_id": "1206.3652v2",
    "retraction_comment": "This paper has been withdrawn by the author due to a crucial error in Theorem 2.6 under the metric of Grassmannian manifolds induced from the riemannian submersion",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Incorrect group embedding in Theorem 2.5: use of SU(1+1) instead of SU(2)",
        "Location": "Section 2, Theorem 2.5 (page 8–9)",
        "Explanation": "The proof asserts a Lie‐algebra monomorphism f: su(1+1)→u(n+m) and then integrates to a group monomorphism ŷf: SU(1+1)→U(n+m), identifying SU(1+1)/S(U(1)×U(1)) with S² and the total space with S³.  However, SU(1,1) is noncompact and not simply connected, and SU(1,1)/S(U(1)×U(1)) is the hyperbolic plane, not the sphere.  The intended compact Hopf bundle requires embedding SU(2), not SU(1,1).  This misidentification invalidates the construction of the rank-1 subbundle isomorphic to S¹→S³→S² and hence the application of the SU(2) holonomy result."
      },
      {
        "Problem": "Unjustified triple‐bracket computation in Lemma 2.1",
        "Location": "Section 2, Lemma 2.1 (page 6)",
        "Explanation": "Lemma 2.1 states an explicit formula for [[X̂, Ŷ], X̂] in terms of the columns of X and Y, but the proof is a one-line assertion “easily obtained from [[X̂, Ŷ], X̂] = X̂(2ŶX̂–X̂Ŷ)–ŶX̂X̂” without verifying block‐matrix products or the resulting coefficients.  This computation is crucial for Theorem 2.2, and any error here undermines the classification of totally geodesic 2-planes."
      },
      {
        "Problem": "Omission of metric normalization on Grassmannian and induced 2-sphere",
        "Location": "Section 2, Theorem 2.6 and surrounding discussion (page 9–10)",
        "Explanation": "The final holonomy formula V(γ)=e^{(1/2)A(γ)i} assumes that each totally geodesic complex 2-surface inherits the same metric normalization as CP¹ in the SU(2) case.  However, the homogeneous metric on U(n+m)/(U(n)×U(m)) scales the induced CP¹ differently in general, and the factor 1/2 in the exponent depends on curvature normalization.  Without specifying or adjusting the metric on Gn,m, the general holonomy constant λ cannot be asserted to be exactly 1/2."
      },
      {
        "Problem": "Lack of justification for horizontal distribution and submersion metric in Theorem 1.1",
        "Location": "Section 1, proof of Theorem 1.1 (page 4–5), equation (1–1)",
        "Explanation": "The proof introduces a left-invariant metric on SU(2) and then writes the horizontal-lift condition h_{SU(2)}((ℓ_{η(t)}^{-1})_*ηʼ(t),e₃)=0 to derive zʼ=sin²x·yʼ, but it never checks that p: SU(2)→CP¹ is a Riemannian submersion with totally geodesic U(1)-fibers under that metric.  The orthogonality condition and the form of the connection are asserted without verifying that the chosen basis (e₁,e₂,e₃) yields the correct Levi-Civita connection or that p is metric."
      },
      {
        "Problem": "Incomplete verification of the Lie triple-system condition in Theorem 2.2",
        "Location": "Section 2, Theorem 2.2 (page 7)",
        "Explanation": "Theorem 2.2 asserts that a 2-dimensional subspace m′⊂m is a Lie triple system iff certain scalar conditions on X and Y hold.  In the proof, only the two triple brackets [[X̂,Ŷ],X̂] and [[Ŷ,X̂],Ŷ] are checked, but one must verify [ [u,v],w ]∈m′ for all u,v,w∈m′.  The argument does not establish closure under all triple brackets (e.g. mixed combinations), so the characterization of totally geodesic submanifolds may be incomplete or incorrect."
      }
    ],
    "think_summary": [
      "**Calculating integral area**\n\nLet’s compute the area using double integrals. I’m starting with the expression involving integrals of sin functions. I noticed a mistake while transitioning between formulas for cosine and sine. I thought about applying trigonometric identities and simplifying them correctly. Eventually, I figured out how to relate the sine and cosine components. It’s important to be precise here, as mixing steps can lead to confusion. The final expression gives a clearer understanding of the integral area calculated.",
      "**Reviewing mathematical derivations**\n\nThe formula checks out: \\(2b(\\sin^2(p+a)-\\sin^2p) = b(\\cos2p - \\cos2(p+a))\\). They’re calculating the change in \\(z\\) along the boundary and integrating around a rectangle using parameterization. Their conclusion about vertical change being half the area is reasonable. \n\nMoving to Section 2, the definitions around \\(U(n) x U(m)\\) and the commutator triple bracket are a bit confusing. They assert a questionable identity in their proof without clear justification, which makes me think it could be incorrect or inadequately supported.",
      "**Identifying key problems**\n\nI’ve noticed several issues in the geometric embedding of \\(CP^1\\) into \\(Gr(n,m)\\) that could lead to serious consequences. Firstly, there's confusion between \\(SU(1+1)\\) and \\(SU(2)\\) in Theorem 2.5, invalidating the rank-1 subbundle identification. Then, in Lemma 2.1, the triple bracket calculation lacks proof, which is crucial for Theorem 2.2. Also, metric normalization isn't specified, casting doubt on related claims. Lastly, the horizontal distribution in Section 1 isn’t justified, and Theorem 2.2 doesn’t adequately prove triple bracket closure. I'll compile these into a structured format."
    ],
    "token_usage": {
      "input": 7418,
      "thinking": 5440,
      "output": 1014
    }
  },
  {
    "entry_id": 58,
    "retraction_id": "1511.00057v2",
    "paper_id": "1511.00057v1",
    "retraction_comment": "Several proofs were found to be incomplete or in error including the proof that quantum rotations can induce arbitrary noise weights. A fully corrected version of this paper is published as: A. Paris, G. Atia, A. Vosoughi, and S. Berman, \"Hidden quantum processes, quantum ion channels, and 1/f-type noise\", [REDACTED-NAME], vol. 30, num. 7, pp. 1830-1929 (2018), doi:https://doi.org/10.1162/neco_a_01067",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Invalid generalization of Born’s rule to multitime correlations",
        "Location": "Section 2.2.1, Equation (5)",
        "Explanation": "Equation (5) assumes that one can assign joint probabilities tr(A₁⊗⋯⊗Aₙ·Ψ(t₁,…,tₙ)) to observables at distinct times without specifying any measurement instrument or accounting for non-commutativity and measurement back-action. In standard quantum theory, multitime correlations require a detailed specification of the measurement operations (via completely positive instruments or POVMs) and cannot be captured by a single family of density operators on H⊗ⁿ. As stated, Eq. (5) is mathematically under-constrained and physically unsound, undermining all subsequent results that rely on these multitime moments."
      },
      {
        "Problem": "Existence and uniqueness of activator Q for arbitrary hidden Markov models is unproven",
        "Location": "Section 2.2.4, Theorem 2.2.4.1",
        "Explanation": "Theorem 2.2.4.1 claims any classical HMM can be realized as a diagonalizable hidden activated measurement model by finding a quantum activator Q whose squared-modulus matrix Q|2| reproduces the HMM transition rates. However, no construction or proof is given that for a general continuous-time rate matrix K one can solve Q|2|=I+K/λ for a unitary or norm-preserving Q. In fact, arbitrary rate matrices may not admit such a ‘square-modulus’ root that yields a valid activator, invalidating both directions of the claimed equivalence."
      },
      {
        "Problem": "Unsound derivation of the energy operator from the kinetic rate matrix",
        "Location": "Section 3.1, Equation (10)",
        "Explanation": "Equation (10) defines the operator E by inverting the relation K = λ(T)·D_π·e^{-E/k_BT}(I–ππᴴ)·D_π^{-1}. This mixes a matrix exponential, a rank-1 projector, and similarity transforms in a way that does not guarantee the existence of a unique Hermitian E, nor that the right-hand side has the spectral properties of a valid infinitesimal generator (negative diagonal entries summing its column, non-negative off-diagonals). The dimensional analysis and operator domains are also inconsistent, making the ‘derivation’ of E ill-posed."
      },
      {
        "Problem": "Construction of the ion activator violates stochasticity constraints",
        "Location": "Section 3.1, Equation (11)",
        "Explanation": "Equation (11) prescribes Q(T)|2| = I + [0,…,0,1]ᵀ⋅(1–e^{-E_e/k_BT}), a rank-1 update that generally breaks the requirement that the columns of Q|2| sum to unity (a necessary condition for Lemma 2.2.2.1). No condition is imposed to ensure non-negativity of all entries or that Q(T)|2| arises from any norm-preserving operator Q(T). Thus the proposed activator does not in general define a valid measurement-activation process, invalidating the model of temperature-dependent channel kinetics."
      },
      {
        "Problem": "Lack of rigorous derivation for 1/f⁽ᵅ⁾ noise via maximum entropy",
        "Location": "Section 3.2, following Lemma 3.2.0.1",
        "Explanation": "The paper asserts that maximizing entropy under constrained activation energies yields noise spectra S(f)∼1/fᵅ, but provides only an outline invoking a ‘generalized van der Ziel–McWhorter formalism’ without detailed asymptotic analysis or justification of the continuum limit required to obtain a true power law. In known models, mixtures of exponential relaxations at best approximate 1/f over limited ranges, and the unbounded 1/f behavior is mathematically non-normalizable. The absence of a precise calculation or specification of the energy distribution invalidates the claim of a biophysical explanation for universal 1/f noise."
      }
    ],
    "think_summary": [
      "**Identifying critical errors**\n\nThe user is asking for critical errors and unsoundness in conclusions, specifically summarizing the top five. I need to focus on flaws in mathematical definitions and theorems. One major error involves defining a quantum stochastic process as trace class operators, which may inadequately correspond to physically consistent processes. They overlook crucial aspects like complete positivity and may not ensure dynamical consistency. Additionally, they're missing elements related to measurement disturbances and non-commutativity. These shortcomings weaken their conclusions significantly."
    ],
    "token_usage": {
      "input": 19246,
      "thinking": 1536,
      "output": 942
    }
  },
  {
    "entry_id": 59,
    "retraction_id": "1704.08680v2",
    "paper_id": "1704.08680v1",
    "retraction_comment": "Withdrawn by the author. In particular, Lemma 4 and hence Lemma 5 are incorrect invalidating the claimed result",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Phase III never adds the newly tight edge e into the main solution T, so the returned T may be disconnected or miss required edges.",
        "Location": "Section 2, Algorithm 3 (steps 7–12 and return)",
        "Explanation": "In Algorithm 3, T3 is grown by adding each tight edge e to T3, but T itself (initialized to T′₂) is only modified via shortcut operations; the newly tightened edges e are never inserted into T.  As a result, the final returned T can lack edges needed to connect all terminals, invalidating correctness."
      },
      {
        "Problem": "The shortcutting operations in Phase III can destroy connectivity or feasibility without assurance that T remains a valid Steiner tree.",
        "Location": "Section 2, Algorithm 3 and Section 2.2 (after Lemma 5)",
        "Explanation": "Each shortcut removes two edges (vi, si) and ei and adds one edge e′i=(vi, wi), reducing the total edge count.  The paper does not prove that these replacements preserve connectivity among terminals or maintain feasibility of the solution, so T may become disconnected or invalid."
      },
      {
        "Problem": "Lemma 3’s claim that any frontier ∆₁(C) of a Phase II minimal cut C contains no terminals is unsupported.",
        "Location": "Section 2.2, Lemma 3",
        "Explanation": "The proof argues by reverse‐delete on the Phase I tree T′₁, but minimal cuts C in Phase II need not align with redundancy arguments from Phase I.  There is no guarantee that an edge ei∈δ₁(C) incident to a terminal would have been deleted in T′₁, so ∆₁(C) may include terminals, breaking the lemma’s key structural claim."
      },
      {
        "Problem": "Lemma 4’s mapping from each high‐degree dual C to |δ₁(C)| distinct degree‐1 duals is flawed when minimal cuts overlap.",
        "Location": "Section 2.2, Lemma 4",
        "Explanation": "The argument assumes for each outgoing edge of C there exists a separate minimal cut C′ of degree 1 that grows synchronously, but does not account for overlaps between cuts or that such C′ may not exist.  Without a one‐to‐one correspondence, the claimed count of degree‐1 duals (∑αᵢ) can fail, invalidating the lower bound on low‐degree duals."
      },
      {
        "Problem": "The integrality gap bound uses only the Phase II dual y² but ignores the Phase III dual y³, so dual feasibility against the final T is unproven.",
        "Location": "Section 2.2, Theorem 7",
        "Explanation": "After shortcutting in Phase III, the authors continue to use y² to bound c(T) and assert dual feasibility, but they never show that y² remains a feasible dual of BCR for the modified tree T.  The growth of y³ and its relation to T are omitted, so the claimed bound Z*_{BCR}≥∑y² may not hold."
      }
    ],
    "think_summary": [
      "**Identifying flaws in the argument**\n\nI need to identify up to five critical problems that could invalidate the conclusions drawn in this paper. I'm focusing on logical flaws, incorrect lemmas, and mischaracterizations while disregarding minor issues. The paper presents a combinatorial algorithm for the Steiner tree with an established integrality gap. I should first check the soundness of the primal-dual algorithm and the feasibility of the constructed dual variables. There are specific aspects, like the dual constraints and how they grow duals in the different phases, that require careful analysis.",
      "**Analyzing performance proof issues**\n\nIn the performance proof, they claim that c(T) is less than or equal to the sum of |δ1(S)| y^2_S, and that this sum is bounded by 6/5 times Z*. However, I see potential issues with Lemma 4 regarding degree 1 duals. They assert that for each set C with δ1(C) = d>1, there are d degree-1 duals, relying on a reverse-delete argument. I need to verify if their reasoning holds, particularly about the degree conditions and connectivity after reverse-deleting edges. Can there still be edges entering set C after this process?",
      "**Identifying critical flaws**\n\nThe analysis seems to overlook phase III duals and their potential improvements, mainly relying on y2. In Lemma 5, they assert an inequality between y1 and y2 but question if that's always true, especially if δ1(C) equals zero for some sets. This introduces critical problems in the proofs within several lemmas. Possible issues include:\n\n1. The description in Algorithm 3 is flawed due to a disconnected T.\n2. Lemma 3 mistakenly assumes interactions across phases.\n3. Lemma 4 might fail with overlapping sets concerning degree-1 duals.\n4. A misapplication of Cauchy-Schwarz in Lemma 6 is evident.\n5. The integrality gap assumes dual feasibility despite primal changes, without proof. \n\nLet’s verify the validity of Lemma 6 further."
    ],
    "token_usage": {
      "input": 24393,
      "thinking": 5184,
      "output": 701
    }
  },
  {
    "entry_id": 60,
    "retraction_id": "2406.04846v2",
    "paper_id": "2406.04846v1",
    "retraction_comment": "The claim in this manuscript is incorrect due to a mistake in Eq. (6). The reason is that expression in Eq. (6) is derived in real arithmetic. It is incompatible with the modulo 2 arithmetic in the state ket",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Gate set depends on desired accuracy ε, not a fixed finite set",
        "Location": "Conclusion (page 3)",
        "Explanation": "The construction uses gates P(πℓ/2^m) with m chosen as a function of the target precision ε, so the instruction set grows without bound as ε→0. This violates the requirement of a fixed finite gate set independent of ε and undermines the claim of solving the Nielsen–Chuang challenge."
      },
      {
        "Problem": "Circular assumption in transversal decomposition (Eq. 4)",
        "Location": "Equation (4), page 2",
        "Explanation": "The formula P_L(α)=⊗_j P(α v_j) assumes access to arbitrary physical rotations P(α v_j). But the whole goal is to approximate such rotations fault-tolerantly. Without a primitive FT implementation for arbitrary-angle P gates, the decomposition is unsound and merely shifts the problem rather than solving it."
      },
      {
        "Problem": "Lack of rigorous error and fault-tolerance analysis for the recursive scheme",
        "Location": "Section “Method” (pages 2–3)",
        "Explanation": "The paper does not bound how approximation errors and physical noise propagate through the recursive gate teleportation circuits (Figs. 1–2). No threshold condition or cumulative error estimate is provided, so there is no guarantee the final approximation achieves the stated accuracy ε under realistic noise."
      },
      {
        "Problem": "Ignoring resource overhead for ancilla state distillation",
        "Location": "Discussion of Figs. 1–2, page 2",
        "Explanation": "The scheme relies on repeatedly preparing high-fidelity |cat⟩ and |Θ(α)⟩ logical states via post-selection and stabilizer checks. The overhead for these distillation procedures can be exponential in the desired error rate, invalidating the claimed gate-count scaling of O(log(1/ε) log log(1/ε) …)."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 6317,
      "thinking": 1664,
      "output": 463
    }
  },
  {
    "entry_id": 61,
    "retraction_id": "2306.09163v2",
    "paper_id": "2306.09163v1",
    "retraction_comment": "Theorem 1 is false for A = F_2[x]/(x^3): (A, +) \\cong C_2 x C_2; (A, \\circ) \\cong C_4",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Theorem 1 claims that for any finite nilpotent F_p-algebra A, the number of subgroups of the adjoint group (A, ◦) equals the number of F_p-subspaces of (A, +).",
        "Location": "Page 3, Theorem 1",
        "Explanation": "This is false in general. For example, let A be the 3-dimensional nilpotent F_p-algebra with basis {a,b,c} and multiplication ab=c, all other products 0. Then (A, ◦) is the Heisenberg group of order p^3, which has p^2+p+1 subgroups of order p but only p+1 subgroups of order p^2, whereas the F_p-vector space A has p^2+p+1 subspaces of each dimension 1 and 2. Hence s(A, ◦)≠s(A,+), contradicting Theorem 1 and invalidating its proof."
      },
      {
        "Problem": "Corollary 2 and all subsequent asymptotic estimates for s(A, ◦) rely on the false equality s(A, ◦)=s(A,+).",
        "Location": "Page 4, Corollary 2",
        "Explanation": "Since Theorem 1 fails already in dimension 3, the claim that s(A, ◦) grows like p^{n^2/4} (as for vector spaces) is unsupported. Nonabelian adjoint groups can have strictly fewer subgroups than the corresponding additive vector space."
      },
      {
        "Problem": "All Galois correspondence ratio (GCR) computations for 4-dimensional algebras use s(A, ◦)=s(A,+) to simplify the denominator, but this step is invalid.",
        "Location": "Page 5–7, Section 3",
        "Explanation": "The explicit formulas for GCR(L/K,G,H)=i(A)/s(A, ◦) are replaced by i(A)/s(A,+) under Theorem 1. Since Theorem 1 is false, the computed GCR values (e.g. for the algebra A_{24}) do not in general reflect the true ratio, invalidating the numerical results and comparisons."
      },
      {
        "Problem": "The general asymptotic bound in Theorem 3 on the GCR for the free algebra A(n,e) depends on the incorrect subgroup count s(A, ◦)=s(A,+).",
        "Location": "Page 9–10, Theorem 3",
        "Explanation": "The derivation of the bound GCR≲p^{- (n^2/4)(n^{2e}/(n−1))} uses the (invalid) equality of subgroup and subspace counts. Without this, there is no justification that the denominator grows as fast as the additive case, so the claimed decay of GCR for large p,n,e is unsubstantiated."
      }
    ],
    "think_summary": [
      "**Analyzing counting and references**\n\nIt seems like the counting in Section 3 might be correct, though there are some minor referencing issues that aren't critical. \n\nHowever, there's a significant concern regarding how left ideals are counted. They rely on sub-Hopf algebras in the Galois correspondence, which needs further clarification. The relationship between left ideals and sub-Hopf algebras appears to hold only for radical rings, and further justification is necessary to show that their enumeration aligns. Also, in Theorem 1, they claim the same enumeration for RREF, which seems flawed.",
      "**Examining row operation complexities**\n\nI'm analyzing row operations that resemble circle operations, akin to addition and scalar multiplication, but I realize vector space operations must be distributive over sums. While circle operations yield a group structure, row addition is a group operation, and scalar multiplication acts repeatedly within deeper ideals. I need to ensure subtraction uses power series rather than linear methods for RREF. \n\nI wonder if there are counterexamples, particularly with noncommutative nilpotent algebras. Also, I question the need for commutativity in their \"matrix RREF with circle operations,\" especially since they could be mistakenly mixing operations. \n\nThere's also a potential flaw in Section 4 regarding counting ideals. They derive upper bounds but assume conditions on pivot distributions that might not hold simultaneously for all r-vectors.",
      "**Identifying discrepancies in algebra**\n\nI'm reviewing parameters in the algebraic context. The lower bound claims certain parameters provide an acceptable basis, but there may be issues with the definition of A(n,e) as free noncommutative algebra, particularly regarding the associativity. \n\nTheorem 1 raises critical concerns, especially the flawed proof relating to row operations and circle group settings. They claim equality in certain enumerations without establishing a bijection, making the theorem too strong. I also notice flaws in estimating upper bounds on parameters and assumptions linking left ideals to sub-Hopf algebras. Lastly, I wonder about the subgroup counts, considering possible counterexamples to address these discrepancies."
    ],
    "token_usage": {
      "input": 7947,
      "thinking": 3840,
      "output": 656
    }
  },
  {
    "entry_id": 62,
    "retraction_id": "1607.07976v3",
    "paper_id": "1607.07976v2",
    "retraction_comment": "The statement about approximate equality of phase and group velocities is incorrect. Therefore, all arguments based on this statement are unfounded",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Misapplication of energy–momentum four-vector invariance for electromagnetic fields",
        "Location": "Section IV.A, derivation of Eq. (9b)",
        "Explanation": "The authors treat the laser pulse like a massive particle with an invariant “rest” energy H₀ in the Pulse Co-Moving (PCM) frame and use Lorentz invariance of the four-momentum K⁴=(H_FT/c, K_F). But for electromagnetic fields K⁴ is null (H_FT²–c²K_F²=0) and has no rest frame. One cannot boost to a frame where the field’s energy is minimal in the way one does for a massive particle. This invalidates the derivation of the group-velocity evolution Eq. (9b)."
      },
      {
        "Problem": "Inconsistent definition and use of the PCM frame with variable group velocity",
        "Location": "Sections II & IV.A, throughout derivations of Eqs. (5)–(7)",
        "Explanation": "The PCM frame is defined via a Lorentz boost at the pulse’s group velocity v_g, yet in the subsequent analysis v_g is allowed to vary. Lorentz transformations assume a fixed boost velocity; mixing a ‘co-moving’ frame whose velocity evolves with time is not legitimate. This inconsistency pervades the energy–momentum and conservation-law derivations."
      },
      {
        "Problem": "Assumed equality of local radiation phase velocity and global group velocity",
        "Location": "Sections III & IV.C, discussion around Fig. 3 and Eq. (10)",
        "Explanation": "The paper asserts that in the PCM frame the laser’s carrier frequency ω′≈0, so that in the lab frame v_ph=ω/k equals the pulse’s group velocity. But for electromagnetic waves in plasma v_ph>c while v_g<c, so they cannot be equal. The observed ‘equality’ in simulations actually conflates the plasma‐wave phase fronts with the light’s phase—and cannot justify setting ω′≈0."
      },
      {
        "Problem": "Unjustified neglect of radiation back-reaction and shaky adiabaticity criterion",
        "Location": "Section V, derivation of Eq. (12)",
        "Explanation": "The condition for adiabatic pulse evolution (Eq. (12)) is derived by comparing a phenomenological ‘force density’ to the local plasma momentum density without a systematic multi-scale analysis. Key nonlinear couplings and the very back-reaction the paper neglects are exactly what drive pulse modulation. Thus the asserted ‘adiabatic regime’ may not exist under the stated criteria."
      },
      {
        "Problem": "Over-reliance on 1D3V PIC simulations for inherently multidimensional physics",
        "Location": "Section III, simulation setup and validation",
        "Explanation": "All validations use 1D3V PIC runs. In realistic under-dense plasmas, transverse effects (self-focusing, hosing, filamentation) strongly influence pulse evolution, wakefields, and dispersion. Conclusions drawn from purely 1D geometry may not hold in higher dimensions."
      }
    ],
    "think_summary": [
      "**Identifying critical problems**\n\nThe user wants a JSON list outlining up to five significant issues. I need to focus on critical errors, not minor ones. First, I’ll evaluate the validity of assumptions, particularly regarding the Lorentz transformations and the PCM frame. There seems to be a conflation between local and global velocities, which might contradict observations. Lastly, the derivation involving the energy-momentum four-vector might misapply principles by treating electromagnetic fields like particles, leading to complications with the invariance of energy.",
      "**Identifying critical issues**\n\nThe derivation of dH/dt=0 in the PCM frame may not hold if group velocity changes, creating potential circular reasoning. I need to check if they properly factor in radiation reactions, especially at high gamma factors, which may be significant. They also apply the quasi-static approximation beyond valid regimes, ignoring variations in pulse shape. Furthermore, they could misinterpret radiation dispersion, failing to recognize that no rest frame exists for electromagnetic fields. I will pinpoint the top five critical issues based on these observations."
    ],
    "token_usage": {
      "input": 29151,
      "thinking": 1344,
      "output": 712
    }
  },
  {
    "entry_id": 63,
    "retraction_id": "2006.15984v5",
    "paper_id": "2006.15984v4",
    "retraction_comment": "This paper has been withdrawn by the author due to a crucial definition error of Canonical Huffman Table",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Surrogate objective for file‐size change may not reflect true bit‐level costs",
        "Location": "Section 4.1.2, Equations (5)–(8)",
        "Explanation": "The authors replace actual post‐embedding VLC lengths with self‐information estimates (Eq. (6)) derived from estimated frequencies. Huffman code lengths must be integer and subject to canonicalization and marker‐escaping rules; the surrogate log-based objective can diverge substantially from real bit counts, so the optimization may select mappings that appear optimal under the approximation but actually increase file size."
      },
      {
        "Problem": "Arbitrary selection of only 10 zero-frequency symbols for mapping",
        "Location": "Section 5.1.1",
        "Explanation": "The method fixes the number of RSs (run/size symbols) to be remapped at 10 based on empirical trials. No theoretical justification or adaptive criterion is provided, so the approach may fail on images whose RS distributions require more (or fewer) symbols to meet capacity or file‐size goals, undermining the claimed generality and optimality."
      },
      {
        "Problem": "Coarse GA encoding restricts mapping multiplicities to {1,2,4,8}",
        "Location": "Section 5.1.2",
        "Explanation": "By limiting each xi (the number of VLCs mapped to a symbol) to only four discrete values, the solution space is severely pruned and many potentially better mappings (e.g. xi=3,5,6,7) are excluded. This ad hoc quantization may prevent finding true optimal or even near-optimal code mappings and biases the reported performance."
      },
      {
        "Problem": "Ignoring byte alignment, zero‐byte padding and marker‐escaping overhead",
        "Location": "Section 4.1 (paragraph after Eq. (8))",
        "Explanation": "The authors explicitly exclude from F(x) any overhead due to byte alignment, zero‐byte stuffing or JPEG marker-escaping. In practice these overheads can add up to several bytes per DHT or scan segment and may outweigh the small gains predicted by the idealized model, calling into question the real file‐size preservation results."
      },
      {
        "Problem": "Capacity estimation formula neglects variable‐length code boundary costs",
        "Location": "Section 4.1, Equation (2)",
        "Explanation": "The embedding capacity C(x)=∑ fi·⌊log₂(xi)⌋ assumes each occurrence of a mapped symbol conveys exactly ⌊log₂(xi)⌋ payload bits. It overlooks the bit‐alignment and prefix‐code boundary effects inherent to VLC streams, so actual capacity may be lower (or higher) than predicted, risking failed embedding or misleading capacity/vs‐size trade-offs."
      }
    ],
    "think_summary": [
      "**Evaluating capacity formula flaws**\n\nIt seems the capacity formula I'm considering is flawed. It should involve the floor of log2 and the mapped codes, but it overlooks important factors like the code for appended bits and their distribution. They might be ignoring the overhead of appended bits, which is significant.\n\nI need to highlight a few critical problems here: \n\n1) Their approximation for VLC bit lengths is based on estimated frequencies rather than actual Huffman-generated lengths, which could lead to inaccurate code mapping optimizations. \n2) Also, selecting only 10 NFRSs for mapping appears arbitrary and lacks a solid theoretical foundation.",
      "**Identifying critical flaws**\n\nIt seems there are several key issues undermining the universal claims of the approach discussed. For one, the hard-coded restriction of certain values in GA encoding can limit search space and potentially exclude optimal solutions. Secondly, the overhead of byte alignment and zero-byte padding isn’t accounted for, which might significantly impact file size calculations. \n\nLastly, they assume uniform distribution of additional data bits without analyzing the risks of data extraction errors, which could be a problem. Additionally, their capacity constraint might misestimate actual capacity due to overhead, leading to possible embedding failures.",
      "**Outlining critical issues**\n\nI think it's essential to address the lack of attention to appended bits after mapping, as code mappings can alter their sequences. This creates discrepancies in file size calculations. \n\nHere’s how I'll organize the critical points:\n\n1) The reliance on a surrogate objective function for file size may not accurately reflect actual changes.\n2) Selecting only 10 NFRSs for mapping lacks theoretical backing.\n3) GA encoding's strict limits on values reduces potential solutions.\n4) Ignoring byte alignment and padding can invalidate size preservation claims.\n5) Assuming uniform distribution of bits post-encryption might lead to inaccurate estimates. \n\nNow, I’ll prepare a JSON list to summarize these."
    ],
    "token_usage": {
      "input": 30496,
      "thinking": 1536,
      "output": 616
    }
  },
  {
    "entry_id": 64,
    "retraction_id": "1512.06616v5",
    "paper_id": "1512.06616v4",
    "retraction_comment": "Withdrawn after detection of a major error in the C-E expansion",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Neglect of evanescent (classically forbidden) modes in scattering-induced transmission conditions",
        "Location": "Section 3, Equations (5)–(7)",
        "Explanation": "The mapping between incoming momentum p and outgoing p′ via sc|p| = s′c|p′| + δV may fail to admit a real p′ when s c|p| < δV (i.e. p′x imaginary). Physically this corresponds to evanescent (tunneling) modes in the sharp potential region, but the model treats all such momenta as if there were propagating transmitted waves, imposing transmission conditions even where T=0 and R=1. Ignoring evanescent decay invalidates the interface coupling for sub-threshold energies."
      },
      {
        "Problem": "Unjustified solvability of the coupled Milne boundary‐layer problems",
        "Location": "Section 5.2, Equations (43a)–(43c) and Theorem 5.2",
        "Explanation": "The paper asserts existence of four exponential end-states for a system of four half-space, half-range Milne problems coupled by a non-homogeneous transmission condition at ξ=0, yet provides no argument that this coupled system satisfies the coercivity or positivity conditions guaranteeing well-posedness. Classical Milne theory covers scalar or decoupled systems; the coupled spinorial case may lack the necessary spectral gap for exponential decay."
      },
      {
        "Problem": "Ad hoc Maxwell-Boltzmann approximation invalidates functional transmission conditions",
        "Location": "Section 5.2, Equations (51)–(53)",
        "Explanation": "To reduce the full momentum-dependent transmission identity for Fermi-Dirac distributions to scalar density relations, the authors replace (1 + e^{…})^{-1} by e^{…} (Maxwell-Boltzmann), equate exponentials, and drop constant terms. This approximation fails at low energies (where quantum degeneracy is significant) and cannot ensure the original functional equation holds for all p, undermining the derivation of the diffusive transmission conditions."
      },
      {
        "Problem": "Oversimplified BGK collision operator violating physical conservation laws",
        "Location": "Section 4, Equations (8)–(11)",
        "Explanation": "The collision term Cs = (w_eq - w_s)/τ enforces only density relaxation towards a local Fermi-Dirac distribution, neglecting conservation of momentum and energy. In graphene, carrier-carrier and carrier-phonon interactions generally conserve energy (and sometimes momentum), so a one-moment BGK model does not correctly capture the true relaxation dynamics. Consequently, the derived drift-diffusion limit may misrepresent transport coefficients and fail to respect fundamental conservation laws."
      },
      {
        "Problem": "Non-self-consistent treatment of electrostatic potential coupling",
        "Location": "Section 3 (end) and Section 6, Model (54)",
        "Explanation": "The quantum region uses a fixed sharp potential V in the scattering problem, while the classical regions use a separate smooth potential U. No Poisson equation or self-consistent feedback of the computed carrier densities on V or U is included. In real charge transport, space-charge effects modify the potential profile, so decoupling the potentials from the carrier distributions leads to an internally inconsistent model that cannot capture essential electrostatic screening or barrier lowering."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 10050,
      "thinking": 2112,
      "output": 765
    }
  },
  {
    "entry_id": 65,
    "retraction_id": "2309.14057v2",
    "paper_id": "2309.14057v1",
    "retraction_comment": "Our description in Chapter 3, Section 3.2 of the paper is too repetitive with the paper \"Object detection meets knowledge graphs\". There is an error in the description of formula (5) in Section 3.3. And a detailed reasoning process is required for formula (5). Therefore, we wish to request a retraction of the paper",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Inconsistent and ill-specified dimensionality in the GRM mapping equations",
        "Location": "Section 3.4 (Equations 7–8) and Figure 5",
        "Explanation": "The paper defines a compatibility weight h_{v→x_i} via h = exp([v,x_i]W)/Σ exp([v,x_i]W) as a scalar for each class node v and feature location x_i, but [v,x_i] is a 2D-vector (concatenation of D-dim embeddings), W is given as a D×D convolutional weight, and H_v is said to be C×(H·W). Then Eq. (8) writes X^{l+1} = σ(H_v V^{l+1} + X), implying H_v V^{l+1} is (C·HW × C)×(C×D) or similar—a clear dimension mismatch. The paper never explains how these matrices align, so the core GRM feature fusion is mathematically unsound."
      },
      {
        "Problem": "Unspecified random‐walk‐with‐restarts parameters yield an ill‐defined label correlation matrix",
        "Location": "Section 3.2 (Equation 3)",
        "Explanation": "They claim to compute E_{c,c′} by letting R_{c,c′}=lim_{t→∞}p(r_t=c′|r0=c;α) for a random walk with restarts, but omit any definition or choice of restart probability α or stopping criterion. In standard theory, an infinite‐step random walk without restart converges to a stationary distribution independent of start node, so without a formal personalized PageRank or restart mechanism the matrix E is undefined or degenerate. This undermines the validity of all downstream uses of E in the GCN."
      },
      {
        "Problem": "Benchmarking on MS-COCO contradicts claims of state-of-the-art performance",
        "Location": "Section 5.1.2 (Table 3)",
        "Explanation": "Table 3 reports CSE (Kweon et al. 2021) as an image‐only (I) method achieving 52.6% mIoU on MS-COCO val, which already outperforms the paper’s 46.1% result. Yet the authors assert their method is top among I-only methods. Furthermore, they state that 'methods employing additional saliency maps fare worse,' but CSE actually uses saliency. This mislabelling and omission of stronger baselines invalidates the paper’s major empirical claims."
      },
      {
        "Problem": "Unclear integration of GCN‐derived classifiers into CAM generation",
        "Location": "Section 3.3 and Figure 3(a)",
        "Explanation": "The paper introduces GCN to produce a set of interdependent classifiers V_c, then multiplies these with global‐pooled features to get scores used in a focal‐loss, and claims to use them for CAM generation. However, it is never specified whether CAMs are generated from the original CNN classifier x′, from the GCN‐derived V_c, or some combination. Without this detail, it is impossible to attribute any CAM improvement to the graph reasoning module."
      },
      {
        "Problem": "Symmetric normalization of ConceptNet edges may distort inherently directional relations",
        "Location": "Section 3.1 (Equation 2)",
        "Explanation": "ConceptNet contains directed, semantic relations (e.g. IsA, UsedFor), but the authors immediately symmetrize E by E+I and Q^{-1/2}ÊQ^{-1/2}. This discards relation directionality and type information, potentially mixing unrelated semantics. No ablation is shown comparing directed vs. symmetrized graphs, so it is unclear whether this heavy simplification still captures valid inter-class knowledge."
      }
    ],
    "think_summary": [
      "**Identifying flaws in comparison**\n\nThe table shows CSE at 52.6, indicating a major flaw in comparison or reporting. They've mis-cited results for MS-COCO, failing to recognize CSE performance. They mention CSE as using saliency maps, but classify it incorrectly. Their claim states that methods using additional saliency maps perform worse, but this contradicts their own categorization. This mislabeling leads to confusion. Additionally, their GRM module only evaluates one layer at ASPP without testing multi-level integration, questioning their generalization claim.",
      "**Identifying critical problems**\n\nI’ve pinpointed several critical issues. First, there's a technical error in the GRM regarding dimensionality and equations, leading to mapping discrepancies. Second, the computation of the semantic relationship matrix is flawed since it lacks details on random walks and parameters. Third, the evaluation on MS-COCO mislabels CSE, leading to contradictions that invalidate their SOTA claim. Fourth, the integration of GCN classifiers into CAM is unclear, creating confusion. Lastly, there's a lack of clarity about ConceptNet relations being symmetric or directional, which could distort relationships."
    ],
    "token_usage": {
      "input": 30469,
      "thinking": 1792,
      "output": 855
    }
  },
  {
    "entry_id": 66,
    "retraction_id": "1206.0667v2",
    "paper_id": "1206.0667v1",
    "retraction_comment": "This paper is withdrawn by the author because the paper did not prove the second inequality of (4.3), which is unlikely to hold in general",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Invalid transversality argument in Proposition 3.4",
        "Location": "Section 3, Proposition 3.4 (page 8)",
        "Explanation": "The proof asserts that by a generic perturbation of the Hamiltonian G one can independently perturb the flows φ^s_G for all s to achieve transversality of the chain Ξ’s self‐intersections. However, G(s,x) is a single time‐dependent Hamiltonian, and its time‐s flows cannot be varied independently for different s. Without a legitimate infinite‐dimensional transversality framework, the claim that the self‐intersection locus of Ξ has codimension ≥1 (hence measure zero) is unfounded, undermining the key area estimate (3.8). "
      },
      {
        "Problem": "Unjustified use of the coarea formula on the singular chain Ξ",
        "Location": "Section 3, inequality (3.8)",
        "Explanation": "The application of the coarea formula requires a Lipschitz map from a smooth or rectifiable manifold, whereas Ξ is only a piecewise‐smooth chain with corners and self‐intersections. No verification is given that the projection π_1 restricted to Ξ satisfies the regularity or rank conditions for coarea to hold. This casts doubt on the estimate of ∫_Ξ dΘ∧Ω₁ and thereby on Theorem 3.1."
      },
      {
        "Problem": "Assumption of a uniformly C⁰-small engulfable homotopy is unjustified",
        "Location": "Section 1 (after Theorem 1.1) and Section 3 (start of proof of Theorem 3.1)",
        "Explanation": "The argument requires a smooth Hamiltonian homotopy H(s,t,x) contracting the given topological loop φ_F such that each intermediate flow φ^t_H(s) remains C⁰-close to the identity (engulfability). For an arbitrary contractible topological Hamiltonian loop F, no construction or reference guarantees the existence of such a uniformly small homotopy. Without this smallness, the key estimates depending on d(φ_G,id)→0 need not hold, invalidating the contradiction in Section 4."
      },
      {
        "Problem": "Extension of the generating function over Σ_G;Δ² lacks smoothness for Stokes’ theorem",
        "Location": "Section 4, Lemma 4.2 and Lemma 4.3",
        "Explanation": "The paper extends the basic phase function constantly over the 2-simplices Σ_G;Δ² to form the cycle Σ_add_G, then applies Stokes’ theorem on the 'flat' surface C_{x₀x}. But Σ_G;Δ² is only a topological union of affine triangles in the fiber; there is no guarantee it sits as a smooth (or even piecewise-smooth) submanifold with oriented boundary. This invalidates the use of Stokes’ theorem in Lemma 4.3 and the subsequent quantitative estimates."
      },
      {
        "Problem": "Undefined mod-2 degree in Lemma 2.3 undermines surjectivity of π₂",
        "Location": "Section 2, Lemma 2.3",
        "Explanation": "Lemma 2.3 asserts that the projection π₂: Σ_add_F→M has mod-2 degree 1 and hence is surjective. However, Σ_add_F is constructed by gluing strata and 'cliff-wall' pieces without a clear manifold or cycle structure. No proof is given that Σ_add_F is a well-defined mod-2 cycle or that π₂ is transverse, so the mod-2 degree is not rigorously defined. Without surjectivity of π₂, Lemma 4.2 (existence of x_e with π₂(x)=π₂(x_e)) fails, collapsing the final comparison argument."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 11864,
      "thinking": 2112,
      "output": 866
    }
  },
  {
    "entry_id": 67,
    "retraction_id": "1503.03000v3",
    "paper_id": "1503.03000v2",
    "retraction_comment": "This paper has been withdrawn by the author because the renormalization constructed in it is not compatible with renormalization in perturbative quantum field theory. Main Theorem of \\S5 is not true",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Undefined existence and convergence of the time-ordered exponential in the infinite-dimensional Weyl–Moyal algebra",
        "Location": "§1, equation (5)",
        "Explanation": "The paper defines U via the formal series T exp∫Hˆ dt in the infinite-dimensional Weyl–Moyal algebra without specifying any topology, norm, or domain on which this series converges or even makes sense beyond a purely formal power series. No analytic or functional-analytic foundation is provided, so the non-perturbative evolution operator is not rigorously constructed."
      },
      {
        "Problem": "Unjustified assumption on the divergence structure of regularized graph integrals",
        "Location": "§2, paragraph following equation (8)",
        "Explanation": "The key technical hypothesis—that for every graph Γ the divergent part T(Uε(Γ)) is a polynomial in ε⁻¹ and log ε— is asserted without proof. In general, the asymptotic structure of divergences depends delicately on the theory and regularization. Without establishing this for the class of Hamiltonians and regularizations considered, the renormalization machinery cannot be guaranteed to apply."
      },
      {
        "Problem": "Use of the Hopf algebra of all graphs fails to address overlapping divergences",
        "Location": "§2, equations (7)–(8)",
        "Explanation": "Unlike Connes–Kreimer’s Hopf algebra of one-particle–irreducible graphs, the algebra generated by all graphs does not naturally separate primitive from composite divergences. There is no demonstration that the coproduct in (8) properly organizes overlapping divergences, so the recursive R-operation may not subtract subdivergences correctly in general."
      },
      {
        "Problem": "No control over convergence or summability of the renormalized graph expansion",
        "Location": "§2, equation (15)",
        "Explanation": "The renormalized evolution operator U˜ is defined as an (infinite) sum of R(Γ)|ε=0 over all graphs, but there is no discussion of convergence or summability of this series in SV′. As with most perturbative expansions, the series is likely divergent, and without additional structure it cannot be given a non-formal meaning."
      },
      {
        "Problem": "Main equivalence with standard renormalized S-matrix is stated without proof",
        "Location": "§5, Theorem",
        "Explanation": "The central claim—that the constructed S-matrix and Green functions reproduce those of perturbative QFT—is merely asserted and left unproved. Without demonstrating this correspondence (including unitarity, analytic continuation, and agreement of counterterms), the paper’s main conclusion remains unjustified."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 6881,
      "thinking": 1280,
      "output": 615
    }
  },
  {
    "entry_id": 68,
    "retraction_id": "2010.16005v2",
    "paper_id": "2010.16005v1",
    "retraction_comment": "There is a potential issue with trilinear estimates and the indices needs to be adjusted",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Incorrect application of polynomial bounding (Lemma 5) in estimate (3.9)",
        "Location": "Section 3, proof of Theorem 2, equation labeled (3.9)",
        "Explanation": "The authors bound a fourth‐degree polynomial in ξ₂ by a product of lower‐degree factors using Lemma 5, which only applies to two polynomials of the same leading degree. Here the left‐hand side behaves like |ξ₂|² for large |ξ₂|, while the right‐hand side is O(|ξ₂|^{1.5}), so the inequality fails at infinity and the subsequent uniform estimate breaks down."
      },
      {
        "Problem": "Change of variables dξ₁→dμ invalid at degenerate points",
        "Location": "Section 3, proof of Theorem 2, around the change μ=τ−γ(ξ−ξ₁)−τ₂−γ(ξ₁−ξ₂)",
        "Explanation": "The Jacobian dμ/dξ₁ vanishes when the quadratic coefficient 3ξ−3ξ₂−2 equals zero, but the authors do not treat this resonance case separately. As a result, the integral estimate using Lemma 4 fails where the change‐of‐variable is singular."
      },
      {
        "Problem": "Unjustified uniform bound requiring exact choice of b,b₀",
        "Location": "Section 3, final paragraph of Theorem 2 proof (‘To finish the proof… b₀ ≤3/4’) ",
        "Explanation": "The argument requires 1−b₀≥1/4 and then sets b₀=3/4 exactly. At this endpoint the factor |ξ|^{3/4}/⟨τ−γ(ξ)⟩^{1−b₀} need not be bounded by 1 for all (ξ,τ), yet no strict inequality or additional decay is provided, invalidating the claimed L^∞ control."
      },
      {
        "Problem": "Failure to handle low‐frequency (ξ≈0) and resonant interactions in trilinear estimate",
        "Location": "Section 3, proof of Theorem 2, whole trilinear estimate argument",
        "Explanation": "The proof does not address the region where one or more frequencies are small or satisfy subtle resonance relations, which in mixed‐dispersion equations typically require s>1/2. Claiming the estimate holds down to s=0 overlooks known counterexamples and lack of smoothing at low regularity."
      },
      {
        "Problem": "Omission of proof of time‐continuity and embedding into C([−T,T];H^s)",
        "Location": "Section 2, proof of Theorem 1, after contraction mapping",
        "Explanation": "While a fixed‐point in X^{s,b} is obtained, the authors do not show how this yields a solution continuous in time with values in H^s, nor verify further regularity properties. Without this embedding, well‐posedness in the standard sense is not established."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 13607,
      "thinking": 5632,
      "output": 691
    }
  },
  {
    "entry_id": 69,
    "retraction_id": "2305.08639v2",
    "paper_id": "2305.08639v1",
    "retraction_comment": "Proof of Theorem 5.4 is wrong. In particular, the map that is claimed to be a homomorphism, it is not a homomorphism. Also, some of the main results of the paper rely on that Theorem",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Invalid subgroup inclusion Sp_n(Z)[m] ⊆ Sp_n(Z)[2] for odd m",
        "Location": "Proposition 3.1 (page 9)",
        "Explanation": "The proof of Proposition 3.1 asserts that for odd m one has Sp(Z)[m] < Sp(Z)[2], and hence any element in Sp(Z)[m] arises from B_n[m]. But congruence subgroups of coprime levels do not nest: being ≡1 mod m (m odd) does not imply ≡1 mod 2, so Sp(Z)[m] is not contained in Sp(Z)[2]. This invalidates the identification ρ(B_n[m])=Sp(Z)[m] and thus breaks Theorem D."
      },
      {
        "Problem": "Misapplication of Newman–Smart level quotient theorem",
        "Location": "Lemma 3.2 (page 10)",
        "Explanation": "Lemma 3.2 applies Theorem 2.1 to the quotients Sp(Z)[2ℓ]/Sp(Z)[2pℓ], but Theorem 2.1 requires the higher level subgroup to divide the lower (i.e. LL′|L), whereas here 2pℓ>2ℓ so 2pℓ∤2ℓ. Hence the claimed isomorphism Sp(Z)[2ℓ]/Sp(Z)[2pℓ]≅sp(Z/p) is unfounded, breaking Lemma 3.2 and Theorem E."
      },
      {
        "Problem": "Unjustified identification B₄[3]=G₃₄",
        "Location": "Introduction, paragraph before Corollary 1.1 (page 3)",
        "Explanation": "The paper asserts that for n=4 the mod-3 congruence subgroup B₄[3] is generated solely by cube powers of half-twists and hence equals G₃₄. However, no proof is given that the ‘mod-3 center maps’ can be omitted or expressed as products of σᵢ³, so B₄[3] may strictly contain G₃₄. This undermines the computations of H₁(B₄[3]) via H₁(G₃₄)."
      },
      {
        "Problem": "Insufficient justification for generating set of B₄[3]",
        "Location": "Proposition 2.2 (page 6–8)",
        "Explanation": "Proposition 2.2 claims that a specific list of 12 conjugates of σᵢ³ generates B₄[3], but the proof consists of a long sequence of ad hoc braid relations without a clear structural argument or reference. If the generating set is incorrect or incomplete the later abelianization computations for B₄[3] are invalid."
      },
      {
        "Problem": "Surjectivity of the Johnson-type homomorphism φ not established",
        "Location": "Theorem 5.5 (page 17–18)",
        "Explanation": "Theorem 5.5 constructs a map φ:G₃ₙ→Hom(H,Θ_N) and claims existence of a free abelian quotient of rank n−1. However, no argument is given that φ is surjective onto a rank-(n−1) subgroup (or that its image has the claimed rank). Without surjectivity the asserted epimorphism in Theorem A is unproved."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 28125,
      "thinking": 2944,
      "output": 781
    }
  },
  {
    "entry_id": 70,
    "retraction_id": "1706.06242v2",
    "paper_id": "1706.06242v1",
    "retraction_comment": "An error has occurred: The classical Kolmogorov result about characterization of compactness is usually applied with linear operators. Unfortunately, commutator of Hardy-Littlewood maximal operator is a sublinear one",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Unjustified extension of compactness from C_c^∞ symbols to all CMO symbols",
        "Location": "Theorem 1.2, end of section 3",
        "Explanation": "The authors prove compactness of [Σ_b,M] only for b∈C_c^∞ but then assert the same for all b∈CMO without showing that the commutator depends continuously on the symbol in the BMO-norm. Without an approximation argument or continuity estimate ∥[Σ_{b_n},M]−[Σ_b,M]∥→0 as b_n→b in BMO, the result cannot extend from C_c^∞ to CMO."
      },
      {
        "Problem": "Missing definition and boundedness of the bilinear fractional maximal operator M_α",
        "Location": "Proof of Theorem 1.1, beginning of (a)⇒(b), page 5",
        "Explanation": "The inequality [b_1,M]_1(f_1,f_2)(x) ≤ ∥b_1∥_{Lipα} M_α(f_1,f_2)(x) invokes a bilinear fractional maximal operator M_α that is never defined or referenced, nor is its L^{p_1}×L^{p_2}→L^q boundedness established. This gap undermines the argument that the commutator is finite or bounded."
      },
      {
        "Problem": "Use of balls instead of cubes without equivalence justification",
        "Location": "Claims 1 and 2 in the proof of Theorems 1.1 and 1.2, pages 5–7 and 9–11",
        "Explanation": "Although M is defined via suprema over cubes Q, the proofs of the key estimates resort to balls B_1, B_2 without explaining why replacing cubes by balls preserves the required bounds for the bilinear (fractional) maximal operator. The lack of a cubes↔balls equivalence may invalidate the sup-difference estimates."
      },
      {
        "Problem": "Application of the Sobolev–Besov embedding at the critical index without justification",
        "Location": "Proof of Theorem 1.1, (b)⇒(a), page 8",
        "Explanation": "The embedding F˙_{p,∞}^α↪L^q is used at the critical smoothness α=n(1/p−1/q), but the authors do not cite or verify the precise conditions under which the homogeneous Triebel–Lizorkin space F˙_{p,∞}^α continuously embeds into L^q. Without a rigorous reference or proof, the deduction that boundedness into F˙ implies boundedness into L^q may fail."
      },
      {
        "Problem": "Reliance on unproved boundedness of the bilinear fractional maximal M_α in L^p estimates",
        "Location": "Proof of Theorem 1.1, end of (a)⇒(b), page 8",
        "Explanation": "The final norm estimate uses the fact that M(M_s(f_1)M_s(f_2)) is bounded on L^p (to deduce ∥M_s(f_1)M_s(f_2)∥_{L^p}≤C∥f_1∥∥f_2∥). However, this implicitly relies on boundedness properties of a bilinear maximal operator composed with fractional maximal functions, none of which are proved or cited in the paper. This gap casts doubt on the boundedness claim."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 9829,
      "thinking": 9920,
      "output": 800
    }
  },
  {
    "entry_id": 71,
    "retraction_id": "1510.01988v2",
    "paper_id": "1510.01988v1",
    "retraction_comment": "This paper has been withdrawn by the authors due to an error in Lemma 2; terms involving the normal derivative of $\\rho$ are missing",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Misapplication of the Dominated Convergence Theorem in the proof of Lemma 5",
        "Location": "Section 2, proof of Lemma 5",
        "Explanation": "The authors invoke the Dominated Convergence Theorem on the integral\n\n    ∫₀¹ t |t x−y|^{−k−2} |(t x−y)_⊥|² dt\n\nas x→y, claiming a uniform integrable bound.  But near t→1 the integrand behaves like (1−t)^{−(k−2)}, which is not integrable for k≥3.  Hence the DCT argument as stated fails and the asserted o(1/|x−y|^{k−1}) estimate lacks justification."
      },
      {
        "Problem": "Unjustified half-sphere contribution from removing a small geodesic ball at a boundary point",
        "Location": "Section 2, near display (2) in the proof of Theorem 1",
        "Explanation": "When excising the small geodesic ball D_ε(y) centered at a boundary point y∈∂Σ, the new boundary Σ∩∂D_ε(y) is a half-sphere in (Σ,ρ²g), not a full sphere.  The authors assert by scaling that its measure is (½) ωₖ ρ^{k−1}(r) ε^{k−1}+o(ε^{k−1}) without any justification of the ‘half’‐factor nor a careful treatment of geodesic balls in a manifold with boundary."
      },
      {
        "Problem": "Typographical inversion in the divergence estimate for U^⊤",
        "Location": "Section 2, Lemma 2 (bounding div_{Σ,ρ²g} U^⊤)",
        "Explanation": "The text writes “k ρ^k(|x|) |x|^{k+2} I(|x|) ≤ 1/|x|^2,” whereas the correct relation needed is\n\n    k I(|x|) / (ρ^k(|x|) |x|^{k+2}) ≤ 1/|x|^2,\n\nwhich follows from the monotonicity ρ(s)≤ρ(|x|) on [0,|x|].  The misplacement of ρ^k and |x|^{k+2} between numerator and denominator is nontrivial to the argument."
      },
      {
        "Problem": "Ambiguity between Euclidean and hyperbolic ball radii in Corollary 1",
        "Location": "Introduction & Corollary 1",
        "Explanation": "Theorem 1 is formulated for the Euclidean ball Bⁿ(r) (|x|<r) equipped with metric ρ²g, but Corollary 1 invokes ‘geodesic balls of radius r in Hⁿ’ without clarifying that in the ball model the Euclidean radius s satisfies r=2 arctanh(s).  The identification of the Euclidean radius in (ℝⁿ,ρ²g) with the hyperbolic geodesic radius is left implicit."
      },
      {
        "Problem": "Use of ‘Σ is infinitesimally Euclidean’ without rigorous justification",
        "Location": "Section 2, proof of Theorem 1 (just after Lemma 5)",
        "Explanation": "To estimate the conormal near a boundary point y∈∂Σ the authors claim that in geodesic normal coordinates Σ looks Euclidean so that ν=−(x−y)/(ρ(r)|x−y|)+o(1).  A detailed argument using local parametrizations or exponential coordinates in the conformal metric is missing."
      }
    ],
    "think_summary": [
      "**Analyzing geodesic and Euclidean radius**\n\nI'm noticing that if Bn has a geodesic radius, it translates to a Euclidean representation ball with a specific radius conversion formula. There's some ambiguity here about whether they are equating the geodesic and Euclidean radii, as the mapping isn't clearly mentioned. My understanding is that the Euclidean ball radius corresponds to the hyperbolic geodesic radius. They did miss a small detail in specifying this relationship, but it doesn't seem to be a critical mistake. I'll want to verify the condition C2 as well.",
      "**Evaluating proof errors**\n\nI'm analyzing a proof involving Euclidean and hyperbolic geometries, noting that while certain aspects are correct, the sign in conformal monotonicity shouldn't apply to decreasing densities. Given the instructions to ignore minor issues, I’m trying to find up to five critical problems, and so far I haven’t found any that invalidate the conclusions. There are minor typos and gaps, but they seem correctable.\n\nHowever, I did spot a potential issue with the dominated convergence misuse in Lemma 5, which could be critical. They don't justify the bound adequately, and it requires separate estimates. Another gap is the vague definition of the D_epsilon geodesic ball and the area factor argument, which isn't fully justified. These are gaps but might not fundamentally undermine the final results.",
      "**Identifying key proof issues**\n\nIn assessing the user's request for critical problems, I’ll highlight five main issues: \n1) Misapplication of dominated convergence in Lemma 5,\n2) Justification lacking for the boundary half-ball measure,\n3) Sloppy notation in Lemma 2's inequalities,\n4) Potential confusion between geodesic and Euclidean balls in the hyperbolic model,\n5) The phrase \"infinitesimally Euclidean\" isn't justified. \n\nI’ll document these as JSON entries to keep track and ensure clarity in the presentation."
    ],
    "token_usage": {
      "input": 6994,
      "thinking": 10688,
      "output": 861
    }
  },
  {
    "entry_id": 72,
    "retraction_id": "1212.5552v2",
    "paper_id": "1212.5552v1",
    "retraction_comment": "There are signal errors in eqs. (17) to (25)",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Residual entropy at ρ=2 phase boundaries is computed as S = ln(2) – ln(3 – √5), implying a non-integer effective degeneracy",
        "Location": "Section V (paragraph following Fig. 5), also discussion around Eq. (53)",
        "Explanation": "Residual entropy in a system with discrete microstates should be of the form S = ln g with g an integer degeneracy per cell. The value ln(2) – ln(3 – √5) ≃ 0.962 cannot correspond to ln g for any integer g, indicating a miscalculation or misinterpretation of the ground-state degeneracy and thus undermining the claim of a \"non-trivial\" residual entropy."
      },
      {
        "Problem": "Inconsistent notation and misuse of Coulomb parameters V, V1, and V2",
        "Location": "Definition of the Hamiltonian (Eq. 2), eigenvalues (Eqs. 16–21), and Boltzmann weights (Eqs. 41–43)",
        "Explanation": "The paper initially defines V1 as the plaquette Coulomb repulsion and V2 as the nodal–plaquette repulsion, but subsequently writes eigenvalues in terms of V and V1 (omitting subscripts) and Boltzmann factors mixing both V and V2. This notational inconsistency makes it unclear which interaction appears in each term and casts doubt on the correctness of the exact eigenvalues and partition function."
      },
      {
        "Problem": "Mischaracterization of the twofold-degenerate ground states at ρ = 2",
        "Location": "Section III, Eqs. (24)–(30) and accompanying text describing |F1⟩ and |F2⟩",
        "Explanation": "The assignment of energies E1 and E4 to the two degenerate states |F1⟩ and |F2⟩ uses nd=2 for one and nd=0 for the other, yet the corresponding particle counts (triangle plus nodal) do not both sum to 2 in the way described. This inconsistency in nodal versus plaquette occupancy undermines the identification of the ρ=2 region and the resulting zero-temperature phase boundaries."
      },
      {
        "Problem": "Omission of constant energy shifts in the decoration mapping",
        "Location": "Decoration transformation step (Eqs. (38)–(46)) and definition of the mapping prefactor f",
        "Explanation": "The mapped effective Hamiltonian H_e omits the constant terms arising from the trace over plaquette degrees of freedom (e.g., additive shifts in H_i,i+1), yet these contribute an overall factor to the partition function. Failing to track all constant shifts consistently can lead to an incorrect normalization f and thus wrong free energy and thermodynamics."
      },
      {
        "Problem": "Double counting of nodal sites and boundary conditions not explicitly handled",
        "Location": "Global Hamiltonian definition H = Σ_i H_{i,i+1} (Eq. 1) and subsequent transfer-matrix construction (Eq. 47)",
        "Explanation": "Each H_{i,i+1} involves both nodal sites d_i and d_{i+1}, so summing over i with periodic boundary conditions leads to each nodal site appearing twice. The paper does not explicitly address this overlap or the correct counting of boundary contributions, which may alter the effective transfer matrix and hence the partition function per cell."
      }
    ],
    "think_summary": [
      "**Questioning Residual Entropy**\n\nI'm looking at how they define S as ln(2) - ln(3 - √5) and wondering if that's even positive. It looks like 3 - √5 is around 0.764, leading to ln(2) - ln(0.764) being approximately 0.962. But then there's the mention of microstates—are there truly three states here? They say one is non-degenerate and the others are two-fold degenerate, which feels off. Why sqrt(5)? That’s tied to the golden ratio, which seems suspicious. I need to explore this further.",
      "**Exploring Residual Entropy Issues**\n\nI'm grappling with the idea of non-integer degeneracy, which feels odd. It seems they're suggesting residual entropy per cell is around 0.962, but can degeneracy truly be non-integer? Plus, isn't the microstate growth exponential with cell numbers? They might be calculating residual entropy per cell through eigenvalues, but it gets complex with partition functions. Residual entropy not matching ln(g) is unusual, especially if there’s a coupling effect. Their interpretations seem flawed, and I need to dive deeper into their method and assumptions—there may be critical errors here.",
      "**Analyzing Eigenvalue Confusion**\n\nI'm puzzling over the eigenvalues which seem to depend on parameters V and V1, but the notation is mixed up. It looks like they use V without clear subscripting, and this leads me to question if V is really meant to refer to V2. In the equations, they denote interactions but might be reversing definitions, creating confusion. The inconsistency with how they handle V1 and V2 could result in significant errors in their calculations regarding Boltzmann weights and thermodynamics. I need to clarify their notation to understand the implications better.",
      "**Identifying Potential Issues**\n\nI'm considering several critical problems in this analysis. First, there's a potentially incorrect calculation of residual entropy that implies a non-integer degeneracy, which isn't physical. Notational confusion around the Coulomb parameters could lead to ambiguous expressions in the Boltzmann weights. Mischaracterization of degenerate ground states may affect the phase diagram, and essential constant energy shifts might have been omitted in the mapping. Lastly, there's a risk of errors due to boundary conditions and double-counting in the Hamiltonian sum, needing clearer treatment. These issues need addressing!"
    ],
    "token_usage": {
      "input": 17406,
      "thinking": 7168,
      "output": 784
    }
  },
  {
    "entry_id": 73,
    "retraction_id": "1612.09148v2",
    "paper_id": "1612.09148v1",
    "retraction_comment": "The second equality in eq. 11 does not hold for the general case. Thus, the conclusion does not follow from the premises and the claim of the paper is not proven",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Incorrect symmetry relation for force constants in Equation (11)",
        "Location": "Equation (11)",
        "Explanation": "The paper asserts Φ_{iα,i′α′}(n)=Φ_{iα,i′α′}(n′) (with R_{n′}=–R_n) without swapping indices.  The correct relation is Φ_{iα,i′α′}(n)=Φ_{i′α′,iα}(–n), so one must interchange the atom and coordinate indices when inverting the cell index.  As written, the derivation of the manifestly real form in Equations (12)–(13) is based on a misapplied symmetry and is therefore invalid."
      },
      {
        "Problem": "Circular reasoning in the unitary diagonalization proof",
        "Location": "Section \"Finally, we need to review ... decomposition:\" (second derivation of reality)",
        "Explanation": "To prove D is real symmetric, the authors invoke that its eigenvectors can be chosen real, then use those real eigenvectors (matrix U) to reconstruct D as U Λ U^T.  But the ability to choose real eigenvectors already relies on D being real symmetric (or at least having the symmetry established in Equations (12)–(13)).  This makes the second proof logically circular and does not independently establish the reality of D."
      },
      {
        "Problem": "Neglect of non‐analytic long‐range Coulomb contributions",
        "Location": "Overall derivation leading to Equation (13)",
        "Explanation": "For polar crystals, the dynamical matrix contains a non‐analytic macroscopic electric‐field term (e.g. the LO–TO splitting) that cannot be written simply as a finite sum of cos(q·R_n).  The paper’s form in Equation (13) omits this contribution, so the claimed general real‐symmetric form fails for ionic materials and near q→0."
      },
      {
        "Problem": "Assumption of perfect infinite crystal and arbitrary q",
        "Location": "Derivation of Equations (12)–(13)",
        "Explanation": "The rearrangement pairing R_n and –R_n requires summation over all lattice vectors and relies on periodic boundary conditions with q commensurate to the supercell.  In numerical implementations with truncated force‐constant ranges or incommensurate q vectors, the pairing breaks down and D(q) may acquire non‐paired imaginary contributions, invalidating the manifestly real form."
      },
      {
        "Problem": "Omission of basis‐site phase factors in the Bloch ansatz",
        "Location": "Equations (7) and (9)",
        "Explanation": "The Bloch ansatz is applied as s_{n i α}(t)=ε_{i α}e^{i q·R_n}/√m_i, omitting the internal basis coordinate τ_i.  The more general form is e^{i q·(R_n+τ_i)}, and the neglected τ_i–τ_{i'} phase can lead to complex off‐diagonal elements in D(q) even when the lattice is centrosymmetric.  By ignoring τ, the paper’s derivation is not valid for crystals with multi‐atom bases."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 4254,
      "thinking": 4480,
      "output": 725
    }
  },
  {
    "entry_id": 74,
    "retraction_id": "1205.5450v3",
    "paper_id": "1205.5450v2",
    "retraction_comment": "The paper has been withdrawn due to an error in the maximal norm estimate that we haven't been able to overcome",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [],
    "think_summary": [
      "**Evaluating critical errors**\n\nThe user wants me to check the attached paper for critical errors that might invalidate its conclusions. I need to provide up to five significant problems in a specific JSON format, focusing on mathematical or logical errors. The paper deals with the well-posedness of the dispersion generalized Benjamin-Ono equation, so I'll need to carefully examine whether they've adequately proven the main Lemma and if their methodology involving various norms is sound. Let’s get started!",
      "**Identifying potential issues**\n\nI think there might not be any fatal errors in the paper, but if I can't find any, I may return an empty list indicating \"no critical problems found.\" However, there’s a potential oversight related to the use of weighted Sobolev spaces. They haven't considered that these spaces aren't Banach algebras under multiplication, which could affect their application of the Leibniz rule. I need to examine the specific inequalities and bounds they've used to ensure they're valid. Let me dig deeper into these mathematical details!",
      "**Finalizing the assessment**\n\nI’ve realized that if a function is in L^p for p>1, it also belongs to L^1, according to Hölder's inequality. So, L^p effectively controls L^1, and they do seem to use a small T. Everything appears consistent, though there might be some oversight with constants, but no critical errors are found. Therefore, I can confidently return an empty list of problems. However, I'll consider noting a potential issue with the smoothness assumption of the solution map if that fits better!"
    ],
    "token_usage": {
      "input": 20728,
      "thinking": 3008,
      "output": 7
    }
  },
  {
    "entry_id": 75,
    "retraction_id": "1806.09512v2",
    "paper_id": "1806.09512v1",
    "retraction_comment": "The calculations are not strictly correct because the Fermi momenta for protons and neutrons were not scaled correctly",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Circular data selection and fitting procedure leading to model bias",
        "Location": "Section III.A (pages 5–7)",
        "Explanation": "The extraction of the phenomenological scaling function f*(ψ*) relies on an initial selection of ‘QE’ data via a density criterion using a provisional f*. Those same data are then refitted to redefine f*, and the process iterated. This circular approach risks absorbing non-QE contributions (e.g. residual inelastic tails, MEC) into the scaling function and parameters kF and M*, undermining the model’s predictive power for truly independent data and inflating its apparent success."
      },
      {
        "Problem": "Neglect of explicit two-body currents (MEC) and other scaling-breaking mechanisms",
        "Location": "Section II (formalism) & Section III (throughout)",
        "Explanation": "SuSAM* assumes factorization of the cross section into one-body responses times a universal f*(ψ*), treating multi-nucleon processes (meson-exchange currents, short-range correlations, inelastic excitations) only as an uncertainty band. By not including MEC explicitly in the fit, the extracted scaling function and mass parameters are contaminated, especially at higher q, calling into question the validity of conclusions drawn for neutrino scattering or kinematics where two-body processes are significant."
      },
      {
        "Problem": "Assumption of a constant relativistic effective mass M* despite clear ω-dependence",
        "Location": "Section II (definition of M*), Section IV.L & Fig. 18 (page 21)",
        "Explanation": "The model fits a single M* per nucleus, yet Fig. 18 shows that the optimal M* inferred from peak positions varies systematically with energy transfer ω, especially at high q where M* decreases. Maintaining M* as a constant introduces a systematic bias in peak centroids and undermines the claim of a universal scaling variable ψ* for all kinematics."
      },
      {
        "Problem": "Ad hoc choice of uncertainty band width and circular χ² error estimation",
        "Location": "Section III.C (page 7)",
        "Explanation": "In the global fit the uncertainty band for f* is fixed arbitrarily to ±0.1 without statistical justification. This same band width is then used to compute a χ² that weights distances of data points to the band center by that fixed error, leading to circular error estimates and no independent assessment of theoretical uncertainty."
      },
      {
        "Problem": "Application of Fermi-gas superscaling to deuteron where Fermi sea assumptions fail",
        "Location": "Section IV.A (page 10)",
        "Explanation": "The deuteron (²H) has only two nucleons and no defined Fermi sea, yet SuSAM* treats it with a Fermi momentum kF and effective mass M* as if nuclear matter were present. While the fit reproduces some data, the physical basis of applying a mean-field scaling approach to ²H is unsound, casting doubt on the universality claim across all A."
      }
    ],
    "think_summary": [
      "**Evaluating fit validity**\n\nI'm pondering how the deuteron is treated as an infinite Fermi gas, which seems flawed because factorization is failing. Yet, the researchers are still fitting it. This might bias their results. \n\nThey mention fitting 50 parameters at once without discussing convergence or starting points, raising the risk of local minima issues. I need to identify critical problems that could invalidate their conclusions, like assumptions of scaling and the circular fitting procedure leading to potentially tautological outcomes.",
      "**Identifying model flaws**\n\nI'm analyzing potential flaws in the model, which might introduce circular reasoning and arbitrary choices. They haven't validated their findings with an independent dataset, which raises concerns. \n\nThe constant effective mass assumption overlooks momentum/energy dependence, leading to systematic biases. Ignoring Coulomb distortion for heavy nuclei makes predictions unreliable, while their uncertainty estimation lacks justification. I think I should highlight the top five critical issues, like the circular data selection, neglect of essential factors, and the internal inconsistency in their effective mass assumption. This undermines the reliability of their conclusions.",
      "**Identifying critical issues**\n\nI'm considering how the deuteron, with only two nucleons, doesn't form a Fermi sea, making the kF formalism inappropriate. It’s important to focus on light nuclei, like 2H, where Fermi gas scaling assumptions fail. \n\nI think the critical problems are the circular data selection bias, neglect of essential mechanisms, the inconsistent effective mass assumption, and the unjustified error estimations. I plan to compile these concerns into a structured list, ensuring each problem includes its location references for clarity."
    ],
    "token_usage": {
      "input": 56769,
      "thinking": 1664,
      "output": 692
    }
  },
  {
    "entry_id": 76,
    "retraction_id": "2102.11234v2",
    "paper_id": "2102.11234v1",
    "retraction_comment": "In step (3) of the proof of Theorem 1.3. it is claimed that $\\delta^2_{N,N} = 2\\delta$. This is not the only case that can occur and therefore there is a gap in the proof",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Conflation of counting‐metric nearest neighbors with geometric nearest neighbors in the proof of Theorem 1.3",
        "Location": "Proof of Theorem 1.3, Step (1) and Step (2)",
        "Explanation": "The argument uses Lemma 2.1—which concerns the *index‐difference* (counting metric) nearest neighbor hi(N)—to draw conclusions about the *geometric* nearest neighbor δ1(N,N). In particular, it is asserted that whenever δ1(N,N)<min AN−1 one must have nn1(Nα)=α. But δ1 refers to *Euclidean* (or Lq) distances, whereas hi refers to *index gaps*. There is no justification that the index‐gap minimiser coincides with the geometric‐distance minimiser in the new-small-distance case, so the claim nn1(Nα)=α (and the ensuing set-inclusion arguments) break down."
      },
      {
        "Problem": "Unjustified bound “at most two” in Lemma 2.2 for general Lq norms",
        "Location": "Lemma 2.2 and its proof",
        "Explanation": "The lemma asserts that for algebraically independent α in d≥2 and any Lq metric, no point can have more than two equidistant nearest neighbors. The proof relies on algebraic independence to force coordinate‐wise uniqueness of the maximal coordinate difference (for L∞) or equality of all qth‐power summands (for Lq). But neither argument rules out configurations where two different neighbors achieve the same max‐coordinate in different indices, or where sums of distinct qth powers coincide without term‐by‐term equality. There is no rigorous algebraic‐independence argument covering these possibilities, so the ‘≤2’ bound is unproven and likely false in general."
      },
      {
        "Problem": "Invalid inference that gN=1 in Theorems 3.2/1.4 from h1(N) being constant",
        "Location": "Section 3, Theorem 3.2 (and hence Theorem 1.4)",
        "Explanation": "To conclude gN(α)=1 one must show that *every* δ1(i,N) (geometric nearest‐neighbor distance of iα) is the same. The paper instead shows that h1(n) (the *index* gap of 1α) is constant over some n‐interval and then, by Lemma 2.1, infers hi(N)=h1(N) for all i. But Lemma 2.1 only gives hi(N)=h1(N−i+1), so unless h1(k) is constant for *all* k=1,…,N the hi(N) will vary. No argument ensures h1(k)=constant on the entire range k=1…N, so nearest‐neighbor index gaps (and hence geometric distances) do differ among points, contradicting the claim gN=1."
      }
    ],
    "think_summary": [
      "**Reviewing logical errors**\n\nThe user wants me to analyze a paper for critical errors and unsoundness. They’ve provided text up to page 11, and I need to examine logical correctness, focusing on issues like flawed proofs and unproven claims regarding nearest neighbor growth.\n\nThe paper addresses multi-dimensional Kronecker sequences and includes several theorems. My task is to find the top five critical errors or unsound issues that could invalidate the conclusions, documenting the problem, location, and explanation for each. I’ll start with Theorem 1.3 and assess the related proof and lemma implications.",
      "**Analyzing proof assumptions**\n\nThere’s an assertion that distances are equal on the torus, and that seems sound. The min distance condition works. However, in Theorem 2.1, they claim that for Lq metric, having the same max coordinate means no ties in coordinate differences, but actually, the pattern equality matters for distance equality.\n\nThen, in section 3, they propose constructing α2 using subsequences of convergents but don’t prove α2 exists. They need to ensure that their infinite sequence is correct. Additionally, they assume α3 aligns perfectly with denominators of α1, leading to complications. I’ll need to clarify these points further.",
      "**Identifying critical errors**\n\nI notice several key issues in the proof. First, they confuse counting metrics with geometric metrics when addressing nearest neighbors, using Lemma 2.1 incorrectly. Then, there's an unsupported claim in Lemma 2.2 regarding point distances. Additionally, Theorem 1.3 fails due to insufficient justification for set inclusions and a reliance on a one-dimensional case.\n\nIn exploring Theorem 1.4, they seek a property across all Lq metrics but only verify for Lp norms, raising questions about their conclusions. They don't confirm that nearest neighbors vary across all dimensions effectively. I should clarify these critical points for accuracy."
    ],
    "token_usage": {
      "input": 11516,
      "thinking": 8384,
      "output": 668
    }
  },
  {
    "entry_id": 77,
    "retraction_id": "1303.3756v2",
    "paper_id": "1303.3756v1",
    "retraction_comment": "This paper has been withdrawn by the author because Jarzynski's equality takes also processes into consideration which do not satisfy the 2nd law. Consequently, a modified non-equilibrium thermodynamics has to be developped taking into account the \"anti-irreversible admixtures\"",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Conflation of the canonical inverse‐temperature parameter β with a non‐equilibrium contact temperature Θ",
        "Location": "Section 2.3, Eq. (17)",
        "Explanation": "Jarzynski’s β enters only as the Lagrange multiplier fixing the initial canonical ensemble, not as a physical temperature of the system during the process. Replacing the non‐equilibrium contact temperature Θ by the ‘thermostatic’ T at all times (Eq. 17) has no basis in either the microscopic derivation or phenomenological thermodynamics, and thus undermines the entire reinterpretation of Jarzynski’s equality."
      },
      {
        "Problem": "Invalid application of the exponential map to an averaged inequality",
        "Location": "Section 3.2, Eqs. (29)–(30)",
        "Explanation": "Starting from ⟨W⟩ ≥ ΔF (Eq. 28) the author applies exp(–β·) to both sides to conclude exp(–βΔF) ≥ exp(–β⟨W⟩), then replaces exp(–β⟨W⟩) by ⟨exp(–βW)⟩ (Eq. 30). This step is mathematically invalid—Jensen’s inequality gives ⟨exp(–βW)⟩ ≥ exp(–β⟨W⟩), not the reverse—so the derived inequality contradicts basic convexity properties and cannot stand."
      },
      {
        "Problem": "Misinterpretation that Jarzynski’s equality holds only in the reversible limit",
        "Location": "Section 3.2, Eq. (31) and surrounding text",
        "Explanation": "The paper asserts that exp(–βΔF)=exp(–βW) can occur only if ΔF=W (i.e., reversibility), and so JE is confined to reversible processes. In reality, Jarzynski’s equality is derived from microscopic time‐reversibility and Liouville’s theorem without assuming any reversible (quasi‐static) evolution of the system; individual trajectories generally dissipate W>ΔF, yet the non‐equilibrium equality ⟨exp(–βW)⟩=exp(–βΔF) still holds."
      },
      {
        "Problem": "Erroneous claim that equal system and reservoir temperatures imply zero heat exchange",
        "Location": "Section 3, paragraph ii)",
        "Explanation": "The author states that if the system’s (thermostatic) temperature T equals the reservoir temperature T*, then ⋅Q=0. This neglects non‐equilibrium driving and fluctuations: even with equal instantaneous temperatures, a driven system can exchange heat with the bath. There is no general thermodynamic principle forbidding heat flow merely because an internal temperature parameter matches the reservoir’s temperature."
      },
      {
        "Problem": "Flawed mapping between phenomenological non‐equilibrium processes and microscopic phase‐space trajectories",
        "Location": "Section 3.1 and Eq. (20)",
        "Explanation": "The paper treats path‐integrals of macroscopic free‐energy rates along ‘non‐equilibrium process families’ as if they directly correspond to fluctuations of work along single microscopic trajectories. This identification overlooks that microscopic work distributions arise from specific Hamiltonian dynamics (with microreversibility), not from arbitrary phenomenological entropy‐production variables and contact temperatures. Consequently, the derived inequalities have no bearing on Jarzynski’s microscopic derivation."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 5349,
      "thinking": 1344,
      "output": 780
    }
  },
  {
    "entry_id": 78,
    "retraction_id": "2002.03104v2",
    "paper_id": "2002.03104v1",
    "retraction_comment": "As pointed out by [REDACTED-NAME] via MO: \"This only shows that there is no trivial, i.e. purely analytic, argument proving that this expression is unbounded. But there might be only finitely many odd perfect numbers, or there might be some relation between n and q, which implies that the ratio is bounded.\"",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Misuse of ‘unboundedness’ to derive contradictions for fixed values",
        "Location": "Section 3.(1) and repeated in Theorems 4.4, 4.5, 4.6",
        "Explanation": "The author treats the function f(z)=z+1/z being unbounded over all positive z as implying that for a specific odd perfect number (with fixed q and n), the quantity σ(q²)/n + σ(n)/q² cannot be bounded above by any finite constant. This confuses a universal property of a function with existence of a finite bound for a fixed argument. For any fixed q,n the ratio is a finite real number, so no contradiction arises from showing it is ≤ 81/25 or another constant."
      },
      {
        "Problem": "Reliance on unproven or unpublished results for critical inequalities",
        "Location": "Sections 3.(2), 4.1 (uses q< n and q^k< n²)",
        "Explanation": "Key steps invoke results such as n<q² ⇒ k=1 and q< n (citing Dris 2012/2008, Brown 2016, Starni 2017), but these are either conjectures, unpublished preprints, or the author’s own unrefereed work. No rigorous, peer-reviewed proof is provided, invalidating all arguments that depend on these claims."
      },
      {
        "Problem": "Incorrect numeric assertion n>10500 and consequent bounding error",
        "Location": "Section 4.1, Theorem 4.6 case 2",
        "Explanation": "The author claims that from q< n< q² it follows n>10500, without justification. In fact if q≥5 then 5< n<25, so n<25. The derived bound σ(q²)/n + n/q² < C/10500 + 1 is based on this false lower bound and thus the subsequent contradiction is invalid."
      },
      {
        "Problem": "Circular reasoning in eliminating inequality cases",
        "Location": "Sections 4 and 4.1",
        "Explanation": "The paper first classifies four possible orderings of q²,σ(q²),n,σ(n). Then it eliminates cases by appealing to the same boundedness contradiction derived from (1). Since that contradiction is itself unsound, the elimination of cases is circular and yields no valid restriction on the orderings."
      },
      {
        "Problem": "Invalid inference from boundedness to nonexistence of finite solutions",
        "Location": "Throughout proofs of Theorems 4.4, 4.5, 4.6",
        "Explanation": "The author repeatedly infers that if σ(q²)/n + σ(n)/q² is shown to be less than some explicit constant, then this contradicts the ‘unboundedness’ established in (1). However, proving an expression is less than a constant only shows it is bounded above, which is true for any fixed finite m, q, and n. Thus these inferences are fundamentally flawed."
      }
    ],
    "think_summary": [
      "**Analyzing logical flaws**\n\nI'm examining how they’re treating the interplay with odd perfect numbers. They focus on one candidate, claiming that a specific expression is bounded above. But they seem to mix different concepts. They assume for a given odd perfect number, a quantity should be unbounded, leading to contradictions. However, since this refers to a fixed number, they can't argue it's unbounded based on the general behavior of the function. Their logic seems flawed regarding boundedness versus unboundedness in this context.",
      "**Evaluating defined limits**\n\nThey assume some equality that claims σ(q^2)/n + σ(n)/q^2 is bounded by a constant, suggesting a contradiction exists. But there’s no actual contradiction; any finite limit works for a fixed number. It seems like they're mixing universal and existential quantifiers. Then they rely on an unproven conjecture from a previous author, which raises doubts. Moving on to theorems, their logic around bounds and the relationships between the quantities repeatedly leads to flawed conclusions. It's worth questioning the robustness of their statements."
    ],
    "token_usage": {
      "input": 3185,
      "thinking": 1920,
      "output": 694
    }
  },
  {
    "entry_id": 79,
    "retraction_id": "1401.5295v2",
    "paper_id": "1401.5295v1",
    "retraction_comment": "This paper has been withdrawn due to non-inclusion of some terms in equation 16",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Use of uncontrolled mean-field approximation for a 2D strongly correlated Hofstadter system",
        "Location": "Section IV (Mean Field Theory), pages 4–5",
        "Explanation": "The paper employs a simple mean-field decoupling of the nearest-neighbor repulsion in a two-dimensional, low-density quantum Hall setting where fluctuations are known to be strong and mean-field is uncontrolled. As a result, the predicted first-order Landau and topological transitions may be artifacts of the approximation rather than genuine many-body phenomena."
      },
      {
        "Problem": "Restricted decoupling channel ignores competing orders",
        "Location": "Section IV, Eqs. (15)–(18) (page 4)",
        "Explanation": "The interaction is decoupled only in the bond-hopping (current) channel, with no allowance for charge density wave, pairing, or other density instabilities. Without a systematic stability analysis against these competing orders, the identified current-ordered phases may not represent the true global minima."
      },
      {
        "Problem": "Incorrect computation of Chern numbers in symmetry-broken phases",
        "Location": "Section IV B (Translational symmetry breaking), Fig. 5 (page 6)",
        "Explanation": "In phases PH2 and PH3 the translational symmetry is broken, enlarging the real-space unit cell (and shrinking the Brillouin zone) and back-folding bands. The paper nevertheless computes Berry curvature and Chern numbers over the original Brillouin zone, invalidating the topological characterization of these symmetry-broken phases."
      },
      {
        "Problem": "Insufficient evidence that the single-particle gap remains open across the topological transition",
        "Location": "Section IV A (The phase diagram), page 5",
        "Explanation": "The central claim of a gap-preserving first-order topological transition rests on the assertion that the band gap never closes. Yet no quantitative data (e.g., minimum gap vs. interaction strength) or plots are provided at or near the critical V, leaving the key requirement for well-defined Chern numbers unverified."
      },
      {
        "Problem": "Lack of a rigorous many-body topological invariant",
        "Location": "Sections III–IV (throughout)",
        "Explanation": "Topology is diagnosed via single-particle Berry curvature of the mean-field bands. In a truly interacting system, the many-body Chern number need not coincide with mean-field band Chern numbers. Without a non-perturbative definition of the interacting topological invariant, conclusions about quantized Hall conductivity changes may not hold beyond the mean-field level."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 17646,
      "thinking": 3200,
      "output": 618
    }
  },
  {
    "entry_id": 80,
    "retraction_id": "1402.6435v2",
    "paper_id": "1402.6435v1",
    "retraction_comment": "This paper has been withdrawn by the author due to a crucial error in page 20",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Corollary 5.5 proves only non-negativity of heights, not strict positivity as required for nefness",
        "Location": "Corollary 5.5 (page 20) and its use in Theorem 5.6",
        "Explanation": "In the proof of Corollary 5.5 one shows that h_D(x)>−2ε·h_A(x) for every ε>0, hence h_D(x)≥0, but never excludes the possibility h_D(x)=0. Since the definition of nef adelic R-divisor requires h_D(x)>0 strictly, this gap means the implication “σ_bₓ(D)=0 ⇒ h_D(x)>0” is unestablished, undermining the (c)⇒(a) direction of Theorem 5.6."
      },
      {
        "Problem": "Inconsistent exponent in the definition of the filtered volume vol_t(D)",
        "Location": "Definition 2.11 (page 6)",
        "Explanation": "The paper defines vol_t(D)=lim sup_{n→∞} dim_Q F_{n t}(X,nD)/(n^{d+1}/(d+1)!), but F_{n t}(X,nD) has Q-dimension ∼n^d, so this ratio would vanish. Instead, following Boucksom–Chen, one should divide by n^d/d!. This exponent mismatch propagates to Proposition 3.4, casting doubt on all subsequent volume comparisons involving vol_t."
      },
      {
        "Problem": "Use of the integral formula (3.6) without verifying its ‘mild condition’ hypotheses",
        "Location": "Equation (3.6) and Lemma 3.5 (page 10–11)",
        "Explanation": "The standard formula vol_c (D)=(d+1)![K:Q]∫_∆max{G_D,0}du requires that the filtered linear series satisfy certain finiteness or convergence hypotheses (e.g. an ‘ample series’ condition in [2, §3]). The paper omits checking these hypotheses for a general big adelic divisor, so the reliance on (3.6) to relate positivity of G_D to volume equality may be invalid in edge cases."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 22765,
      "thinking": 9856,
      "output": 517
    }
  },
  {
    "entry_id": 81,
    "retraction_id": "2210.17003v2",
    "paper_id": "2210.17003v1",
    "retraction_comment": "The main theorems of the paper (Theorem 13 and 17) are wrong. The statements are only valid in R^2",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Misapplication of Theorem 13 to sets that are only locally conic, not true cones",
        "Location": "Section 4, proof of Theorem 17",
        "Explanation": "Theorem 13 guarantees finite convergence only for globally closed convex cones. In the proof of Theorem 17 the authors truncate and shift the polyhedral sets to K₁=(A∩B_r(x*))−x* and K₂=(B∩B_r(x*))−x*, which are merely locally conic near 0, not actual cones. Outside the ball they need not satisfy λC⊆C or norm-preservation under reflections, so one cannot invoke Theorem 13 on K₁ and K₂."
      },
      {
        "Problem": "Assuming CRM iterates on the truncated/shifted sets coincide with those on the original sets",
        "Location": "Section 4, proof of Theorem 17",
        "Explanation": "After N steps the proof switches to solving the feasibility problem on K₁ and K₂ as if future CRM iterates on A,B were governed by reflections in K₁,K₂. But the reflection operators (and hence CT) for K₁,K₂ differ from those for A,B once points lie near the boundary of the ball, so the original sequence need not stay inside the locally conic region or follow the same trajectory."
      },
      {
        "Problem": "Lack of rigorous proof that CT is well-defined for arbitrary closed convex cones",
        "Location": "Remark 8",
        "Explanation": "The authors assert, by analogy with subspaces, that for any closed convex cones A,B the three points {x, RA(x), RB(RA(x))} always admit a unique circumcenter in their affine hull. No argument is given that these points are never collinear in a way that would destroy existence/uniqueness of the circumcenter, nor that the required affine‐independence holds in infinite dimensions."
      },
      {
        "Problem": "Nonstandard and self-contradictory definition of a closed convex cone",
        "Location": "Definition 10",
        "Explanation": "They define C to be a cone iff C=⋃_{λ>0}λC. Since λ>0 excludes λ=0, this union need not contain 0 unless 0∈C is assumed beforehand. Yet Lemma 11 and the entire argument rely on 0∈C. The misuse of λ>0 instead of λ≥0 undermines basic properties of cones used throughout."
      },
      {
        "Problem": "Omission of the required product-space reformulation when extending CRM to general convex sets",
        "Location": "Sections 2.2 and 4",
        "Explanation": "For two arbitrary convex sets A,B one must reformulate the problem in R^{2n} via X={(x,x)} and Y=A×B to ensure CT is well-defined and convergent (as shown in [7]). In Theorem 17 the authors apply CRM directly to A,B in R^n without verifying that this direct method coincides with the product-space CRM, invalidating the appeal to earlier convergence results."
      }
    ],
    "think_summary": [
      "**Evaluating circumcenter in space**\n\nI’m questioning a claim about the circumcenter being at the origin when points x, y, and z lie on a sphere centered at zero. It seems that if these points are equidistant and not collinear, the circumcenter is indeed unique but not necessarily at the origin. In any Hilbert space with three affinely independent points, the circumcenter could be elsewhere. The fact that reflections preserve distance suggests that there are many equidistant points from x, y, and z. That’s intriguing!",
      "**Examining circumcenter conditions**\n\nI’m exploring the set of points that maintain equal distance from x, y, and z, focusing on their bisector hyperplanes. When the points are affinely independent, it seems the circumcenter is unique, potentially at 0. That’s interesting, as 0 equidistantly represents each point. However, I need to verify if these points are indeed affinely independent, especially under reflections that preserve norms. If x, y, and z are collinear, the circumcenter seems less defined, only existing if they are symmetrically located. This is quite a thought-provoking situation!"
    ],
    "token_usage": {
      "input": 6019,
      "thinking": 7232,
      "output": 703
    }
  },
  {
    "entry_id": 82,
    "retraction_id": "1503.00700v2",
    "paper_id": "1503.00700v1",
    "retraction_comment": "This preprint was withdrawn because the theoretical discussion of the inferred temperature of qubit couplers assumes an equilibrium distribution, which is not the case during the annealing cycles",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Assumption of Boltzmann equilibrium to fit qubit and coupler outputs",
        "Location": "Section III (Eq. 5) and Section IV (Eq. 9)",
        "Explanation": "The method fits the observed spin and spin–pair distributions to classical Boltzmann expectation values at an effective temperature, but quantum annealing outputs need not follow a thermal (Gibbs) distribution of the final Ising Hamiltonian. Non‐adiabatic transitions, freeze‐out phenomena, and quantum coherence effects mean the final state statistics generally deviate from a simple Boltzmann law, invalidating the offset and temperature parameters derived under this assumption."
      },
      {
        "Problem": "Calibration on the null problem may not generalize to arbitrary problem instances",
        "Location": "Section V (Nulling out the composite hi and Jij residual bias) and Conclusions",
        "Explanation": "Offsets h0i and J0l are determined solely from the all-zero Hamiltonian; there is no evidence that these corrections apply when nonzero fields and couplings interact nonlinearly. Without cross-validation on real optimization problems, the claim that null calibration improves solver performance is unsupported."
      },
      {
        "Problem": "Independent treatment of hi and Jij ignores crosstalk and higher-order coupling effects",
        "Location": "Section III–IV methodology and Section V",
        "Explanation": "The paper assumes that residual biases in individual fields and couplers can be measured and nulled separately and then linearly superposed. In hardware, control errors, crosstalk between qubits, and embedding chains produce non-local, nonlinear interactions, so offsets are not strictly additive or independent, undermining the effectiveness of sequential hi and Jij corrections."
      },
      {
        "Problem": "Insufficient statistical sampling for robust offset estimation",
        "Location": "Section II (10 iterations of 10,000 anneals each per sweep)",
        "Explanation": "Each h or J sweep uses only ten programming cycles and 10,000 anneals per submission to estimate mean values and fit offsets. Given device noise, day-to-day drift, and gauge variability, this low sample count yields high uncertainty in h0i and J0l estimates, risking overfitting to transient noise rather than stable hardware biases."
      },
      {
        "Problem": "Lack of demonstration on non-null optimization problems",
        "Location": "Conclusions",
        "Explanation": "All reported results concern the null Hamiltonian (all hi, Jij zero). No experiments validate that the calibration method yields improved ground‐state success rates or solution quality on actual problem instances, so the practical benefit of the technique remains unproven and may not extend beyond trivial null tests."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 19110,
      "thinking": 768,
      "output": 609
    }
  },
  {
    "entry_id": 83,
    "retraction_id": "1202.1896v2",
    "paper_id": "1202.1896v1",
    "retraction_comment": "This paper has been withdrawn by the author due to a critical mistake in the circle graph algorithm",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Lemma 1 only applies to ‘optimal’ colorings and the DP ignores all non-optimal solutions",
        "Location": "Page 2, Lemma 1",
        "Explanation": "Lemma 1 asserts that any black-and-white coloring can be assumed optimal (every uncolored vertex has both a black and a white neighbor), and the DP only searches among such optimal colorings.  However, a valid coloring with the prescribed sizes (b,w) may leave some vertices without neighbors of one color—so not ‘optimal’ in their sense—yet still be a correct solution.  The paper does not show that every solution can be transformed into an optimal one with the same (b,w), so the DP may miss valid colorings."
      },
      {
        "Problem": "No polynomial bound is given on the size of the DP tables in Theorem 2",
        "Location": "Page 5, Theorem 2 proof (last paragraph)",
        "Explanation": "The authors admit they “restrain from analyzing the size of the table” used in the circle-graph DP, yet they claim a polynomial-time algorithm.  Without explicit bounds on the number of scanlines, successors, and chains ∆, nor on the number of state entries (b, w, r, per-successor counts), one cannot conclude the procedure runs in polynomial time."
      },
      {
        "Problem": "State definition in the circle-graph DP is underspecified and insufficient for correctness",
        "Location": "Page 5, Definition of DP table for scanline s′",
        "Explanation": "The DP state records (b,w), a maximal scanline s′, a count r, and for each successor scanline the number of chords crossing it.  It does not capture which specific chords belong to which subregions or their adjacency relations.  This coarse state cannot ensure that when combining subproblems, no forbidden adjacencies between B and W sets are introduced, so the correctness of the DP is unjustified."
      },
      {
        "Problem": "Misleading complexity analysis in Theorem 1",
        "Location": "Page 3, Theorem 1 proof",
        "Explanation": "The paper claims an O(n^8) time bound per piece and O(n^4) pieces, but a closer count gives O(n^10) or even O(n^12) overall.  While still polynomial, the miscalculation and lack of tight analysis casts doubt on both efficiency claims and the authors’ command of the DP’s combinatorial structure."
      },
      {
        "Problem": "No proof that the circle-graph DP correctly enforces independence between B and W sets across scanline separators",
        "Location": "Page 4–5, Section 3, combining pieces around a scanline",
        "Explanation": "The algorithm concludes by “checking all scanlines s, and combining colorings of the two pieces incident with s,” but fails to show how to guarantee that no black vertex in one piece is adjacent to any white vertex in the other.  Without a rigorous separator argument or state that records inter-piece adjacency constraints, the algorithm may accept invalid colorings."
      }
    ],
    "think_summary": [
      "**Exploring vertex coloring issues**\n\nI'm pondering whether adding a black vertex without a black neighbor can increase b, although it seems b is fixed. Maybe it's about moving colors between the black and white sets? I wonder if optimality means if an uncolored vertex lacks any black neighbors, could we color it black and potentially grow our sets? But if uncolored has no black or white neighbors, we can't color more due to b and w being fixed. It seems that I need to think more about how solutions fit into the lemma's assumptions and restrictions.",
      "**Analyzing vertex coloring constraints**\n\nI'm thinking about these exact cardinalities and the role of uncolored vertices. If an uncolored vertex has no black neighbor, could I add it to the black set without violating maximality? It seems contradictory, as optimality hinges on the number of neighbors. If I have a valid coloring with exact b black vertices and an uncolored vertex has no white neighbor, could I add it to the white set? However, both b and w are fixed, complicating matters. It looks like optimality focuses more on neighborhood coverage than strict cardinality.",
      "**Identifying issues in coloring algorithms**\n\nI'm figuring out that when coloring, I can't add to sets if their sizes are already set. However, if b or w hasn't reached the target, there's still room to explore. The dynamic programming approach focuses on optimal colorings, which means it might overlook valid non-optimal solutions. I see several critical problems to address, like missing polynomial bounds and incomplete DP states for circle graphs. I think I’ll create a JSON list to organize these issues and their explanations for clarity."
    ],
    "token_usage": {
      "input": 4167,
      "thinking": 1792,
      "output": 676
    }
  },
  {
    "entry_id": 84,
    "retraction_id": "1304.5962v2",
    "paper_id": "1304.5962v1",
    "retraction_comment": "This paper has been withdrawn by the author due to a crucial error in equation 13",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Flawed justification of constant cancellation boundary",
        "Location": "Lemma 2.1 (page 3)",
        "Explanation": "The proof that both free boundaries c1(τ) and c2(τ) collapse to the constant K relies on a comparison argument using the parabolic maximum principle on unbounded domains without verifying decay or boundary conditions at S→∞ or regularity at the kink S=K. As stated, this argument is insufficient to guarantee that no nontrivial time‐dependent boundary can lower the value, so the claim c1(τ)=c2(τ)=K is unsubstantiated."
      },
      {
        "Problem": "Underdetermined PDE: missing boundary conditions at S=0 and S→∞",
        "Location": "Section 2, Eqs. (7)–(10) (page 5)",
        "Explanation": "The pricing PDE is posed separately on (0,K) and (K,∞) with Dirichlet data at τ=0 and S=K, but no conditions are imposed as S→0 or S→∞. Without specifying behavior at the ends of the spatial domain, the solution on each half–line is not uniquely determined, invalidating the subsequent integral representation."
      },
      {
        "Problem": "Inconsistent boundary condition in lower cancellation region",
        "Location": "Equation (6) (page 3)",
        "Explanation": "The third subproblem for 0<S<c2(τ) incorrectly reads V(c1(τ),τ)=c2(τ)−K+δ instead of V(c2(τ),τ)=c2(τ)−K+δ. This typo misassigns the boundary value to the wrong spatial point, breaking the definition of the cancellation region and the well–posedness of the PDE there."
      },
      {
        "Problem": "Incorrect parameters in exponential change of variables",
        "Location": "Theorem 3.2, definitions of a and b (page 6)",
        "Explanation": "The values of a and b claimed to eliminate the drift and discount terms in the transform ε=e^{aτ−by}w do not satisfy the algebraic relations obtained by substituting into (∂τ−L0)ε=0. As a result, residual first–derivative or zero–order terms remain, so w does not solve the pure heat equation used in the Lemma."
      },
      {
        "Problem": "Mis–scaled heat kernel in integral formula",
        "Location": "Equation (13) (page 5)",
        "Explanation": "The kernel factor e^{−(log(K/S))^2/(2σ^2s)}/s^{3/2} and the prefactor involving ‘Kb’ and logarithms do not match the classical half–line heat kernel with diffusion constant σ^2/2 (which requires exponent denominator 4c^2s=2σ^2s and prefactor 1/(2c√πs)). This scaling error renders the pricing formula quantitatively incorrect."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 7045,
      "thinking": 6336,
      "output": 682
    }
  },
  {
    "entry_id": 85,
    "retraction_id": "1908.07349v3",
    "paper_id": "1908.07349v2",
    "retraction_comment": "the results of theorem 2 is not correct",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Undefined symbol 'r' used in coefficient bounds",
        "Location": "Theorem 3.6 (equations (3.15), (3.23)) and Corollaries 3.7–3.12",
        "Explanation": "The bounds for |a₂| and |a₃| involve a constant 'r' (e.g. in (3.23)) that is never defined in the paper. Without a definition of 'r', all estimates containing it are ambiguous and unusable."
      },
      {
        "Problem": "Incorrect sign in the expansion of ϕ(u(z)) and ϕ(v(w))",
        "Location": "Equations (3.6) and (3.7)",
        "Explanation": "The paper writes ϕ(u(z)) = 1 − Σ B₁ K¹ₙ(c₁,…,cₙ;B₁,…,Bₙ) zⁿ, but since ϕ(z)=1+B₁z+…, the composition ϕ(u(z)) should begin with a plus sign. This reversal of sign invalidates the subsequent coefficient comparisons."
      },
      {
        "Problem": "Missing modulus in the bounded‐class definition",
        "Location": "Definition 2.2, inequalities (2.8) and (2.9)",
        "Explanation": "The intended condition |F(z)−M|<M is written simply as 'F(z)−M < M' without absolute‐value bars. This omission changes the nature of the constraint and makes the definition ambiguous."
      },
      {
        "Problem": "Inconsistent definitions of the Faber‐polynomial coefficients Kⁿᵖ",
        "Location": "Equations (2.4) vs. (3.8)",
        "Explanation": "In (2.4) Kⁿᵖ is defined via the Dᵐₙ sums, while in (3.8) a different formula involving ρᵢ and Bᵢ appears. The paper does not reconcile these two expressions, so it is unclear which is used in proofs."
      },
      {
        "Problem": "Trivializing assumption aₖ=0 for k=2,…,n−1",
        "Location": "Theorem 3.1 (premise of equation (3.1))",
        "Explanation": "Requiring all intermediate Maclaurin coefficients to vanish reduces f(z) to z+aₙzⁿ+… , which for generic aₙ is not univalent (or bi‐univalent). This overrestrictive hypothesis makes the estimate of |aₙ| apply only to degenerate cases, undermining the theorem's relevance."
      }
    ],
    "think_summary": [
      "**Critiquing mathematical validity**\n\nThe user wants me to identify critical errors in a paper's conclusions, focusing on significant issues rather than minor ones. I should capture up to five major problems with details like \"Problem,\" \"Location,\" and \"Explanation.\" I'll analyze the mathematical content, definitions, proofs, and results. For instance, the use of negative factorials in Faber polynomial expansions may lead to convergence issues, while assumptions in Theorem 3.1 limit the generality of results. This critique could reveal flaws that affect the validity of the conclusions.",
      "**Identifying critical errors**\n\nThere are significant oversights in the paper, especially regarding the variable \"r,\" which is likely intended to be sqrt(2) but remains undefined, leading to invalid coefficient estimates in Theorem 3.6. The signs of coefficients in equations (3.9) and (3.10) raise concerns about consistency, although they might not impact the final bounds. The assumptions in Theorem 3.1 restrict functions too much, yielding trivial cases and undermining significance without rendering the conclusions mathematically unsound. This needs careful reconsideration."
    ],
    "token_usage": {
      "input": 9736,
      "thinking": 2048,
      "output": 622
    }
  },
  {
    "entry_id": 86,
    "retraction_id": "1310.0331v4",
    "paper_id": "1310.0331v3",
    "retraction_comment": "this paper has been withdrawn by the author due to a crucial error in equation 5",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Unsupported inference that pairwise entanglement implies fractional statistics",
        "Location": "Page 1–2, Section II, after Eq. (1)",
        "Explanation": "The paper asserts that because two fermions occupy M single-particle states in a pairwise entangled form, they obey fractional statistics (filling factor ν=2/M). This conflates entanglement structure with exchange statistics. Fractional (anyonic) statistics arise from nontrivial exchange phases in two dimensions, not from writing a two-fermion state as an entangled superposition of Slater determinants. No braiding argument or exchange phase calculation is provided, so the conclusion is baseless."
      },
      {
        "Problem": "Misinterpretation of Slater-determinant expansions as evidence of purely two-particle entanglement in Laughlin states",
        "Location": "Page 2–3, Section II, around Eq. (5)–(6)",
        "Explanation": "The authors expand the N=3 Laughlin polynomial in monomials and then group terms to exhibit pairwise antisymmetrized factors, claiming this shows the Laughlin state is a sum of states each with only two entangled particles. In reality, the Laughlin wavefunction exhibits genuine multipartite entanglement, and any antisymmetric polynomial can be expanded over Slater determinants. The procedure neither demonstrates a physical two-particle entanglement pattern nor implies the absence of higher-order correlations."
      },
      {
        "Problem": "Flawed derivation of a modified Bose-Einstein distribution by enforcing occupation equality",
        "Location": "Page 3, Section III, Eqs. (10)–(12)",
        "Explanation": "The authors impose an ad hoc constraint ni=ni+1 (mean occupations in paired levels) to model entanglement and then recompute the partition function as ∏i∑ni e^{-β[ni(εi−μ)+ni(εi+1−μ)]}. This incorrectly mixes pure-state entanglement constraints with grand-canonical statistical sums. There is no justification that entanglement enforces equal mean occupations or that one should restrict the ensemble in this way. The resulting modified distribution lacks a valid foundation in statistical mechanics."
      },
      {
        "Problem": "Incorrect construction of the fermionic partition function under entanglement constraints",
        "Location": "Page 3, Section III, Eqs. (13)–(15)",
        "Explanation": "In the fermion case, the paper similarly constrains ni=ni+1 and writes Z=∏i(1+e^{-β(εi+εi+1−2μ)}), leading to a modified Fermi-Dirac law. This ignores indistinguishability and the fact that thermal ensembles are mixed states not pure entangled states. The method treats entanglement as a simple combinatorial restriction on occupations, which is unjustified, and fails to account for the proper counting of many-body states."
      },
      {
        "Problem": "Unphysical assumption of preserved pure-state entanglement at finite temperature without interactions",
        "Location": "Page 3, Section III, opening paragraph",
        "Explanation": "The model assumes an ideal, non‐interacting many-body system in thermal equilibrium that nonetheless maintains a fixed pattern of pairwise pure-state entanglement across energy levels. In realistic statistical mechanics, thermalization causes dephasing and destroys coherent entanglement unless protected by interactions or symmetries. The assumption that entanglement constraints survive at finite temperature and modify macroscopic statistics is unsubstantiated and likely contradicts standard thermodynamic behavior."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 5262,
      "thinking": 1664,
      "output": 806
    }
  },
  {
    "entry_id": 87,
    "retraction_id": "2308.02854v2",
    "paper_id": "2308.02854v1",
    "retraction_comment": "The assumption that the convex hull of d+2 points in R^d is either a d-simplex or a bi d-simplex is true only in d<4. In higher dimensions, there are more simplical polytopes, among which the cyclic polytope maximalizes the number of facets. As a consequence, there is no simple linear relation between the number of vertices and facets in d>3, from which one could connect the expected values",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Invalid enumeration of combinatorial types for Hd+2 when d ≥ 4",
        "Location": "Section 3, around Equation (3)",
        "Explanation": "The paper asserts that for d+2 random points in R^d the convex hull is almost surely either a d-simplex or two d-simplices sharing a hyperface. In fact, for d ≥ 4 there are multiple combinatorial types of simplicial d-polytopes with d+2 vertices (e.g. various cyclic and stacked polytopes), each having different numbers of facets. Hence the dichotomy in Equation (3) is false for higher dimensions."
      },
      {
        "Problem": "Faulty derivation of the linear relation between f0 and f_{d−1}",
        "Location": "Just before Equation (4)",
        "Explanation": "The claimed identity (d−1)·f0(H_{d+2}) − f_{d−1}(H_{d+2}) = (d−2)(d+1) is based on the two-case enumeration in Equation (3). Since that enumeration fails for d ≥ 4, the linear relation does not hold almost surely in higher dimensions."
      },
      {
        "Problem": "Breakdown of the proof of Theorem 1 for d ≥ 4",
        "Location": "Proof of Theorem 1 (page 4)",
        "Explanation": "The derivation of the integral formula for E[vold(H_{d+1})] relies critically on taking expectations in the invalid linear relation between f0 and f_{d−1}. With that relation unsound for d ≥ 4, the resulting formula in Theorem 1 is not justified beyond the classical d ≤ 3 cases."
      },
      {
        "Problem": "Incorrect check against known low-dimensional cases",
        "Location": "Theorem 1 (page 2)",
        "Explanation": "Specializing the stated formula to d=2 yields E[area(H3)] = 1 − (3/2)∫(Γ^2+(1−Γ)^2), which does not match the classical mean triangle area in two dimensions (e.g. the known 1/12 factor for uniform convex bodies). This discrepancy indicates a miscalculation or misnormalization in the constant terms."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 3754,
      "thinking": 3264,
      "output": 529
    }
  },
  {
    "entry_id": 88,
    "retraction_id": "1501.01153v3",
    "paper_id": "1501.01153v2",
    "retraction_comment": "Submission withdrawn due to the error in equation 35 on dimensional grounds",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Massless hadron approximation near transition temperature",
        "Location": "Section 5, Eqs. (56), (57) for pions; Section 5, Eq. (65) for nucleons",
        "Explanation": "The authors treat pions and nucleons as massless species when computing their pressure and energy density near Tc≈150–200 MeV. In reality mπ≈140 MeV and mN≈940 MeV are comparable to or exceed the temperature, so the ideal massless gas formulas drastically misestimate the hadronic thermodynamics and hence the critical temperature."
      },
      {
        "Problem": "Ideal gas QGP equation of state with only a constant bag term",
        "Location": "Section 4, Eqs. (47)–(50)",
        "Explanation": "The quark–gluon plasma is modelled as a noninteracting gas plus a fixed bag constant B, neglecting both perturbative interactions (e.g. O(αs) corrections) and nonperturbative effects known to be significant near Tc. This crude EOS does not reproduce lattice QCD thermodynamics and leads to unreliable estimates of pressure, energy density, and the transition point."
      },
      {
        "Problem": "Unjustified determination of bag constant from SU(2) dual QCD parameters",
        "Location": "Section 2 Eq. (11) & Section 4 Eq. (43)",
        "Explanation": "The bag constant B is derived via Eq. (43) from vector glueball masses obtained in a dual SU(2) QCD model with an ad hoc quartic potential (Eq. 11). No calibration or consistency check with SU(3) lattice data or phenomenological B values is provided, rendering B (and all subsequent numerical Tc) arbitrary."
      },
      {
        "Problem": "Mismatch of gauge groups: SU(2) model used for SU(3) thermodynamics",
        "Location": "Section 2 (dual QCD construction) and throughout thermodynamics sections",
        "Explanation": "The dual QCD framework is formulated for G≡SU(2), yet the paper applies its parameters directly to real-world QCD thermodynamics (which is SU(3)). Differences in color degrees of freedom and group dynamics between SU(2) and SU(3) invalidate quantitative conclusions about the QGP phase transition in QCD."
      },
      {
        "Problem": "Invalid comparison with SU(2) lattice data",
        "Location": "Section 6, Fig. 6 (interaction measure plots)",
        "Explanation": "Results for the trace anomaly and conformal measure are compared against SU(2) lattice simulations, but real QCD is an SU(3) theory. This mismatch in the number of colors alters the degrees of freedom and thermodynamic behavior, so the agreement claimed is not meaningful for SU(3) QCD."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 21032,
      "thinking": 1280,
      "output": 652
    }
  },
  {
    "entry_id": 89,
    "retraction_id": "1910.10153v2",
    "paper_id": "1910.10153v1",
    "retraction_comment": "The function \\phi(x) which we define above Eq.8 is infinitely peaked in the thermodynamic limit; the long-time-tail behavior enters through higher derivatives of the entropy",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Unjustified factorization of many-body eigenstates into decoupled thermal blocks",
        "Location": "Main text, Derivation section (just above Eq. (6)); Supplementary Information Sec. 2, leading assumptions (i)",
        "Explanation": "The authors assume that any eigenstate of the full Hamiltonian can be written as a product of eigenstates of spatially separated 'thermalized blocks' of size ξ(ω). However, ETH guarantees only that local observables in energy eigenstates resemble thermal averages—not that eigenstates factorize into tensor products over mesoscopic regions. This uncontrolled assumption undermines the validity of the entire block-tunneling perturbation theory."
      },
      {
        "Problem": "Overextension of random matrix theory beyond its regime of validity",
        "Location": "Main text, paragraph below Eq. (6) and Eq. (7)",
        "Explanation": "The derivation treats the internal dynamics of each block as a Gaussian random matrix with energy-independent off-diagonal matrix elements up to the single-particle bandwidth U. In reality RMT applies only for level splittings below the Thouless energy ∼D/ξ² of the block. Extending RMT to ω∼U neglects crucial energy-dependent structure of matrix elements and invalidates the quantitative formulas (Eqs. (8,9))."
      },
      {
        "Problem": "Neglect of intra-block diffusion and dynamic correlations",
        "Location": "Main text, same paragraph as above (discussion around Eq. (7))",
        "Explanation": "By assuming that charge and energy diffusion within a block are instantaneous on timescales ∼1/ω, the authors omit any internal hydrodynamic processes. Yet long-time tails and subleading transport corrections stem precisely from slow diffusive dynamics inside regions of size ξ. Ignoring these processes is inconsistent with the goal of capturing hydrodynamic corrections and undermines the non-analytic ω-dependence claims."
      },
      {
        "Problem": "Heuristic and potentially incomplete construction of the thermal current operator",
        "Location": "Supplementary Information Sec. 2, Eq. (21)–(26)",
        "Explanation": "The replacement of the microscopic energy current density by the commutator [J₁, H_R–H_L]/2 is argued through neglecting boundary terms, but the derivation omits all higher-order interaction contributions and relies on assuming those boundary effects are negligible. This ad hoc operator substitution lacks a systematic justification and may miss important thermal transport channels, invalidating subsequent low-temperature formulas (Eq. (9))."
      },
      {
        "Problem": "Overly universal Wiedemann–Franz relation without necessary hydrodynamic input",
        "Location": "Main text, Eq. (11)–(12) and accompanying discussion",
        "Explanation": "The paper claims the ratio (σ^LT T)/(T α^LT T)·(κ^DC)/(α^DC) is a temperature-independent constant. Yet a general hydrodynamic derivation (Eq. (12)) shows this ratio depends on thermodynamic susceptibilities ζ=∂²S/∂µ∂T and Υ=∂²S/∂T². The extra assumptions needed to eliminate ζ and Υ are neither physically justified nor generally satisfied, so the proposed 'generalized Wiedemann–Franz law' lacks true universality."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 20104,
      "thinking": 1408,
      "output": 768
    }
  },
  {
    "entry_id": 90,
    "retraction_id": "1306.5006v3",
    "paper_id": "1306.5006v2",
    "retraction_comment": "We have decided to withdraw the paper due to a crucial error in equation (9), that is in the definition of the p-value. This invalidates the results reported into the manuscript",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Invalid permutation test under lag-specific independence",
        "Location": "Section 3.3, Eq (9)",
        "Explanation": "The permutation p-value method assumes that the pairs (X_i, X_{i+r}) are exchangeable under the null. However, under H_0: f_r(x,y)=g(x)g(y) at lag r, the series may still exhibit dependence at other lags. Permuting the entire series breaks all serial structure, not just at lag r, so the null distribution generated by permutations is not valid for testing independence at a single lag when other dependencies exist. This can lead to incorrect type I error control and undermines the validity of the KL-autodependogram conclusions."
      },
      {
        "Problem": "No correction for multiple testing across lags",
        "Location": "Sections 2 and 4 (critical line construction)",
        "Explanation": "The autodependogram and KL-autodependogram plot l tests at l lags using a single level-α critical line, but do not adjust for multiple comparisons. With l separate hypothesis tests, the family-wise error rate is substantially inflated. Without a procedure (e.g., Bonferroni or false discovery rate control), the reported size and power at individual lags do not translate into reliable overall inference, invalidating claims about the methods’ performance."
      },
      {
        "Problem": "Incompatible calibration of significance thresholds across methods",
        "Location": "Section 6 (simulation comparisons)",
        "Explanation": "In the simulation study, the ADF uses asymptotic χ² critical values, the ACF uses normal-theory limits, and the KL-ADF uses permutation p-values with only B=99 replicates. These different approaches yield tests with non-comparable finite-sample size properties—especially at small n—so the direct comparison of their rejection rates may reflect differences in calibration rather than true relative power, compromising the conclusion that KL-ADF uniformly outperforms ADF."
      },
      {
        "Problem": "Reliance on a single, density-optimal bandwidth for divergence estimation",
        "Location": "Section 3.1, discussion of h_{LCV}",
        "Explanation": "The paper adopts h_{LCV}, the likelihood cross-validation bandwidth optimized for marginal density estimation, to estimate both joint and marginal densities in the KL divergence. Yet bandwidths optimal for density estimation need not be optimal for hypothesis testing or divergence estimation. No sensitivity analysis or data-driven bandwidth selection for ∆_r is provided, potentially biasing the KL estimates and affecting both size and power of the test in ways that could invalidate the claimed improvements."
      },
      {
        "Problem": "Potential bias from coarse and fixed-grid numerical integration",
        "Location": "Section 3.2, Eq (8)",
        "Explanation": "The integral in the KL divergence is approximated by summation over a fixed 100×100 grid defined by the sample range extended by one quarter. For heavy-tailed or multimodal distributions (e.g., GARCH models), this grid may inadequately cover the support or be too coarse, leading to significant numerical bias in ∆_r. Such bias can artificially inflate apparent power, undermining the paper’s simulation and empirical conclusions."
      }
    ],
    "think_summary": [
      "**Identifying statistical issues**\n\nI'm noticing some challenges when comparing per-lag rejection rates, especially since their levels might not be directly comparable. I want to raise a specific problem: incommensurate significance thresholds across methods. In Section 6, the ADF uses asymptotic χ^2 critical values, while KL-ADF relies on permutation p-values, and ACF follows approximate normal limits. These differing methods affect type I error distributions, particularly for smaller sample sizes. This could skew conclusions about KL-ADF's superiority based on threshold selection rather than genuine test quality. Let's keep these concerns organized in JSON format."
    ],
    "token_usage": {
      "input": 20514,
      "thinking": 3136,
      "output": 725
    }
  },
  {
    "entry_id": 91,
    "retraction_id": "1704.08680v6",
    "paper_id": "1704.08680v5",
    "retraction_comment": "Algorithm does not terminate. Even if fixed, Claim 9 is wrong",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "No proof that the multi-phase “dual growth with variable rates” procedure terminates or makes progress",
        "Location": "Section 4 (pp. 10–12), in the description of Primal-Dual-2 and the “r ≥ 2 phases” scheme",
        "Explanation": "The algorithm re-runs primal-dual growth in successive ‘phases’ as long as N_ℓ \\ N_{ℓ−1} ≠ ∅, where N_ℓ is the set of Steiner nodes of degree 0 at the end of phase ℓ. However, the paper never shows that δ_ℓ(v) (the degree of v at hitting time in phase ℓ) can only increase from phase to phase, nor that eventually all nodes leave degree 0. Without a monotonicity or progress invariant one cannot even guarantee termination (let alone polynomial time) or that the algorithm will reach the claimed “last phase.”"
      },
      {
        "Problem": "Induction in Theorem 6 ignores the altered (slower) growth rates for partial Steiner nodes",
        "Location": "Section 5, “Proof of the approximation ratio 7/6”, Theorem 6",
        "Explanation": "The proof asserts that for any new edge added in the last phase, the increase in the left‐hand side (actual cost) equals the increase in Σ z_i(i) − t on the right‐hand side, by invoking Proposition 7.  But in phase r the partial Steiner nodes grow at a rate |Δ^{r−1}(v)∩L^t_v|/|L^t_v|·(1/|C∩R|) rather than 1/|C∩R|.  The proof never accounts for this slowdown in the invariant, so the central inductive inequality c(C)≤Σ_{i∈C∩R}z_i(i)−t need not hold."
      },
      {
        "Problem": "Unanalyzed post-processing (MST on L) can break the primal-dual cost invariant",
        "Location": "Section 4, Algorithm 2 and subsequent paragraph (“post-processing step”)",
        "Explanation": "After running the Primal-Dual baseline, the algorithm replaces the returned tree by an MST over the union of all Steiner‐component links L.  No argument is given that this replacement preserves the key per-component cost bound c(C)≤2·Σ z_i(i)−t (or its 7/6-scaled analogue).  MST edges may not coincide with tight dual edges, so the primal-dual coupling used in the analysis is invalidated as soon as this step occurs."
      },
      {
        "Problem": "Claim 9’s exchange argument for aligning an optimal solution T with the link set L is flawed",
        "Location": "Section 5, Claim 9",
        "Explanation": "The paper claims that any optimal T can be transformed by discarding certain edges and adding cheapest links from L without increasing cost, ultimately making E(T)⊆L.  But the local swaps (especially when reconnecting u by links of cost ≤t_max in place of edges of cost ≥t_max) are not shown to preserve global connectivity, nor is it ruled out that the ‘cheapest links’ inserted might actually be more expensive than the edges removed in a sparse or asymmetric component.  Thus one cannot guarantee OPT_I1≥OPT_I2 as needed."
      },
      {
        "Problem": "Claim 10’s bound on the total distortion ∑D(e_i) is algebraically incorrect or at least unsubstantiated",
        "Location": "Section 5, Claim 10",
        "Explanation": "The derivation of ∑_{i=1}^s D(e_i) ≤ ((s−3)t + (4−s)t_s)/2 relies on a telescoping sum and worst-case assumptions about which links are selected.  The paper omits many intermediate steps, and for small s the resulting bound can even become positive or exceed the claimed amount.  Without a rigorous and correct distortion lemma, the distortion-compensation argument in Theorem 8 collapses."
      }
    ],
    "think_summary": [
      "**Identifying critical problems**\n\nI need to pinpoint up to five significant issues in the paper that could undermine its conclusions. For example, I see a complexity concern regarding multiple phases and how they claim to have O(|V|) phases. They discuss certain nodes, but I question their assumptions about the changes in degrees over phases. There's also a need for clarity on the proof of Theorem 6 and its reliance on induction, which appears to overlook critical details about node behavior.",
      "**Analyzing paper vulnerabilities**\n\nThe authors assume the growth matches costs without considering that growth rates differ in the second phase. They equate increments on both sides of an equation but overlook adjustments for varying growth rates. The dual is infeasible without a scaling factor, which they attempt to address later. Moreover, there are issues with Theorem 8, including changes to costs without demonstrating critical bounds. Lastly, the distortion bound computations seem flawed, and the assumptions made about edge-swapping might jeopardize feasibility. The main concerns lie in monotonicity and induction correctness."
    ],
    "token_usage": {
      "input": 26456,
      "thinking": 1280,
      "output": 928
    }
  },
  {
    "entry_id": 92,
    "retraction_id": "0710.2117v5",
    "paper_id": "0710.2117v4",
    "retraction_comment": "This paper has been withdrawn by the author due to a critical error in the geometric formulation of the principle of inertial motion",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Misinterpretation of Lorentz transformations as preserving Euclidean 4-volume and ratios",
        "Location": "Section 2.2, Eq. (5), pages 2–3",
        "Explanation": "The paper claims that Lorentz transformations conserve ratios of sections of lines, areas, 3D volumes, and 4D Euclidean volumes. In reality Lorentz transformations preserve the Minkowski metric and its associated hyperbolic angle structure, not Euclidean distances or volumes. Euclidean 4-volumes and arbitrary Euclidean ratios are not invariant under genuine Lorentz boosts, so the geometric foundation of the entire Euclidean approach is unsound."
      },
      {
        "Problem": "Unjustified assumption of invariance of Euclidean space-time cell volumes under motion",
        "Location": "Section 2.1 (“Normalized light clocks”), page 2",
        "Explanation": "The definition of “normalized light clocks” assumes that the Euclidean 4-volume of a clock’s elementary cell remains unchanged under arbitrary constant-velocity motion. However, Euclidean volumes are frame-dependent and no symmetry or physical law ensures their invariance under Lorentz transformations. This ad hoc postulate lacks both empirical support and mathematical justification and critically underpins subsequent derivations."
      },
      {
        "Problem": "Ill-defined Euclidean general covariance principle ignoring Lorentzian signature and causality",
        "Location": "Section 3.1 (“Euclidean principle of general covariance”), page 6",
        "Explanation": "The author formulates a ‘general covariance’ under arbitrary coordinate changes in a 4D Euclidean space (++++) rather than in a Lorentzian (−+++) manifold. This construction fails to incorporate the causal (light-cone) structure essential to relativity and contradicts the tensorial formulation of physical laws on pseudo-Riemannian manifolds, invalidating any link to general relativity."
      },
      {
        "Problem": "Ad hoc free-fall motion law based on one-way light-speed variations",
        "Location": "Section 3.2, Eq. (7) and surrounding discussion, pages 6–7",
        "Explanation": "The proposed law that a particle’s acceleration is determined by spatial derivatives of local one-way light speeds is introduced without derivation from any action principle, equivalence principle, or tensorial dynamics. Its approximate form is coordinate-dependent, not generally covariant, and breaks down under Lorentz transformations, undermining the claim that it represents a physical law of inertial motion."
      },
      {
        "Problem": "Arbitrary mixing of asymmetric VSL models to correct radial acceleration sign",
        "Location": "Section 5, discussion of models 0, 1, 2 and Fig. 12, pages 8–9",
        "Explanation": "To resolve the wrong sign of radial acceleration, the author concocts a new model by grafting together spatial lengths from one model and time scales from another, each with different one-way light-speed conventions. This patchwork has no underlying physical or geometric justification, demonstrating that the desired acceleration arises from coordinate choices rather than any consistent theory."
      }
    ],
    "think_summary": [
      "I'm considering the user's request to evaluate the attached paper for major errors that could undermine its conclusions. They specifically want me to focus on serious issues like conceptual errors, contradictions, and flaws in core arguments without getting bogged down by minor typos. The paper revolves around a specific metric and its derivation in Euclidean geometry, so I'll need to carefully assess any misuse of geometry or incorrect derivations. This sounds like a fascinating challenge! I'll structure my findings in a clear JSON format as requested.",
      "I'm analyzing the user’s paper, noting some potential critical errors. The author generalizes variable speeds of light and tries to reconstruct a model but seems to misapply Euclidean principles. Using Euclidean geometry to derive Lorentz transforms appears flawed, as it might confuse different types of transformations. The concepts of normalization and covariance lack proper justification. There are issues with the motion law's assumptions and the fundamental conflict with relativity when discussing light speed. I need to summarize these findings clearly.",
      "I'm analyzing the user's paper and noting some critical issues. The author seems to confuse coordinate artifacts with physical reality, particularly regarding light speed choices. Misinterpretations of Lorentz transformations as Euclidean rotations are problematic, as they don't preserve necessary geometric properties. The assumptions on light clocks and general covariance lack proper justification. The approach to radial acceleration also appears to mix models without a sound basis. Ultimately, using a Euclidean metric to model these concepts leads to significant flaws. I’ll summarize these in JSON format."
    ],
    "token_usage": {
      "input": 11809,
      "thinking": 1856,
      "output": 702
    }
  },
  {
    "entry_id": 93,
    "retraction_id": "1111.3825v3",
    "paper_id": "1111.3825v2",
    "retraction_comment": "This paper has been withdrawn by the author due to a crucial error in lemma 3.7",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Unjustified assumption of trivial parabolic structure",
        "Location": "Definition 2.4; used in Theorem 3.1",
        "Explanation": "The paper assumes without proof that the tame nilpotent harmonic bundle constructed via Jost–Zuo carries a trivial parabolic structure (i.e. all flat sections have growth bounded by arbitrarily small exponents). This property is crucial for the L2–intersection comparison, but Jost–Zuo’s theorem only yields a tame harmonic metric, not the vanishing of all nontrivial weights in the parabolic filtration."
      },
      {
        "Problem": "Missing real structure on the harmonic bundle",
        "Location": "Definition 2.5; invoked in Theorem 3.11",
        "Explanation": "The authors require a real structure on (E,θ,h) to apply Mochizuki’s purity theorem for the partial Koszul complex, but they neither prove nor cite that the reductive complex representation ρ admits such an anti‐linear involution. Without a genuine real form, the key surjectivity in Theorem 3.11 fails."
      },
      {
        "Problem": "Application of purity theorem to a complex (not real) local system",
        "Location": "Theorem 3.11",
        "Explanation": "Mochizuki’s Theorem 9.6 on the purity of weights in H*(Π(N1,…,Nn)) presupposes a polarized (real) variation of Hodge structure or at least a real form. Here V is only a complex reductive local system; no polarization or real structure is established, so the needed weight‐filtration purity may not hold."
      },
      {
        "Problem": "Use of L2–Hodge decomposition for a non‐unitary flat connection",
        "Location": "Propositions 3.9 and 3.10",
        "Explanation": "The arguments in §§3.9–3.10 follow Kashiwara–Kawai’s Hodge‐theoretic decomposition for unitary flat bundles. In the present Higgs‐bundle setting the flat connection D does not preserve the harmonic metric (hence is not unitary), and no justification is given that D+D* is essentially self‐adjoint or that an L2–Hodge decomposition holds."
      },
      {
        "Problem": "Inconsistent definition of the (t,x,y) coordinate change",
        "Location": "Page 6, formula “z_j = exp(2πi x_j − (t y_j)^{−1})”",
        "Explanation": "The transition from the punctured‐polydisc coordinates z_j to (t,x,y) must satisfy |z_j|=exp(−y_j/t) for the Poincaré metric estimates. The paper’s formula uses (t y_j)^{−1} in the exponent, which is inconsistent with standard parametrizations and undermines the subsequent metric equivalences in Lemmas 3.5–3.8."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 6975,
      "thinking": 3264,
      "output": 675
    }
  },
  {
    "entry_id": 94,
    "retraction_id": "1711.04838v3",
    "paper_id": "1711.04838v2",
    "retraction_comment": "Lemma 9.1 is incorrect and so the main result is wrong",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Lemma 9.1’s homology argument miscounts intersections and draws an invalid contradiction",
        "Location": "Section 9, Lemma 9.1",
        "Explanation": "The proof assumes a single non-trivial double point circle C carries all three triple points, so C^U and C^L must intersect three times. But from Lemma 8.3 one triple point lies on a double point arc, not on C, so C^U∩C^L has only two crossings. The numerical homology contradiction fails, invalidating the lemma."
      },
      {
        "Problem": "Case analysis in Theorem 9.2 relies on unproven homology distinctions of decker curves",
        "Location": "Section 9, Theorem 9.2",
        "Explanation": "The authors enumerate seven combinatorial connection types and declare each impossible by arguing that certain upper and lower decker curves represent generators of H1(F) and must intersect oddly. However, they never prove that these curves are non-separating generators or that intersection counts alone determine distinct homology classes in the genus-one surface. The conclusions therefore lack foundation."
      },
      {
        "Problem": "Lemma 5.2’s use of Roseman moves to eliminate triple points is insufficiently justified",
        "Location": "Section 5, Lemma 5.2",
        "Explanation": "The lemma claims that if a b/t–m/t or b/t–b/m edge bounds a disk whose projection contains at most one triple point, the diagram is not t-minimal. The proof invokes a complex sequence of R-moves (R-1+, R-7, R-5+, R-6–) without verifying that these can be performed in the presence of nearby singularities or that they do not introduce new triple points. Critical local obstructions are ignored."
      },
      {
        "Problem": "Lemma 8.5’s inequality on total degenerate edges lacks proof",
        "Location": "Section 8, Lemma 8.5",
        "Explanation": "The argument reduces to the statement #d(T1)+#d(T2) ≥ 4, where #d(T) is the number of degenerate double edges at T. This inequality is asserted without combinatorial or topological justification (it is not a direct corollary of earlier lemmas). The subsequent elimination of non-degenerate colourings therefore stands on an unproven premise."
      },
      {
        "Problem": "Lemma 8.3 unjustifiably generalizes Satoh’s result from 2-knots to higher-genus surface-knots",
        "Location": "Section 8, Lemma 8.3",
        "Explanation": "The lemma is cited to Satoh [9], but that work concerns 2-knots (genus zero). The structural constraints on b/t-edges and Alexander numbering in genus-one diagrams need independent proof. Applying Satoh’s sphere-knot classification to genus-one surfaces is invalid without adaptation."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 18357,
      "thinking": 1984,
      "output": 681
    }
  },
  {
    "entry_id": 95,
    "retraction_id": "2010.13204v2",
    "paper_id": "2010.13204v1",
    "retraction_comment": "The conclusion of the paper is likely wrong. The second necessary integration step which is barely mentioned at the end of section B introduces another correlation between the vacuum fluctuations which will negate any gain of the here proposed method. I overlooked this. Thanks to [REDACTED-NAME] for figuring it out",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Neglect of technical noise sources in feedback loop",
        "Location": "Section II.B (Random noise in active regeneration systems)",
        "Explanation": "The analysis models noise solely as vacuum (shot) noise v_AM, but omits realistic noise contributions from detectors, amplifiers, amplitude modulators, and other electronics in the feedback loop. Such technical noise (e.g. electronic readout noise, modulator driver noise, thermal fluctuations) can dominate over shot noise, impose limits on achievable gain r, and degrade SNR. Ignoring these sources invalidates the claim of equivalence to an ideal optical cavity."
      },
      {
        "Problem": "Failure to account for loop delay and limited bandwidth",
        "Location": "Section II.A (Active Regeneration in ALPS)",
        "Explanation": "The proposed active feedback involves a round-trip delay of 2τ and finite electronic processing time. Yet the derivation assumes instantaneous feedback and perfect phase condition φ_RT=N·2π. In practice, loop delays and limited bandwidth introduce frequency-dependent phase shifts and reduce effective gain at the signal sideband frequency Ω, potentially leading to instability or insufficient coherent build-up, undermining the idealized geometric-series gain model."
      },
      {
        "Problem": "Invalid equivalence of electronic gain r to optical reflectivities ρ₁ρ₂",
        "Location": "Sections II.A and II.B (Definition and use of r = 2E₀²g)",
        "Explanation": "The paper substitutes the cavity round-trip reflectivity ρ₁ρ₂ with r=2E₀²g, treating them interchangeably in SNR formulas. However, ρ₁ρ₂ is a passive, frequency-independent optical parameter, whereas r depends on the injected LO field amplitude E₀ and adjustable electronic gain g, with different units and dynamic behavior. This mismatch invalidates the direct comparison and the conclusion that SNR is 'essentially identical'."
      },
      {
        "Problem": "Omission of phase noise accumulation across feedback iterations",
        "Location": "Section II.A (Geometric series solution for PSig)",
        "Explanation": "The infinite-sum solution for the signal field assumes perfect phase coherence of successive reinjections (φ_RT exactly N·2π). In reality, residual phase noise from the RF phase shifter φ_RF, laser frequency jitter, and electronics will decorrelate the reinjected signal over multiple passes, reducing build-up and altering both signal amplitude and noise statistics. The paper does not quantify this effect, risking overestimation of gain."
      },
      {
        "Problem": "Ignoring non-ideal modulator characteristics and losses",
        "Location": "Sections II.A and II.C (Active regeneration setup and implementation issues)",
        "Explanation": "Real amplitude modulators exhibit finite extinction ratios, insertion loss, limited linearity, and add their own noise. The model treats the modulator as lossless and perfectly linear, thereby neglecting attenuation of both signal and LO, leakage of unwanted frequencies, and added modulator noise. These factors reduce effective gain and introduce extra noise, potentially invalidating the predicted SNR performance."
      }
    ],
    "think_summary": [
      "**Identifying critical issues**\n\nI'm breaking down the SNR derivation, focusing on some conceptual errors I’ve noticed. They derived SNR ∝ r nS T/(1 - r^2), which makes sense, but when r is almost 1, the denominator approaches 0, leading to a big SNR. However, noise can also spike, especially near the lasing threshold where loop noise dominates.\n\nI need to highlight five critical issues: electronic and modulator noise being ignored, loop delay and finite bandwidth not considered, phase noise overlooked, assumptions of equivalence made, and neglecting out-of-loop vacuum noise injection. Let's outline these points clearly."
    ],
    "token_usage": {
      "input": 8402,
      "thinking": 1216,
      "output": 691
    }
  },
  {
    "entry_id": 96,
    "retraction_id": "2208.11892v3",
    "paper_id": "2208.11892v2",
    "retraction_comment": "Lemma 3.2 does not hold. A counter example is $f \\equiv 1$",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Unproved existence of divergence-preserving quasi‐interpolation operator Ih in three dimensions",
        "Location": "Appendix A (A.2–A.3)",
        "Explanation": "The main Theorem 2.1 is stated for N=2,3, but the construction of Ih satisfying (H2)–(H3) is only given for planar (N=2) Taylor–Hood and MINI elements. The 3D case is claimed to be 'straightforward' without proof. Without a verified 3D operator, the inf–sup stability and divergence‐compatibility needed for the finite element approximation in N=3 are unestablished, invalidating the extension of the main result to three dimensions."
      },
      {
        "Problem": "Insufficient justification of W³,p–regularity for the Stokes resolvent on the half-space in 2D",
        "Location": "Lemma 3.2, page 7",
        "Explanation": "Lemma 3.2 invokes a |λ|*||u||_{W¹,p} bound for the half-space Stokes resolvent in N=2 by 'modifying' the N=3 argument of [27], but no details or references are provided. This W³,p(Ω)×W²,p(Ω) regularity underpins the global error estimates; without a validated half-space resolvent estimate in 2D, the higher‐order regularity claims (and hence the error analysis) lack foundation."
      },
      {
        "Problem": "Unjustified star-shaped assumption for local domains in Bogovskiĭ construction",
        "Location": "Lemma 6.1, page 13",
        "Explanation": "The proof of the local L² estimate for the pressure error uses the Bogovskiĭ operator on D̃_j = B(x_j,(1+¾)d)∩Ω_h and asserts D̃_j is star-shaped with respect to a ball of radius O(d). However, when x_j lies on ∂Ω_h, D̃_j is a half-ball cut by the polygonal boundary, and the claim that it contains a full inscribed ball and is star-shaped is not rigorously justified. If D̃_j fails the star-shaped condition, the Bogovskiĭ estimate (3.1) does not apply."
      },
      {
        "Problem": "Incorrect inclusion of endpoint values in the p-range",
        "Location": "Theorem 2.1 and p-restriction, page 5",
        "Explanation": "Theorem 2.1 allows p up to and including 2N/(N−2), but the proof’s kick-back argument requires the strict inequality p<2N/(N−2) (and similarly p>2N/(N+2)). At the endpoints the absorption of terms fails, so the resolvent and error estimates cannot be claimed at p=2N/(N−2) or p=2N/(N+2) without additional arguments."
      },
      {
        "Problem": "Gap in covering the full resolvent parameter range",
        "Location": "Section 8.1, page 18",
        "Explanation": "The analysis splits into regimes |λ|≤d^{-2}=(Kh)^{-2} and |λ|≥αh^{-2} for an arbitrarily fixed α, but does not relate α to K^{-2}. Unless α≤K^{-2}, there is an intermediate range K^{-2}h^{-2}<|λ|<αh^{-2} not covered by either case. Without specifying α=K^{-2}, the proof fails to establish the estimate uniformly for all λ∈Σ_δ."
      }
    ],
    "think_summary": [
      "**Analyzing exponent conditions**\n\nI'm noticing a potential issue with the conditions surrounding p in the equation p=2N/(N-2). The critical point occurs at p=6, but I need to dig deeper into whether the proof allows for endpoints. They seem to require p to be strictly less than 2N/(N-2) and greater than 2N/(N+2). \n\nIf I set p=6, the exponent evaluates to 0, which can't be absorbed into the left-hand side, leading me to think p=6 should be excluded.",
      "**Identifying critical problems**\n\nI've identified some key issues here. First, the construction of the divergence-modified quasi-interpolation operator in 3D is missing and unproven, though the main theorem covers N=3. There's also an unsupported assumption regarding the star-shaped nature of local domains near boundaries, which needs justification. The theorem's p-range includes endpoints, but the proof requires strict inequalities. Furthermore, a critical reference is absent for the W3,p resolvent estimate in 2D. Lastly, the assumption about triangles having interior vertices might not always hold."
    ],
    "token_usage": {
      "input": 28470,
      "thinking": 7616,
      "output": 825
    }
  },
  {
    "entry_id": 97,
    "retraction_id": "1507.00282v2",
    "paper_id": "1507.00282v1",
    "retraction_comment": "This paper has been withdrawn due to a crucial error in the proof of Proposition 14. The authors are very thankful to [REDACTED-NAME] for kindly pointing it out",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Incorrect claim that the Nijenhuis tensor vanishes on mixed-type vectors",
        "Location": "Section 5, proof of Proposition 12, just after equation (13)",
        "Explanation": "The authors assert “N(Z_i, \\ubar Z_j) = 0, which follows immediately from (3).”  However, for an almost complex (nonintegrable) structure one generally has N(Z, \\ubar W) ≠ 0.  The identity (3) does not imply vanishing of N on one (1,0) and one (0,1) vector, and this step is essential to the subsequent derivation of the key formula relating d(Jφ) to N(Jφ).  Without it, Proposition 12 is not established."
      },
      {
        "Problem": "Unjustified splitting of d(Jφ) into dφ and N(Jφ) terms",
        "Location": "Section 5, Proposition 12 statement and proof",
        "Explanation": "The main identity d(Jφ) = -2 Im(dφ)_{1,2} - 2 N(Jφ) is asserted after combining equations (13) and (14), but the algebraic passage is neither shown in detail nor are the signs and factors for the N(Jφ) term checked.  Given the delicate nature of complex-type decompositions of dφ and of the trilinear N(·) construction, the derivation is incomplete and may contain sign or numerical errors, calling into question all subsequent results that rely on this formula."
      },
      {
        "Problem": "Potential factor‐of‐two and orientation sign errors in the codifferential computation",
        "Location": "Section 5, proof of Proposition 13, equations (17)–(19)",
        "Explanation": "In deriving (δJφ)(X)+δφ(JX)=2⟨Jφ,N_X⟩ the authors use a combination of basis summations, wedge–star identities, and symmetries of (∇J) that is sketched but not fully justified.  There is a risk of missing 1/2 or sign factors (for instance, in converting α∧β to ⟨α,β⟩vol and in counting off-diagonal terms), which would invalidate the precise relation needed in Proposition 14 to link the Nijenhuis components to the coefficients of φ.  A correct treatment requires a careful bookkeeping of all combinatorial factors."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 8721,
      "thinking": 8576,
      "output": 553
    }
  },
  {
    "entry_id": 98,
    "retraction_id": "1707.00947v2",
    "paper_id": "1707.00947v1",
    "retraction_comment": "I am so sorry, the hypothesis proposed by this paper would be not appropriate because there is no mechanism on which can be based between money and output value in this paper. The equation maybe more empty",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Ad hoc specification of the dynamical quantity equation",
        "Location": "Section “A dynamical quantity equation of exchange”, Equation (2)",
        "Explanation": "Equation (2) is introduced without any microeconomic foundation or empirical justification. The assertion that the gap M(t)−P(t)Y(t) is proportional to the growth rate of P(t)Y(t) is purely ad hoc. All subsequent analytical results and policy implications rest on this unvalidated assumption, so the entire framework is unsound if (2) does not hold in reality."
      },
      {
        "Problem": "Unrealistic long-run price dynamics",
        "Location": "Examples section, Equations (5), (10) & (14)",
        "Explanation": "Under constant or linearly growing money supply the model implies prices P(t)→0 as t→∞ whenever real output grows (g>0), and under linear money growth even exponential declines. These predictions conflict with observed price behavior (prices do not asymptotically vanish), indicating the model’s dynamics mis-represent real economies and invalidate conclusions about balanced inflation–growth paths."
      },
      {
        "Problem": "Flawed empirical validation of the balanced path",
        "Location": "Section “Examples”, Figure 1",
        "Explanation": "The cross-country log–log regression of average inflation on average (money growth − output growth) mixes heterogeneous economies, ignores simultaneity (inflation affects money demand/growth) and contains outliers. Use of logarithms with negative or near-zero inflation rates is mathematically problematic. The high fitted slope does not establish causality or validate the theoretical path c=q−g."
      },
      {
        "Problem": "Unsubstantiated business-cycle typology",
        "Location": "Section “The hypothesis of natural cycle and driving cycle”",
        "Explanation": "The classification into natural vs. driving cycles and eight discrete behaviors (GI, GO, LI, LO, DD, DR, etc.) is based solely on geometric moves in a stylized c–g diagram. No time-series econometric tests, statistical clustering, or real-world cycle dating are provided. Without empirical support, these arbitrarily defined regimes cannot justify the policy or interpretative claims."
      },
      {
        "Problem": "Neglect of money demand, interest rates and expectations",
        "Location": "Through Sections 2–4 (especially post-Equation (2))",
        "Explanation": "By omitting velocity, liquidity preference, nominal interest rates and forward-looking expectations, the model reduces money’s role to an exogenous flow. This oversimplification contradicts well-established monetary dynamics and undermines any recommendations on inflation control or monetary policy."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 20040,
      "thinking": 1216,
      "output": 609
    }
  },
  {
    "entry_id": 99,
    "retraction_id": "2212.04536v2",
    "paper_id": "2212.04536v1",
    "retraction_comment": "Thm. A, Cor. B, are incorrect as stated and would require additional assumptions on q (a result of a missing assumption in another paper). Withdrawn until I obtain a working solution",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Unjustified full Galois‐invariance of the extended GGGC",
        "Location": "Lemma 3.3",
        "Explanation": "The proof invokes [SFT22, Prop. 6.7] to conclude Γˆ_{u,2}^σ=Γˆ_{u,2} for every σ in the full Galois group Gal(ℚ̄/ℚ).  However, that proposition only controls automorphisms acting on p-power roots of unity (i.e. σ(ζ_p)=ζ_p^k).  It does not address the action of σ on other cyclotomic factors appearing in the values of generalized Gelfand–Graev characters.  Without a result covering the entire Galois group, the claimed σ-invariance of Γˆ_{u,2} is unproven."
      },
      {
        "Problem": "Multiplicity argument for picking the correct extension in Theorem A is unsupported",
        "Location": "Proof of Theorem A (end of §3.1)",
        "Explanation": "The argument assumes that for a chosen GGGC Γˆ_{u,2}, exactly one of the two extensions χˆ or χˆ·β of χ to G⁺ appears as an irreducible constituent of Γˆ_{u,2}, and with multiplicity one.  But no computation or reference is given to guarantee that the other extension does not also occur or that multiplicities are exactly one.  Without this, one cannot identify a uniquely σ-invariant constituent and the field‐of‐values conclusion fails."
      },
      {
        "Problem": "Claimed equality of Schur indices lacks justification",
        "Location": "Lemma 3.4",
        "Explanation": "The paper asserts m_ℚ(χ̂)=m_ℚ(χ).  In fact [Isa06, Lem. 10.4] only yields the divisibility m_ℚ(χ̂)∣m_ℚ(χ)[ℚ(χ,χ̂):ℚ(χ̂)].  To deduce equality one needs further input (e.g. that [ℚ(χ,χ̂):ℚ(χ̂)]=1 and that the divisor cannot be proper), which is not provided."
      },
      {
        "Problem": "Rationality of exceptional‐type characters in Table 1 is not fully established",
        "Location": "Proof of Theorem 4.1, Table 1",
        "Explanation": "The table lists principal‐series unipotent characters for exceptional groups claimed to be ℚ-valued by [BC72] with corrections in [Cur75].  However Curtis’s erratum notes subtle exceptions (notably in types E_7 and E_8 for certain q) where values involve nontrivial square roots.  The paper does not verify that the listed characters avoid these exceptions, so their ℚ-rationality is unsubstantiated."
      },
      {
        "Problem": "Inductive reduction in Theorem C uses an almost‐simple quotient whose order need not divide n",
        "Location": "Proof of Theorem C (last paragraph)",
        "Explanation": "The final induction step applies Theorem 4.1 to H=N_G(S)/C_G(S), implicitly assuming |H| divides n so that σ∈Gal(ℚ(e^{2πi/n})/ℚ) acts on its character values.  No argument is given that |H|∣n.  If this divisibility fails, σ need not stabilize Irr(H) as required, and the induction breaks down."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 8918,
      "thinking": 4288,
      "output": 822
    }
  },
  {
    "entry_id": 100,
    "retraction_id": "1405.2435v10",
    "paper_id": "1405.2435v9",
    "retraction_comment": "lemmas 11 is wrong. The conjecture is not proved.",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Unproven existence of the special matrices Lx solving Mu·Lx = Ms",
        "Location": "Lemma 9",
        "Explanation": "Lemma 9 claims that for every mapping Mu there exists a 0–1 matrix Lx with exactly one 1 per row satisfying Mu·Lx = Ms and the required value of (S,x). No construction or compatibility argument is given to show that such an Lx always exists. Without these matrices the entire path‐and‐dimension framework collapses."
      },
      {
        "Problem": "Invalid deduction of equality M_s·Lw = L_{sw} in Lemma 11",
        "Location": "Lemma 11 (proof, around equations (9)–(10))",
        "Explanation": "The proof concludes Lz = M_s·Lw for the synchronizing word s and then derives a rank contradiction. But M_s is singular, so from M_s·Lw ∼_q L_{sw} one cannot conclude equality M_s·Lw = L_{sw}. The use of Lemma 7’s invertibility clause on M_s is illegitimate, invalidating the final contradiction."
      },
      {
        "Problem": "Circular reliance on an assumed minimal synchronizing word s",
        "Location": "Section 3 (definition of the rational series S)",
        "Explanation": "The rational series S and many subsequent dimension bounds are defined in terms of a minimal synchronizing word s and its target state q. Yet the goal of the paper is to prove a bound on the length of s itself. This circularity undermines the logical foundation of the entire argument."
      },
      {
        "Problem": "Unspecified scalar field invalidates linear‐combination arguments",
        "Location": "Lemma 2 and throughout Sections 2–5",
        "Explanation": "The paper freely forms linear combinations of 0–1 matrices with arbitrary coefficients λ_i, but never specifies the underlying field (ℝ, ℚ, finite field, etc.). Over different fields independence properties and the interpretation of λ_i change, so many span and dimension claims need explicit field assumptions to hold."
      },
      {
        "Problem": "Incorrect extension from strongly connected to general automata",
        "Location": "Theorem 2 and Theorem 4",
        "Explanation": "Theorem 2 asserts that proving the bound for strongly connected automata immediately yields it for all complete automata by citing a reduction in [8]. However, that reduction may increase shortest synchronizing‐word lengths, and no detailed argument is given to show the bound survives the reduction."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 16671,
      "thinking": 5504,
      "output": 585
    }
  },
  {
    "entry_id": 101,
    "retraction_id": "1705.07013v2",
    "paper_id": "1705.07013v1",
    "retraction_comment": "The proofs involve use of structured input states which could not be generalized",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Misapplication of the no-signaling theorem to a global deletion operation",
        "Location": "Section II (No Communication Theorem application), around Eq. (10)",
        "Explanation": "The no-communication theorem constrains spatially separated parties performing local measurements on an entangled state. Here, the deletion machine is a single global unitary acting jointly on two copies at the same location. There is no sender and receiver scenario, so no-signaling imposes no restriction on such a global operation and cannot be used to bound the fidelities."
      },
      {
        "Problem": "Imposing constraints on machine outputs for non-identical inputs beyond its domain",
        "Location": "Section II, Eqs. (11)–(12)",
        "Explanation": "A universal deletion machine is only required to act on two identical copies. The paper arbitrarily defines and constrains its action on inputs like |↑⟩⊗|↓⟩, which lie outside the machine’s specified domain. This unjustified extension is critical to their derivation of the fidelity bound."
      },
      {
        "Problem": "Incorrect application of the covariance (universality) condition",
        "Location": "Section II, leading to Eq. (9)",
        "Explanation": "Upon enforcing rotational invariance about the input Bloch vector, the authors simultaneously set b_y = b_z = 0 but then retain an η₂ b_z σ_z term in Eq. (9). The blank-state Bloch parameters and correlation coefficients (t_ij) are inconsistently handled, invalidating the subsequent density-matrix expressions."
      },
      {
        "Problem": "Algebraic error in fidelity normalization",
        "Location": "Eq. (7)",
        "Explanation": "Given ρ_out1 = (𝐼 + η₁ m·σ)/2 and ρ(m) = (𝐼 + m·σ)/2, the fidelity F_p = Tr[ρ(m) ρ_out1] should equal (1 + η₁)/2. The paper instead writes F_p = 1 + η₁/2, which numerically overestimates the true fidelity by 1/2."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 6924,
      "thinking": 2240,
      "output": 518
    }
  },
  {
    "entry_id": 102,
    "retraction_id": "1801.01544v2",
    "paper_id": "1801.01544v1",
    "retraction_comment": "We withdraw the manuscript because Lemma 2.3 is false",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Use of Schauder fixed‐point theorem in L¹ with an Lᑫ–constraint domain",
        "Location": "Lemma 4.3 (page 16)",
        "Explanation": "The authors define the convex set O = {v ∈ L¹(Ω): v ≥ 0, ‖v‖_{Lᑫ(Ω,δˢ)} ≤ Λ} and then apply Schauder in the Banach space L¹(Ω).  But the map v→‖v‖_{Lᑫ(Ω,δˢ)} is not continuous in the L¹–topology unless q>1+s (so that δ⁻ˢ/(q−1)∈L¹), hence O need not be closed in L¹(Ω).  Without closedness of O in L¹ the fixed‐point argument fails and the existence of the approximating solutions {vₙ} is not secured."
      },
      {
        "Problem": "Extension argument in the doubling lemma step omits the negative tail",
        "Location": "Proof of Theorem 3.1, Step 2 (pages 10–11)",
        "Explanation": "To derive a global Liouville contradiction the authors extend v_k by zero outside B(0,k) and assert (−Δ)ˢ ṽ_k ≥ ṽ_kᵖ in B(0,k).  However the zero‐extension contributes a negative ‘tail’ term in the fractional Laplacian, so one only has (−Δ)ˢ ṽ_k = v_kᵖ −∫_{|y|>k}v_k(y)/|x−y|^{N+2s}dy, which can be strictly less than v_kᵖ.  Ignoring this negative contribution invalidates the passage to the Liouville theorem."
      },
      {
        "Problem": "Application of Liouville theorem to a supersolution",
        "Location": "Proof of Theorem 3.1, end of Step 2 (page 11)",
        "Explanation": "The contradiction is derived from [21, Theorem 1.3], which rules out nontrivial entire solutions of (−Δ)ˢ v = vᵖ.  But the limiting v satisfies only the inequality (−Δ)ˢ v ≥ vᵖ (a supersolution), and the cited result does not cover supersolutions.  Without a Liouville theorem valid for (−Δ)ˢ v ≥ vᵖ the blow‐up argument breaks down."
      },
      {
        "Problem": "Bootstrap to L∞ in Proposition 1.6 may not start if p is too small",
        "Location": "Proof of Proposition 1.6 (pages 12–13)",
        "Explanation": "The bootstrap relies on Lemma 2.8 to lift integrability: one needs an exponent t_j eventually exceeding N/(2s) to reach L∞.  But the initial choice t₀=(1+p_s/p)/2 may lie below N/(2s) when p is close to 1, and the recursive formula t_{j+1}=(N t_j)/(p(N−2s t_j)) may not increase fast enough.  No uniform check is given that t_j surpasses N/(2s), so boundedness and hence regularity in C^{2s+α} is not guaranteed."
      },
      {
        "Problem": "Well‐posedness of the normalized boundary trace",
        "Location": "Definition 1.4 (page 4)",
        "Explanation": "The notion of s‐boundary trace trˢ(u) = μ is used to represent every solution as u=G_s[f(u)]+M_s[μ].  But the nonlocal trace defined by β^{1−s}∫_{Σ_β}|u−M_s[μ]|dS→0 need not be unique unless additional growth conditions at the boundary are imposed.  Without uniqueness of the trace the representation (1.13) and subsequent existence and nonexistence results depending on μ may fail."
      }
    ],
    "think_summary": [
      "**Identifying critical errors**\n\nThe user is asking me to check for significant errors and soundness issues in a paper that could compromise its conclusions. I need to focus on identifying critical problems while ignoring minor ones. I’ll specifically look for logical gaps, incorrect conditions, and poorly defined notions. One potential issue I see is with their definition of viscosity solutions for the fractional Laplacian, where they may not appropriately use test functions. This will require careful examination to clarify.",
      "**Spotting critical problems**\n\nI’ve identified a potential critical issue regarding the definitions of test functions in the paper. They state T(Ω) is a subset of X0, but earlier, they defined weak solutions using test functions in Xs(Ω). This difference in norms raises concerns about their justification for using test functions interchangeably. Additionally, they seem to encounter problems with a priori estimates in nonbounded domains, particularly with claims about nonnegative s-harmonic functions and unbounded conditions. This needs careful reevaluation to ensure consistency in their formulations.",
      "**Analyzing potential critical issues**\n\nI've identified several potential critical issues in the paper. They claim to derive a priori estimates while including gradient terms, referencing some prior work, but there are nuances that could lead to misinterpretation. For instance, they suggest that using nonlocal equations with rough kernels requires u to be globally bounded, which may not be the case. I worry about the potential mismatch regarding test function spaces, the application of fixed-point theorems, and assumptions around boundary behavior. I'll need to clarify these concerns in detail to highlight their implications.",
      "**Refining critical issues**\n\nIn my review, I noticed several critical issues that could undermine the paper's conclusions. For instance, the construction of the Schauder fixed point can create a non-closed domain in L^1. The extension of v_k outside its intended domain might lead to violations of necessary inequalities. Also, there's a potential misapplication of the Liouville theorem because it requires an equality sign that may not hold. Other concerns include bounds not being achievable in specific dimensions and non-uniqueness in the boundary trace definition. It’s essential to address these as JSON entries with clear location references for clarity."
    ],
    "token_usage": {
      "input": 26274,
      "thinking": 6144,
      "output": 903
    }
  },
  {
    "entry_id": 103,
    "retraction_id": "2101.02111v2",
    "paper_id": "2101.02111v1",
    "retraction_comment": "We have found some data mistakes in our paper. The inverse transition in Section 3 (Figure 3d) and 7.1 (Figure 31) is probably consequence of inappropriate mesh configuration",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [],
    "think_summary": [
      "Error: File size of 34MB exceeds OpenAI's 32MB limit in a single request. We've tried all size reduction methods but still cannot make it below the limit. Answer defults to an empty list."
    ],
    "token_usage": {
      "input": null,
      "thinking": null,
      "output": null
    }
  },
  {
    "entry_id": 104,
    "retraction_id": "1703.04839v2",
    "paper_id": "1703.04839v1",
    "retraction_comment": "Our discussion omits the dominant tree-level shift (discussed in 1203.0237) of the Higgs quartic coupling in the full theory relative to the Higgs quartic coupling of the low-energy effective Standard Model. This shift can easily the electroweak vacuum. Therefore our conclusions (and upper bound on $f_a$) are invalid",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Crude treatment of threshold decoupling without finite matching corrections",
        "Location": "Section II, around eqs. (2)–(6)",
        "Explanation": "The authors implement decoupling of the heavy PQ scalar and fermions by switching their contributions on and off with step-function factors s_φ and s_Q, but they neglect finite threshold matching corrections at μ = m_φ, m_Q. Such corrections generically shift the Higgs quartic, portal coupling, gauge and Yukawa couplings by O(λ_hφ, λ_φ, y_Q) at the decoupling scales and can substantially alter the RG evolution. Omitting them undermines the claimed stabilization of the Higgs potential and the derived upper bound on f_a."
      },
      {
        "Problem": "Inconsistent loop order in the renormalization group equations",
        "Location": "Section II, after eq. (6)",
        "Explanation": "While the SM β-functions are taken at two loops, the new physics contributions from the KSVZ sector are included only at one loop. Mixed two-loop terms involving the portal coupling (e.g. O(g_3^2 λ_hφ), O(y_t^2 λ_hφ)) and two-loop threshold effects are omitted. For portal couplings as large as 0.1–1, these neglected terms can be comparable to or exceed the one-loop contributions, leading to an unreliable determination of the Higgs quartic running and invalidating the stability analysis."
      },
      {
        "Problem": "Oversimplified effective potential neglects loop effects and multi-field structure",
        "Location": "Section II, eq. (7)",
        "Explanation": "The paper approximates the effective potential as V(h)=λ_h(μ)h^4/4 with μ=h, omitting Coleman-Weinberg corrections from the PQ scalar and fermion loops, gauge-dependence issues of the potential, and the explicit φ-dependence of the two-field system. This simplification fails to capture potential barrier heights, the position of new minima, and bounce actions, so the conclusion that the vacuum is globally stable based solely on λ_h(μ)>0 is not justified."
      },
      {
        "Problem": "No analysis of full two-field vacuum stability including the φ direction",
        "Location": "Section II, discussion around Figs. 1–2",
        "Explanation": "By focusing only on the Higgs direction after decoupling the PQ scalar, the authors ignore the possibility of deeper minima in the combined (H,Φ) field space induced by the portal coupling. A proper vacuum stability analysis must scan all field directions; without this, one cannot exclude that the electroweak vacuum is metastable or that new instabilities appear along mixed H–Φ directions, throwing into question the derived bound on f_a."
      },
      {
        "Problem": "Claimed naturalness of large portal coupling ignores reintroduced fine-tuning",
        "Location": "Introduction, paragraphs 3–5",
        "Explanation": "The paper asserts that λ_hφ ~ O(0.1–1) is natural once renormalization is accepted, but a portal coupling of this size with f_a ≫ v induces a Higgs mass correction δm_h^2 ~ λ_hφ f_a^2 that is many orders of magnitude larger than the observed m_h^2. Absorbing this via renormalization reintroduces an extreme fine-tuning (cancellations at the level of O(10^–14)), undermining the naturalness argument that motivates the large coupling scenario."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 8206,
      "thinking": 1792,
      "output": 803
    }
  },
  {
    "entry_id": 105,
    "retraction_id": "1704.03487v2",
    "paper_id": "1704.03487v1",
    "retraction_comment": "Due to the limitations of the interatomic potential used, it was not possible to ensure the accuracy of some of the results in the paper",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Periodic boundary conditions suppress Orowan looping",
        "Location": "Section 3.2 (Geometry, boundary conditions and loading)",
        "Explanation": "By using periodic boundaries along the glide direction and a single precipitate per cell, the simulations prevent the formation and expansion of Orowan loops around the obstacle. As a result, dislocations are forced to cut the precipitate even in regimes where loop bypass would be energetically favored. The reported absence of Orowan looping is therefore an artifact of the simulation setup, not a robust physical finding."
      },
      {
        "Problem": "Inadequate validation of interatomic potential for precipitate mechanics",
        "Location": "Section 3.1 (Interatomic potential)",
        "Explanation": "The Mendelev EAM potential was selected on the basis of Mg matrix dislocation properties, but its ability to reproduce the formation energy, generalized stacking‐fault energies, and slip behavior inside the complex β-Mg17Al12 phase is not demonstrated. Without direct validation of the precipitate’s intrinsic mechanical response, conclusions about shearing mechanisms and CRSS in the precipitate are unreliable."
      },
      {
        "Problem": "Misapplication of Friedel’s bypass‐barrier model to shearing kinetics",
        "Location": "Section 4.3 (Free energy barrier)",
        "Explanation": "Equation (4) derives from Orowan‐type loop‐bypass transitions, yet it is applied here to a pure shearing mechanism. Moreover, only two temperatures and a narrow stress range are used to fit the model, making the extracted activation free energies (∆F₀) and stresses (τc) unsound. The kinetic analysis therefore does not reliably quantify the true energy barriers for precipitate cutting."
      },
      {
        "Problem": "Inconsistent comparison to Orowan stress using erroneous shear modulus",
        "Location": "Section 4.2 (Mechanics)",
        "Explanation": "The authors compute the theoretical Orowan stress using the low shear modulus (µ=12.8 GPa) of their fitted potential but compare it to CRSS values purportedly representative of real Mg (µ≈16.4 GPa). This inconsistent use of elastic constants biases the conclusion that CRSS exceeds Orowan strength, invalidating the claim that shearing dominates over looping even when τc>τO."
      },
      {
        "Problem": "Unrealistic precipitate geometry and size range",
        "Location": "Sections 3.2 & 4.1 (Model geometry & Interaction mechanisms)",
        "Explanation": "All precipitates are modeled as 3 nm‐thick disks up to only 10 nm in diameter, whereas real β-Mg17Al12 laths in Mg-Al alloys are much larger and have varied aspect ratios. This extreme size and shape simplification likely alters local stress fields and cutting mechanisms, limiting the applicability of the results to real alloy microstructures."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 30069,
      "thinking": 1472,
      "output": 647
    }
  },
  {
    "entry_id": 106,
    "retraction_id": "2402.04633v2",
    "paper_id": "2402.04633v1",
    "retraction_comment": "An auxiliary result (Theorem 2.4) turns out to be wrong. This invalidates the proof of the main result",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Global Hodge decomposition and compactness arguments rely on leafwise (tangentially) elliptic theory but the operator Δ∇ is not elliptic on all of M; it is only transversely degenerate. It is unclear that the L²‐compactness and spectral‐gap properties used in the proof of Proposition 2.1 follow from [3, Cor. C].",
        "Location": "Theorem 2.2 and its use in Section 2.2 (proof of Proposition 2.1)",
        "Explanation": "Álvarez López–Kordyukov’s Hodge decomposition is for the Fréchet space of smooth leafwise forms (tangentially elliptic), not for a fully elliptic operator on M. The paper then extracts an L²‐convergent subsequence and concludes Δ∇ has trivial kernel, but without full ellipticity one does not in general get the compactness or discreteness of spectrum needed to deduce the Poincaré‐type estimate (1)."
      },
      {
        "Problem": "Surjectivity of the averaging map Av onto pullbacks is assumed but not proven on the L²–level, and is critical for descending harmonic forms from Mc to M.",
        "Location": "Section 2.2, in the passage around equations (30)–(32)",
        "Explanation": "The argument uses Av(γₙ) ∈ ker Δπ*∇ and then writes Av(γₙ)=π*(ηₙ) for unique ηₙ. While smooth invariant forms descend, it is not shown that the continuous extension of π* onto L²(Ω¹(Fb,π*NF)) is surjective onto the Av–invariant harmonic L²–forms, nor that those Av–invariants lie exactly in the closed image of π*."
      },
      {
        "Problem": "The Stability Theorem of Hamilton [15] is unpublished and its Nash–Moser tameness hypotheses are invoked without verification that the constructed homotopy operators satisfy them.",
        "Location": "Introduction (’Stability Theorem (Hamilton [15])’) and its use in reducing Theorem to estimate (1)",
        "Explanation": "Hamilton’s result requires not only invertibility of Δ but existence of tame estimates for the homotopy operators H₁,H₂. The paper shows only the analytic inequality (1), but does not check the full suite of tame estimates needed for the Nash–Moser argument on the infinite‐dimensional space of foliations."
      },
      {
        "Problem": "The extension of Álvarez López–Kordyukov’s convergence result (Theorem 2.4) from scalar to vector‐valued sequences in Corollary 2.5 is sketched by a simple induction but may miss subtle cross‐term estimates.",
        "Location": "Corollary 2.5 and its proof",
        "Explanation": "The original theorem concerns scalar k‐forms. The paper asserts a straightforward induction to (Ωᵏ)ᵠ but does not address interactions between different components under the scaled Dirac operators Dₕ or justify uniform control of mixed‐component terms."
      },
      {
        "Problem": "The proof that π* commutes with the codifferential δ∇ (Lemma 1.11) hinges on statement (11) that δπ*∇(π*η) lies in the pullback image, but the local argument given may fail globally or in the L²–closure.",
        "Location": "Lemma 1.11, especially the justification of assertion (11)",
        "Explanation": "To show π* ∘ δ∇=δπ*∇ ∘ π*, the authors must prove δπ*∇(π*η) belongs to im π*. They give a local decomposition using a projectable frame and basic functions, but it is not clear that these local pieces glue to a global pullback in the smooth or L² topology, nor that δπ*∇ preserves the π*–image without extra structure."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 24887,
      "thinking": 3712,
      "output": 920
    }
  },
  {
    "entry_id": 107,
    "retraction_id": "2408.08124v4",
    "paper_id": "2408.08124v3",
    "retraction_comment": "Achieving reliable simulation of closed new domain formation processes using a single phase-field method is unconvincing and requires the use of multiple algorithms for parallel comparison with experiments",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Unsubstantiated vacuum quality in self-packaged microcavity",
        "Location": "Section III (Process and Performance Simulation), page 2",
        "Explanation": "The authors claim that high-angle tilt coating seals the cavity at the chamber’s vacuum level, yet provide no mechanism (e.g., evacuation port or getter) to remove trapped gas. In reality, the microcavity would close with ambient or chamber pressure, not high vacuum, invalidating the assumption of ballistic electron transport and the simulated device performance."
      },
      {
        "Problem": "Inappropriate use of Phase-field method for PVD deposition simulation",
        "Location": "Section III (Process and Performance Simulation), page 2",
        "Explanation": "Phase-field modeling is intended for microstructure and phase transformation problems, not for geometric shadowing and collimated deposition in PVD. Without experimental calibration or validation against known deposition profiles, the simulation of self-sealed cavities is unreliable."
      },
      {
        "Problem": "Oversimplified cut-off frequency model ignores transit-time and parasitics",
        "Location": "Equation (3) and accompanying text, page 1 and page 3",
        "Explanation": "Calculating ft solely as gm/(2π(Cgs+Cgd)) neglects electron transit-time delay, space-charge effects, parasitic inductance, and distributed capacitances in the vacuum channel. This small-signal model cannot predict true sub-THz performance, making the claimed 0.228 THz cutoff frequency unsound."
      },
      {
        "Problem": "Incorrect or inconsistent Fowler–Nordheim parameters",
        "Location": "Equations (2.1)–(2.3), page 1",
        "Explanation": "The expressions and numerical values for αFN and βFN deviate from standard Fowler–Nordheim formulations, lack field-enhancement factors, and omit unit consistency checks. This undermines the accuracy of the field emission current density used in the device simulations."
      },
      {
        "Problem": "Unsupported assumption of ballistic transport in low-vacuum channel",
        "Location": "Section II (Device Design), page 2",
        "Explanation": "The paper cites conflicting literature on low-vacuum operation yet proceeds to model electrons as fully ballistic. Without measured gas composition, pressure, or mean free path data, electron scattering cannot be neglected, invalidating the trajectory and collection-efficiency simulations."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 7477,
      "thinking": 832,
      "output": 553
    }
  },
  {
    "entry_id": 108,
    "retraction_id": "1901.08297v2",
    "paper_id": "1901.08297v1",
    "retraction_comment": "The significance and novelty of the paper were not addressed. There are a couple of errors about calculations: the excitonic decay rates were computed by solving Eq (11), not based on Eq. (13) that was only an initial guess for the decay rate",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Unvalidated approximation for excitonic inelastic scattering rates (neglecting key interaction diagrams)",
        "Location": "Sec. II A (Eq. 13, p. 6)",
        "Explanation": "The excitonic decay rates Γ^{e–e} are estimated by including only the first two self-energy diagrams (A and B) of Fig. 1, omitting the screened particle–hole interaction terms (C and D) in the Bethe–Salpeter kernel. While this approximation was benchmarked for Si₈₀, its transferability to larger Si₂₆ and Si₄₆ clusters is not demonstrated. Neglecting these diagrams can substantially misestimate exciton lifetimes and thus the MEG rate, undermining the key conclusion on excitonic MEG performance."
      },
      {
        "Problem": "Crude adiabatic and harmonic treatment of electron–phonon coupling for nonradiative relaxation",
        "Location": "Sec. II B (Eqs. 22–34, pp. 7–9)",
        "Explanation": "The nonradiative relaxation rates W_{i→j} are computed under the Born–Oppenheimer approximation with only diagonal (adiabatic) PES terms, a displaced harmonic oscillator model, and neglect of off-diagonal nonadiabatic couplings, anharmonicity, and mode mixing. Such approximations are known to fail for high-energy (multi-phonon) excitations and can mispredict phonon bottleneck effects. This invalidates the quantitative comparison between relaxation and MEG rates and thus the claimed ‘relative phonon bottleneck’."
      },
      {
        "Problem": "Unrealistic cluster models without surface passivation",
        "Location": "Sec. III (p. 11)",
        "Explanation": "The Si₂₆ and Si₄₆ clusters are bare (unpassivated) silicon networks with numerous dangling bonds, yet no hydrogenation or ligand termination is included. Surface states in such unpassivated clusters dominate the electronic structure and relaxation dynamics, rendering GW/QP energies, exciton binding, and MEG rate predictions for these models inapplicable to real SCNCs used in photovoltaics."
      },
      {
        "Problem": "Oversimplified cascade assumption for MEG yield based on single-state rate ratios",
        "Location": "Sec. IV A–B (pp. 14–19)",
        "Explanation": "The study infers MEG efficiency solely from per-state ratios γ^{e–e}/γ^{e–p} or Γ^{e–e}/Γ^{e–p}, assuming that whenever inelastic scattering exceeds phonon relaxation the MEG cascade proceeds with unit yield. It neglects the full population kinetics, energy redistribution, and competing multi-exciton annihilation channels. Without solving the coupled rate equations, the conclusion on cluster-size effects on overall MEG is unjustified."
      },
      {
        "Problem": "Reliance on perturbative G₀W₀@LDA without self-consistency or vertex corrections",
        "Location": "Sec. II A (Eqs. 1–4, p. 4)",
        "Explanation": "Quasiparticle energies and lifetimes are obtained via a single-shot G₀W₀ calculation starting from LDA Kohn–Sham orbitals, with no self-consistency or vertex corrections. In small clusters with strong confinement and surface effects, such an approach can yield large errors (>0.5 eV) in excitation energies and mischaracterize inelastic scattering thresholds, critically affecting the predicted energy-dependent MEG behavior."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 26815,
      "thinking": 832,
      "output": 812
    }
  },
  {
    "entry_id": 109,
    "retraction_id": "1705.01127v2",
    "paper_id": "1705.01127v1",
    "retraction_comment": "Paper has been withdrawn since we find that dust effects have an unignorable impact on our analyses. A part of analyses are reported by arXiv:1809.03715",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Severe sample incompleteness and FIR selection bias",
        "Location": "Page 2, §2 Data (Fig. 1 completeness discussion)",
        "Explanation": "The AKARI cross-matched sample is only 2.6% complete at SFR_Hα,corr > 1 M⊙/yr and 24% at > 10 M⊙/yr, preferentially selecting FIR-bright systems and likely missing galaxies with extreme Hα/UV ratios. This biased sampling can artificially strengthen or suppress the observed correlation between Hα/UV and main-sequence offset, invalidating the generality of the conclusions."
      },
      {
        "Problem": "Potential spurious correlation from shared variable",
        "Location": "Page 3, §3 Results (∆MS vs. Hα/UV correlation)",
        "Explanation": "Both ∆MS (defined from SFR_Hα,corr) and the Hα/UV ratio involve the same SFR_Hα,corr measurement, introducing a common variable that can induce a spurious correlation. The brief reference to a Jenkins et al. test is insufficient to account for the full covariance, so the reported strong rank correlation may be an artifact of this overlap."
      },
      {
        "Problem": "Invalid SFR calibrations for rapidly varying SFHs",
        "Location": "Pages 1–2, §1 Introduction & §2 Data (use of Kennicutt 1998 calibrations)",
        "Explanation": "The Kennicutt (1998) relations for converting Hα and UV+IR luminosities to SFR assume continuous star formation over ≳100 Myr and solar metallicity. Applying these calibrations to galaxies with bursty or rapidly quenching SFHs (timescales ∼10–100 Myr) and to low-mass systems with stochastic star formation leads to systematic misestimation of SFRs and hence of the Hα/UV ratio, undermining the key diagnostic of recent SFH variability."
      },
      {
        "Problem": "Unreliable aperture corrections for SDSS Hα measurements",
        "Location": "Page 2, §2 Data (SDSS fibre correction discussion)",
        "Explanation": "Total SFR_Hα,corr is obtained by applying uncertain aperture corrections to fibre-based measurements. Negative corrections are simply discarded, but large positive corrections remain unquantified. Without a rigorous error analysis of these corrections, the total Hα SFRs—and thus the derived Hα/UV ratios—are not reliably measured."
      },
      {
        "Problem": "Inconsistent main-sequence baselines when comparing tracers",
        "Location": "Page 3, §3 Results & Figure 2",
        "Explanation": "∆MS is defined relative to a main sequence fitted using SFR_Hα,corr, yet the analysis also compares against the UV+IR main sequence, which has a different slope and normalization. Failing to adopt a single, consistent baseline for residuals can introduce artificial trends between ∆MS and the Hα/UV ratio."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 12575,
      "thinking": 1856,
      "output": 680
    }
  },
  {
    "entry_id": 110,
    "retraction_id": "2105.03304v2",
    "paper_id": "2105.03304v1",
    "retraction_comment": "Eq (2.4) is not correct and as such this invalidates Theorem 2.3 and consequently all the claimed results on the modulus of the zeros of chromatic polynomial. As fas as we can tell the results for the edge based block polynomials are correct (this concerns Sections 4 and 5). We will probably resubmit this part as part of a new paper at some point in the future",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "False block‐decomposition of even subgraphs",
        "Location": "Lemma 5.2",
        "Explanation": "The authors claim “F ⊆ E is even iff each of its blocks is even,” and so model the even‐set generating function via an edge–block polynomial.  But one can have an Eulerian (even) subgraph F whose block decomposition contains two blocks each with an odd‐degree cut‐vertex (so each block is not even) yet the overall degrees sum to even.  Such an F is counted in E_G(z) but is killed by the proposed B*(G;w).  Hence E_G(z)≠B*(G;w) in general."
      },
      {
        "Problem": "Invalid Fisher‐zeros bound",
        "Location": "Theorem 1.3 / Section 5",
        "Explanation": "The zero‐free disk for Z_G(b) is deduced from the (flawed) representation of E_G(z) as a block polynomial (Lemma 5.2) together with the high‐temperature identity (Lemma 5.1).  Since Lemma 5.2 is false, the entire argument establishing Theorem 1.3 collapses and the claimed Fisher‐zero bound is unsupported."
      },
      {
        "Problem": "Potential misstatement of the high‐temperature identity",
        "Location": "Lemma 5.1",
        "Explanation": "The relation E_G(z) = (1+z)^{|E|}/2^{|V|}·Z_G((1−z)/(1+z)) is quoted without proof and omits several standard normalization factors (e.g.\\ powers of 2 or cosh βJ).  As stated, it may fail on simple examples unless additional constraints on b or z are imposed, so the map from zero‐free regions of E_G to those of Z_G is questionable."
      },
      {
        "Problem": "Unproven edge‐based polymer zero‐freeness",
        "Location": "Theorem 4.2 (Section 4)",
        "Explanation": "Theorem 4.2 extends the vertex‐based cluster‐expansion zero‐free condition to an “edge‐based block polynomial” without supplying a proof.  Key combinatorial decompositions (analogues of Lemma 2.5) are merely sketched.  Without a detailed verification that the required inclusion–exclusion and inductive arguments carry over, the zero‐freeness claim for B*(G;w) remains unsubstantiated."
      },
      {
        "Problem": "Over‐reliance on Penrose’s lemma with non‐distinct weights",
        "Location": "Lemma 3.2 / Corollary 3.3",
        "Explanation": "The Penrose tree identity is invoked to bound alternating sums over all connected subgraphs by a count of spanning trees.  However, the classical proof requires assigning strictly distinct weights to edges to avoid cancellation ambiguities.  Specializing immediately to uniform weights without addressing this subtlety may invalidate the claimed inequality and hence the ensuing tree‐sum bounds in Corollary 3.3."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 14491,
      "thinking": 14720,
      "output": 698
    }
  },
  {
    "entry_id": 111,
    "retraction_id": "1608.03378v2",
    "paper_id": "1608.03378v1",
    "retraction_comment": "This paper has been withdrawn by the author due to an error in equation 5",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Use of intrinsic semiconductor model to extract band gap without accounting for extrinsic carriers or Mott–Peierls physics",
        "Location": "Section III, Eq. 14–15 and Fig. 8 (page 5)",
        "Explanation": "The authors apply the standard intrinsic carrier concentration formula (ni∝T3/2e–Eg/2kBT) to VO2’s semiconducting phase. VO2 is not an ideal intrinsic semiconductor—its carrier density in thin films is strongly influenced by defects, oxygen vacancies, substrate strain, and the Mott–Peierls transition mechanism. Treating n(T) as purely intrinsic neglects these extrinsic contributions and the strongly correlated nature of the gap, invalidating the extracted Eg≈0.40 eV."
      },
      {
        "Problem": "Misapplication of the composite cube model and unphysical mobility ratio",
        "Location": "Section III, Eq. 1–10 and discussion (page 4)",
        "Explanation": "In deriving the untransitional phase fraction β, the authors identify μ2/μ1≈200 from measured mobility data, implying that the intergranular (barrier) phase has a mobility 200× higher than the VO2 grains. Physically, grain boundaries are more resistive and should have much lower mobility. This sign reversal and unphysical parameter assignment undermine the use of the model and the conclusion that a 1/4‐grain‐length untransitional phase exists."
      },
      {
        "Problem": "Lack of Hall‐measurement protocol to eliminate systematic offsets",
        "Location": "Section II (Experimental details)",
        "Explanation": "The manuscript does not describe reversing the magnetic field or current direction to cancel misalignment, thermoelectric, and contact‐offset voltages. Especially at low temperatures where the Hall signal is small, these offsets can dominate, rendering the extracted carrier concentration and mobility values unreliable."
      },
      {
        "Problem": "Neglect of phase coexistence and percolation effects near the MIT",
        "Location": "Section III (discussion of resistivity and n(T))",
        "Explanation": "Across the VO2 metal–insulator transition, metallic and insulating domains coexist, and conduction proceeds via a percolative network. Treating the film as a single homogeneous phase and attributing the resistivity drop solely to increased carrier density ignores changes in connectivity and domain morphology, invalidating conclusions about the driving mechanism being purely carrier‐density‐controlled."
      },
      {
        "Problem": "Uncertainty in film thickness and geometry corrections for van der Pauw measurements",
        "Location": "Section II (XRR thickness, page 2) and throughout Section III",
        "Explanation": "Carrier concentration is inversely proportional to film thickness, but the reported 422 Å thickness has a ±55 Å roughness and may not represent the electrically active layer uniformly. Additionally, no van der Pauw geometric correction factor or contact‐placement analysis is given. These uncertainties can introduce order‐of‐magnitude errors in n and μ, undermining all quantitative conclusions."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 10898,
      "thinking": 2112,
      "output": 689
    }
  },
  {
    "entry_id": 112,
    "retraction_id": "1709.00434v2",
    "paper_id": "1709.00434v1",
    "retraction_comment": "Some of the content, including the assumption of vanishing vector potential at the interface and the application of the formalism to the gapped Dirac materials, are wrong",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Incorrect normalization of the approximate Coulomb kernel L0(y) in Eq. (41)",
        "Location": "Eq. (41) and subsequent moment checks (Eqs. 42–44)",
        "Explanation": "The paper defines L0(y)=½√2 e^{−√2 q|y|}, which integrates to 1/q rather than the claimed 1/(2q). This normalization error propagates into the Green’s‐function operator Ô₀ (Eq. 45) and the integro‐differential reduction (Eq. 47), leading to an overall factor‐of‐2 mistake in the derived dispersion law."
      },
      {
        "Problem": "Unjustified vanishing of the vector potential at the interface",
        "Location": "Assumption in Eq. (18) used in boundary condition Eq. (17)",
        "Explanation": "Setting A( x,y,z=0,t)=0 for all components is physically unwarranted in the presence of dynamic plasmonic fields and an external magnetic field. Neglecting A breaks self‐consistency of the Coulomb gauge and disregards induced magnetic fields, thus invalidating the boundary conditions and the ensuing dispersion relation."
      },
      {
        "Problem": "Over‐simplified edge conductivity model neglects edge‐state and nonlocal effects",
        "Location": "Approximation in Eq. (1)",
        "Explanation": "Treating the conductivity tensor near the edge as a Heaviside step of the bulk σ↔(q,ω) ignores the known modification of graphene’s conductivity by edge states, finite‐width screening, and nonlocal response. Such edge effects are crucial for Dirac materials under magnetic bias and can qualitatively alter the plasmon dispersion."
      },
      {
        "Problem": "Misidentification and incorrect integration of singular charge components at y=0",
        "Location": "Decomposition in Eqs. (33–35) and boundary integral in Eq. (61)",
        "Explanation": "The paper splits ρ̄ into δD and Θ terms but then treats the 2D‐bulk part ρ̄₂ᴰ as contributing a finite δ‐term upon integrating across y=0 (Eq. 61). In fact, Θ[−y] f(y) has no true δ‐component, so the derivation of the derivative jump condition (Eq. 59) is flawed, leading to erroneous matching conditions for Φ and its derivative."
      },
      {
        "Problem": "Questionable applicability of the long-wavelength limits for Dirac materials with valley polarization",
        "Location": "Final simplifications in Eqs. (66–69) and dispersion in Eq. (68)",
        "Explanation": "The conditions η=η̄ and χ+χ̄=0 are taken from 2DEG magnetoplasmons but need not hold in graphene with unequal valley populations or nonlocal Hall response. Relying on these limits risks invalidating the closed-form edge-magnetoplasmon dispersion (Eq. 68) for realistic Dirac materials."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 6088,
      "thinking": 3200,
      "output": 699
    }
  },
  {
    "entry_id": 113,
    "retraction_id": "1610.01669v7",
    "paper_id": "1610.01669v6",
    "retraction_comment": "This paper has been withdrawn by the author due to a crucial error on linear implication between games",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Lack of enforcement of Opponent ‘invisibility’ of initial strategy‐selection tags breaks parametricity and dependent abstraction",
        "Location": "Definition 3.4.1 (Predicative games)",
        "Explanation": "The semantics relies on an initial ‘protocol’ qG.σ that is supposed to be invisible to Opponent, so that Player’s choice of component game or strategy cannot be observed. However, anti-strategies are not restricted at all, so Opponent can inspect these tags and adapt its play accordingly. This undermines the intended abstraction in dependent games and dependent function spaces, since Opponent can distinguish and exploit which σ was chosen, breaking parametricity and potentially invalidating key lemmas about substitution and closure under constructions."
      },
      {
        "Problem": "Use of construction numbers to name games in universe moves is not shown to preserve game structure",
        "Location": "Definition 5.1.1 (3), Corollary 5.2.1 (Effectivity and bijectivity)",
        "Explanation": "To render games ‘effective’, the paper replaces each game or strategy name G with its construction number ♯(G). But there is no proof that this renaming preserves the enabling relation, labeling function, rank assignments, or legality of positions in the induced universe games. Without this, the Tarski-style U-Form, U-Intro, U-Elim and U-Comp rules for universes may fail, since universe games crucially depend on matching names and ranks."
      },
      {
        "Problem": "Full completeness and bijectivity only hold in the ‘elementary’ submodel EPG, not in the full semantic domain",
        "Location": "Section 5.2, Corollary 5.2.1",
        "Explanation": "Surjectivity and injectivity are proved only for the inductively generated fragment EPG (the ‘elementary’ games and strategies), which by construction coincides with the image of the syntax. This does not establish full completeness for the larger semantic universe WPG, so the claim of bijectivity should be qualified: it holds only for the syntactic submodel EPG, not for arbitrary predicative games or strategies."
      },
      {
        "Problem": "Crucial substitution lemmas for dependent function and pair types gloss over nontrivial renaming of threads and tags",
        "Location": "Section 4.6.1 (Q-Subst) and 4.6.2 (P-Subst)",
        "Explanation": "The proofs of the Q-Subst and P-Subst equations rely on rewriting chains of compositions involving φ+, φ++, and various projections, but only provide informal sketches. They assume that the renaming of tags for disjoint unions and the alignment of justifiers commute with the dependent constructions. A gap in this justification threatens the soundness of substitution for Q- and P-types, which is critical for interpreting MLTT’s typing rules."
      },
      {
        "Problem": "Definition of natural-number eliminator RN P uses an unproved least upper bound in the strategy ordering",
        "Location": "Section 4.6.4 (Definition of RN P in Proposition 4.6.8)",
        "Explanation": "RN P(cz,cs) is defined as the sup of the chain RN P(cz,cs)₀ ⊑ RN P(cz,cs)₁ ⊑ … of noetherian strategies. However, the paper does not prove that the set of strategies on a game is a directed complete partial order nor that the supremum of a chain of noetherian strategies yields a well-defined, innocent, well-bracketed, total and noetherian strategy. Without such a completeness result, the recursive definition of RN P is unsound and N-elimination may fail."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 66653,
      "thinking": 2560,
      "output": 849
    }
  },
  {
    "entry_id": 114,
    "retraction_id": "1605.05588v2",
    "paper_id": "1605.05588v1",
    "retraction_comment": "It had to be noted that the assumption was made that all sensors have access to all observations and state estimate vectors. In addition, the summations in the DAQKF Algorithm are on all sensors, not just the neighbouring sensors",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Incorrect model update indexing in the distributed filter",
        "Location": "Algorithm 3, Model update (page 3)",
        "Explanation": "The prediction step reads xˆ_{l,n|n−1} = A_n xˆ_{l,n|n}, but it should propagate from the previous posterior estimate xˆ_{l,n−1|n−1}. Using the current a posteriori estimate instead of the last one breaks the temporal recursion and renders the filter invalid."
      },
      {
        "Problem": "Lack of consensus mechanism or weighting in update summation",
        "Location": "Section III, equations (3)–(5) and Algorithm 3 (page 3)",
        "Explanation": "The algorithm sums local update vectors Δxˆ_{m,n} over neighbors without any averaging or consensus weights. This can lead to divergence or double-counting of information and provides no guarantee that all nodes converge to the centralized solution."
      },
      {
        "Problem": "Unsound fault-detection confidence measure",
        "Location": "Section III, equation (7) and following text (page 4)",
        "Explanation": "The derivation of r_a_{l,m} assumes equal covariances and unbiased estimates at convergence across nodes, which is unrealistic in practice. No thresholding strategy or statistical test is provided, so the proposed metric cannot reliably isolate faulty sensors."
      },
      {
        "Problem": "Physically invalid measurement model using accelerometers",
        "Location": "Section IV, first paragraph (page 4)",
        "Explanation": "Accelerometers measure linear acceleration, not Euler angles. Treating accelerometer outputs as direct roll, pitch, and yaw measurements violates sensor physics and invalidates the observation model used by the filter."
      },
      {
        "Problem": "Linear state evolution on axis‐angle ignores quaternion manifold",
        "Location": "Section IV, definition of state evolution x_n = [κ; ∂κ/∂t]ᵀ (page 4)",
        "Explanation": "Rotational kinematics on SO(3) are inherently nonlinear and constrained (unit norm). Modeling the axis‐angle κ with a linear additive noise model (x_n = A x_{n−1} + ν) is only valid for infinitesimal rotations. For general 3D rotations, this linearization leads to biased and inconsistent estimates."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 8998,
      "thinking": 1856,
      "output": 542
    }
  },
  {
    "entry_id": 115,
    "retraction_id": "2112.14102v2",
    "paper_id": "2112.14102v1",
    "retraction_comment": "The paper was withdrawn due to a mistake in the proof of Theorem 15, in Section 4. The proposed translation is not equirealizable (the cases for disjunction and Release fail)",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Flawed base-case translation in Theorem 15",
        "Location": "Page 11, proof of Theorem 15, base case for ‘ϕ = p’",
        "Explanation": "The reduction translates an environment-only atomic formula p (which is unrealizable) into a GX₀ specification α∧■ψ that is in fact realizable by the system (e.g., by setting the auxiliary s to 1 initially and then 0 forever). Thus the translation fails to preserve realizability and the inductive proof of equirealizability in Theorem 15 is unsound."
      },
      {
        "Problem": "Incorrect resetting construction in Lemma 13",
        "Location": "Page 9, Lemma 13",
        "Explanation": "The formula uses the bounded operator ⬜[0,d]r (defined as a disjunction of Next^i r) to detect a d-step run of r=1, but a disjunction cannot enforce r=1 at every step of a length-d interval. The intended ‘reset’ test requires a conjunction over those positions, so the given construction does not correctly simulate the initial-condition removal and equirealizability fails."
      },
      {
        "Problem": "Omitted blow-up analysis in inductive translation",
        "Location": "Page 11, proof of Theorem 15",
        "Explanation": "The inductive translation from arbitrary Safety LTL formulas to GX₀ introduces new auxiliary atoms and nested Next operators at each step. No bound is given on the growth of temporal depth or formula size, and repeated nesting of Release or Globally operators can lead to an exponential blow-up, invalidating the claim of an efficient reduction."
      },
      {
        "Problem": "Insufficient detail in the alternating–Turing–machine argument for GX₀ ∈ EXP",
        "Location": "Page 6, Proposition 10",
        "Explanation": "The short argument sketch for membership in APSPACE omits handling of the universal quantification over all environment strategies and the acceptance condition for safety games. Without a formal construction, it is unclear that the proposed ATM correctly decides unrealizability in polynomial space."
      }
    ],
    "think_summary": [
      "**Analyzing paper for critical errors**\n\nThe user has asked me to examine the attached paper for serious errors and unsoundness issues that might invalidate its conclusions. I'll prioritize identifying the most critical problems while ignoring minor ones. The focus points include evaluating claims regarding the EXP-completeness of safety LTL realizability and related technical aspects. I’ll pay special attention to Theorem 9 and Proposition 10, ensuring to check for flawed proofs or incorrect complexity arguments. Let’s dive into the details!",
      "**Reviewing acceptance procedures**\n\nThe user is asking me to analyze some technical details regarding the acceptance procedures in a paper about alternating Turing machines (ATMs) and their implications. They mention the ATM needs to accept when a formula is unrealizable and that this relates to APSPACE and EXP complexities. \n\nThey also highlight Proposition 11, discussing the reduction from APSPACE to GX0-Real. I need to ensure that the variables and formulae align correctly for reliability. There are potential errors in how the temporal formula is stated, especially regarding the use of Next operators. I'll closely examine these factors for clarity and correctness.",
      "**Identifying critical problems**\n\nI'm analyzing issues in the paper, and it looks like there are several critical problems to address. First, the translation in the base case of Theorem 15 is wrong, which indicates a flawed proof. There's also a faulty Lemma 13, and the handling of bounded operators is incorrect. Additionally, they conflate equirealizability with logical expressiveness, which is more conceptual than an outright error. Lastly, a potential exponential blow-up due to auxiliary variables isn't analyzed properly. I'll outline these problems in JSON format with specific page references."
    ],
    "token_usage": {
      "input": 14349,
      "thinking": 2816,
      "output": 493
    }
  },
  {
    "entry_id": 116,
    "retraction_id": "1606.07464v6",
    "paper_id": "1606.07464v5",
    "retraction_comment": "The arguments in the Sections 3.2 and 3.3 are not conclusive, and the Markov property is not disproved. Many other statements are though correct, see arXiv:2011.11476v4 (\"Revisiting the stochastic differential equations with multiplicative noise\")",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Misuse of small-time Gaussian propagator to disprove Chapman–Kolmogorov equation",
        "Location": "Section III.2, Equations (3.4)–(3.7)",
        "Explanation": "The author treats the O(τ) short-time expansion (Eq. 3.4) of the transition density as if it were exact, then shows it fails to satisfy the Chapman–Kolmogorov equation (Eq. 3.7). In reality that Gaussian approximation holds only up to o(τ) and cannot be convolved over finite intervals. The apparent violation of Chapman–Kolmogorov is thus an artifact of misusing an asymptotic approximation rather than a true non-Markovian behavior."
      },
      {
        "Problem": "Claim that SDEs with multiplicative noise are not Markovian",
        "Location": "Introduction and Section III.2",
        "Explanation": "It is a standard result in stochastic analysis that solutions of SDEs (with Lipschitz coefficients) under any α-interpretation define (strong) Markov processes. The paper’s conclusion of non-Markovianity arises solely from the flawed use of approximate propagators, not from any intrinsic failure of the exact process to satisfy the Chapman–Kolmogorov property."
      },
      {
        "Problem": "Assertion that only the anti-Itô discretization recovers an approximate Markov property",
        "Location": "Section III.3, Equation (3.8)",
        "Explanation": "The author argues that taking α=1 (anti-Itô) is required for a (quasi-)Markovian description on a coarse-grained time scale. In fact, the continuous-time SDE is Markovian for all α once the correct spurious drift is included. The choice of discretization affects numerical schemes but does not alter the Markov property of the underlying process."
      },
      {
        "Problem": "Steady-state criterion wrongly requires detailed balance (∇·(D∇φ)=0)",
        "Location": "Section VI, Equation (5.9) and subsequent discussion",
        "Explanation": "The paper asserts that a globally quiescent steady state exists only when ∇·(D∇φ)=0 (detailed balance), implying that non-equilibrium stationary states with circulating probability currents cannot be steady. This contradicts well-known examples of Fokker–Planck equations admitting stationary distributions with nonzero currents (e.g. in systems with sustained cycles); detailed balance is sufficient but not necessary for stationarity."
      },
      {
        "Problem": "Derivation of the 'most likely path' neglects fluctuation-induced skew and multi-modality",
        "Location": "Section V.1, Equations (5.1)–(5.3)",
        "Explanation": "The identification of the most likely path with the solution of the deterministic ODE x˙=a(x) relies on a local Gaussian approximation around a single peak. In the presence of multiplicative noise, the mode of the distribution can shift or split due to skewness and non-Gaussian features, so the global 'most likely' trajectory need not coincide with the noiseless flow, invalidating the generality of the analytic formula."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 6478,
      "thinking": 1728,
      "output": 741
    }
  },
  {
    "entry_id": 117,
    "retraction_id": "1806.07478v2",
    "paper_id": "1806.07478v1",
    "retraction_comment": "The results fail to capture the non-simultaneously diagonalizable case",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Flawed perturbation argument in Lemma 8 and Theorem 9: the ‘perturbation’ δN is not small as δ→0, so δR does not vanish",
        "Location": "Section 3, Lemma 8 and Theorem 9",
        "Explanation": "The key step is to write P_S^{-1} N P_S=U_N+δN and argue that adding h⋅δN is an O(δ) perturbation as δ→0. In fact δN=P_S^{-1} N P_S−P_N^{-1} N P_N is independent of the stiffness parameter δ, so h δN does not tend to zero with δ. Hence the claimed bound δR(h U_N,h δ^{-1} U_S)→0 as δ→0 is false, invalidating the eigenvalue approximation that underpins Theorem 9."
      },
      {
        "Problem": "Unjustified assumption V^2=V in Theorem 7 restricts general linear methods without acknowledgment",
        "Location": "Section 3, Theorem 7",
        "Explanation": "Theorem 7’s first part uses the identity V^2=V to simplify δR, but no justification is given that V (the external-stage propagation matrix of a general linear method) satisfies V^2=V in any nontrivial case. Standard GLMs (beyond simple one-step Runge–Kutta) do not have idempotent V, so Theorem 7 as stated applies only to a very narrow subclass, contrary to the claimed generality."
      },
      {
        "Problem": "Misapplication of Bauer–Fike perturbation theorem for non-diagonalizable matrices",
        "Location": "Section 3, proof of Theorem 9",
        "Explanation": "The proof invokes a variant of Bauer–Fike (Theorem 7.2.3 in Golub–Van Loan) to bound eigenvalue shifts of a non-normal, possibly non-diagonalizable matrix. Classic Bauer–Fike requires diagonalizability; the extension to general Schur form involves pseudospectrum estimates not used here. The stated bound |ϕ_i−ψ_i|≤max{θ,θ^{1/q}} is not justified, so the eigenvalue perturbation result is unsound."
      },
      {
        "Problem": "No guarantee that test-equation error E(h)→0 in stiff limit when N and S are not simultaneously triangularizable",
        "Location": "Section 3, discussion after Theorem 9 and Example 12",
        "Explanation": "The authors assert that as the implicitly treated term becomes arbitrarily stiff, the maximal eigenvalue error E(h) between the full stability matrix and the scalar test equations can be made arbitrarily small. But in the non-ST case the change of basis perturbation δN does not vanish with the stiffness parameter, so E(h) need not tend to zero. Example 12 only shows failure for non-stiff eigenvalues of S, not the stiff limit, and there is no proof of uniform convergence."
      },
      {
        "Problem": "Stability criterion based solely on eigenvalue modulus ignores nonnormal transient growth",
        "Location": "Section 2, Definitions 1–2 and throughout stability analysis",
        "Explanation": "The paper equates power boundedness with all eigenvalues inside the unit disk, but power boundedness also fails if there are nontrivial Jordan blocks or strong nonnormality even when |eig|≤1. No analysis of transient growth is provided, yet conclusions about allowable time-steps rest entirely on spectral radius, potentially overlooking destabilizing nonnormal effects."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 34082,
      "thinking": 2880,
      "output": 824
    }
  },
  {
    "entry_id": 118,
    "retraction_id": "2302.13052v2",
    "paper_id": "2302.13052v1",
    "retraction_comment": "There are some errors and inappropriate writings. Thm 4.16 and don't work well for additive invariants",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Existence of the initial localizing invariant Kcont on Pr^dual,κ is assumed but not constructed",
        "Location": "Definition 1.2 (page 1) and Section 3",
        "Explanation": "The paper defines Efimov K-theory K_cont as the initial localizing invariant on the ∞-category Pr^dual,κ under Σ^∞∘Cr, but never constructs the Bousfield localization or otherwise shows that the ∞-category of localizing invariants Fun^lax_loc(Pr^dual,κ,Sp) is presentable or admits the required colimits. Without verifying that this ∞-category has an initial object, the very existence of K_cont (and hence all subsequent results dependent on its universal property) remains unjustified."
      },
      {
        "Problem": "Implicit assumption that every dualizable presentable stable ∞-category is κ-dualizable",
        "Location": "Definition 3.23 (page 17)",
        "Explanation": "The construction of the κ-Calkin category Calk_κ(C) and the continuous extension E_cont(C) relies on choosing a regular cardinal κ for which C is 'κ-dualizable,' i.e. the colimit functor Ind(C^κ)→C admits a fully faithful left adjoint. The paper asserts that for any dualizable presentable stable ∞-category such a κ exists, but offers no proof. The dualizability criteria from [Lur18] guarantee only that C is a retract of some compactly generated category, not that the required left adjoint into Ind(C^κ) exists. If no such κ can be found, Calk_κ(C) is undefined and the definition of E_cont breaks down."
      },
      {
        "Problem": "Left Kan extension along Pr^dual,κ→Pr^dual_st may fail to preserve the localizing (or additive) property",
        "Location": "Definition 3.15 (page 14) and Lemma 3.38 (page 19)",
        "Explanation": "Efimov K-theory and general continuous extensions E_cont are obtained by left Kan extending a localizing (resp. additive) invariant from the small ∞-category Pr^dual,κ to the full ∞-category Pr^dual_st. However, left Kan extension does not in general preserve the property of sending exact (resp. split-exact) sequences to fiber sequences. The brief argument in Lemma 3.38 conflates pointwise exactness on Pr^dual,κ with global preservation of exact sequences on Pr^dual_st, without checking that the Kan extension along a non-fully faithful inclusion remains localizing. This gap puts in doubt whether E_cont actually satisfies the defining invariance properties."
      },
      {
        "Problem": "Use of the Eilenberg swindle on Ind(C^κ) and Calk_κ(C) without verifying existence of countable colimits",
        "Location": "Lemma 3.11 (page 16) and Lemma 3.29 (page 18)",
        "Explanation": "Several vanishing results (e.g. K_cn(Ind(C)^κ)≃0 and E(Calk_κ(Ind(C^λ))^ω)≃0) are proved via the Eilenberg swindle, which requires that the relevant ∞-categories admit countable coproducts. The paper asserts that Ind(C^κ) (and its subcategory Calk_κ) has countable colimits simply because it admits κ-filtered colimits, but does not check that κ-filtered colimits include the required ω-filtered ones or that these categories are presentable. If they lack genuine countable coproducts, the swindle arguments fail and the key κ-independence and vanishing lemmas may be false."
      },
      {
        "Problem": "Assertion that coproducts and products agree in Pr^L relies on undefined LFun/RFun equivalences",
        "Location": "Proposition 2.46 (page 8)",
        "Explanation": "The paper claims that arbitrary coproducts and products in Pr^L coincide because mapping into any D yields LFun(⨿_i C_i,D)≃∏_i LFun(C_i,D). This argument invokes 'LFun' and 'RFun' (left- and right-adjoint-preserving functors) without defining them or showing that they faithfully model colimits and limits in Pr^L. Without a clear identification of these mapping subcategories and a proof that Pr^L's limits and colimits are computed pointwise in this manner, the semi-additive structure of Pr^dual_st (and all downstream results depending on finite sums) is left unsubstantiated."
      }
    ],
    "think_summary": [
      "**Evaluating mathematical proofs**\n\nI see a few potential inaccuracies in the proofs. For instance, in Lemmas 6.4 and 6.5, there's an accidental reference to Lemma 6.4 in 6.5, but that doesn’t impact the proof of Lemma 6.6. Also, while they assert that the Milnor square induces a Cartesian square on Kcont, they leave out some details regarding the excision theorem for infinite categories based on Lemma 7.11 and Corollary 7.12, which might be okay. However, a thorough verification of Kcont and Kcn_cont's product preservation appears to be lacking.",
      "**Identifying critical problems in proofs**\n\nI'm noticing some key issues in the arguments presented. Firstly, there's the nonconstructive claim regarding the existence of an initial localizing invariant for Prdual,κ; it’s asserted but lacks proof. The foundational role of Efimov K-theory isn't solidified without verifying the necessary conditions.\n\nNext, there's an implicit assumption that every dualizable presentable stable ∞-category is κ-dualizable, which isn’t substantiated. Finally, the argument overlooks that the left Kan extension of a localizing invariant might fail to preserve its properties, particularly concerning exact sequences.",
      "**Identifying critical proof errors**\n\nI'm noting some significant assumptions and gaps in the arguments, which could impact Theorem 7.10. For example, they assume base change in original categories leads to base change after Ind and compacts, but they don’t justify it. Here are the five critical problems: 1) The existence and universal property of Efimov K-theory on Prdual,κ are not established. 2) There's an implicit assumption about the Ind functor requiring a left adjoint. 3) They don’t prove that left Kan extension preserves exact sequences, risking Econt's localizing invariance. 4) They use the Eilenberg swindle in a way that might overlook the existence of countable colimits. 5) The equivalence of products and coproducts in Proposition 2.46 relies on undefined functors, compromising the framework of Prdual_st."
    ],
    "token_usage": {
      "input": 42376,
      "thinking": 5440,
      "output": 1044
    }
  },
  {
    "entry_id": 119,
    "retraction_id": "1511.07171v2",
    "paper_id": "1511.07171v1",
    "retraction_comment": "Error in equation 9",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Incorrect jump coefficients in the boundary integral representation",
        "Location": "Page 4, equation (9) and the paragraph immediately following",
        "Explanation": "The classical result for a smooth boundary is that the coefficient c(m) in the single-layer formulation equals 1 inside the domain, 0 outside, and 1/2 on the boundary.  The paper instead assigns c⁺(m)=1 for the exterior domain and c⁻(m)=1 for the interior, reversing the exterior value and failing to set it to zero.  This mis-assignment invalidates the integral equation and any numerical solution based on it."
      },
      {
        "Problem": "Unjustified neglect of the far-field integral and radiation condition",
        "Location": "Page 4, paragraph following equation (8)",
        "Explanation": "The author asserts that the integral over the infinite generator Γ_∞ vanishes under the proposed \"convected radiation conditions\" without a rigorous proof.  For a subsonic uniform flow, the scattered field does not in general satisfy the same decay properties as in the no-flow case, and omitting this term without verifying the directional radiation behavior of the convected Green’s function leads to an unsound formulation."
      },
      {
        "Problem": "Ill-posed definition and use of the “particular normal derivative” operator",
        "Location": "Page 3, equations (5) and (6)",
        "Explanation": "The paper introduces a new derivative operator d/dn_q defined by combining ∂/∂n_q and M∞·∇ without deriving its mapping properties or showing that it preserves the self-adjointness and singular behavior of the original boundary operators.  This ad-hoc manipulation may omit essential convective terms and compromises the correctness of the integral operators."
      },
      {
        "Problem": "Validation only on an interior Neumann duct problem, not on the intended exterior radiation problem",
        "Location": "Section 3 (Axisymmetric cylindrical duct), pages 5–7",
        "Explanation": "Although the formulation is presented for exterior acoustic radiation in an unbounded domain, the numerical tests are confined to an interior rigid-walled duct with Neumann boundary conditions.  No computation of the true radiation pattern or far-field behavior is provided, so the method’s ability to handle the unbounded exterior problem remains unverified."
      },
      {
        "Problem": "Use of simple trapezoidal quadrature for singular and near-singular integrals",
        "Location": "Section 3, numerical implementation paragraph",
        "Explanation": "The paper states that all boundary integrals— including those with the 1/R singularity—are evaluated by a high-order trapezoidal rule.  Standard theory requires specialized singularity extraction or adaptive quadrature for accurate integration near collocation points.  Using a naive trapezoidal rule likely introduces large discretization errors, invalidating the reported accuracy."
      }
    ],
    "think_summary": [
      "**Evaluating boundary conditions and limitations**\n\nI'm noticing that the analysis misses hypersingular integrals when using Neumann boundary conditions, which could mean it's incomplete. Also, concerning the radiation condition, they assume the integral at infinity is zero for convected radiation but don’t provide adequate proof. This seems risky since the Green's function decays differently in various directions. Lastly, their numerical validation only focuses on an interior duct problem, not testing for exterior radiation, leaving the conclusion unsupported.",
      "**Identifying critical errors**\n\nI’m seeing some crucial missteps in the analysis, leading to incorrect conclusions about its general applicability. For one, they use the trapezoid rule for singular integrals, which won’t integrate near singularities accurately. Instead, they should have employed singularity extraction. I need to highlight five key issues: the misassignment of boundary integral coefficients, lack of justification for neglecting far-field integrals, unrigorous definitions for normal derivatives, and inadequate numerical validation focused only on interior problems rather than exterior scenarios. Also, their trapezoid method is inappropriate for singular integrals."
    ],
    "token_usage": {
      "input": 9509,
      "thinking": 1984,
      "output": 655
    }
  },
  {
    "entry_id": 120,
    "retraction_id": "1106.1331v2",
    "paper_id": "1106.1331v1",
    "retraction_comment": "Withdrawn by the authors. Lemma 7.6 is false as stated, and Appendix B is flawed. Corrected and reorganized versions of the material will be posted in papers with different titles",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Lemma 11.5 (Uniform L^p–L^∞ bound for generalized matrix coefficients) lacks a fully rigorous justification of uniformity in π",
        "Location": "Section 11.1 (Lemma 11.5) and Appendix C",
        "Explanation": "The proof sketches how L^p-boundedness of a family Λ⊂Ĝ^K implies km_{v,η}‖_p ≲‖m_{v,η}‖_∞ uniformly in π∈Λ, by appealing to Harish–Chandra expansions and recurrence relations. However, the argument does not establish that the constants in these estimates remain uniform near singular or non‐generic parameters, nor does it fully control contributions along walls of the Weyl chamber. This uniform bound is crucial for the error‐term analysis in Section 11, yet its proof is only outlined."
      },
      {
        "Problem": "Existence of polar decomposition in Lemma 7.6 (strongly unimodular spherical ⇒ polar type) is only sketched",
        "Location": "Section 7.5 (Lemma 7.6), Appendix B",
        "Explanation": "The proof combines Zariski‐open arguments in G_C/K_C A_C H_C with an analytic argument involving continuation of spherical functions f_λ and a limiting sequence p_n=k_n x_n h_n. The choice of test parameters λ, the control of analytic continuations, and the final deduction that x_n lies in A H are only briefly sketched without full details, leaving a significant gap in establishing G=KAH."
      },
      {
        "Problem": "Theorem 6.4 (quantitative decay |m_{v,η}(a)|≤C a^Λ(1+|log a|)^d) is proved only by a terse sketch",
        "Location": "Section 6.2 (Theorem 6.4)",
        "Explanation": "The proof cites an adaptation of [46] and outlines shifting weights by writing v=dπ(X)u and splitting X into h + (a+m+n) parts. But it does not fully justify the controlled expansion of dπ(Ad(a)X_2)u, the necessary estimates on non‐unitary Harish–Chandra modules, or the convergence of the series that yields the improved exponent. The lack of detailed bounds threatens the soundness of the claimed decay rate."
      },
      {
        "Problem": "Proof of non‐VAI when H is non‐reductive in Proposition A.1 uses charts and volume bounds without rigorous injectivity or uniformity",
        "Location": "Appendix A (Proof of Proposition A.1)",
        "Explanation": "The construction of unbounded smooth L^p-functions on Z=G/H proceeds by choosing X∈z(l_0)∖h, building coordinate maps Φ_t on subspaces v, and summing bumps χ_{−n}. However, the argument presumes that for large |t| each Φ_t is injective on a fixed neighborhood Q and that Jacobian bounds hold uniformly, without detailed verification of these facts in the general non‐reductive setting. This is central to excluding VAI but is only sketched."
      },
      {
        "Problem": "Factorizability condition (9.10) for generalized balls is assumed but not generally proved",
        "Location": "Section 9.3",
        "Explanation": "The main term counting Theorems 10.1 and D require that chosen families of balls (intrinsic BR or representation‐defined BR,ρ) 'factorize well' across every non‐rigid factorization Z→Z*. The paper checks this for symmetric spaces and certain examples, but no general criterion or proof is given to ensure that natural geometric balls always satisfy lim_{R→∞}sup_{g∈Q}1^F_R(gH*)/|U_R|=0. Without this, the main counting theorems may fail in untested cases."
      }
    ],
    "think_summary": [
      "**Identifying critical problems**\n\nThe user wants me to pinpoint up to five critical problems in a paper, specifically formatting them in JSON entries. I'll focus on identifying significant conceptual gaps, logical flaws, and unsound proofs, rather than minor issues. \n\nFor instance, I'll look closely at Theorem A, which hinges on certain assumptions like connectivity and reductive types. Theorem B, on the other hand, makes assertions about spectral bounds without discussing necessary conditions like unitarity. This meticulous analysis is key to addressing significant concerns within the paper.",
      "**Analyzing assumptions and theorems**\n\nI notice that there’s an assumption regarding P H open implying proper decomposition, but they didn't specify H-distinguished representation. They presuppose a specific distribution vector as fixed too, which could require a non-degeneracy assumption. \n\nTheorem C mentions property (I) for strong spherical type, which I need to check against unimodularity equivalence. Theorem D discusses main term counting but raises questions about co-compactness and central contributions. Theorem E analyzes error terms in triple product spaces, noting potential suboptimal bounds. There’s a potential issue with uniformity in Lemma 11.5 that needs closer examination.",
      "**Highlighting critical errors**\n\nI'm focusing on key errors throughout this analysis. Starting with the need for uniform bounds in Lemma 11.5, I found inadequate justification regarding error terms. Then, the proof for Theorem 7.6 raises concerns, particularly its complexity and missing details, especially relating to analytic continuation arguments. \n\nI also noticed issues in the non-VAI case for Theorem 2.2, where they didn't sufficiently justify diffeomorphism requirements. Lastly, the assumptions made for bounds in Theorem B lack clarity. These errors could seriously impact the overall validity of the proofs.",
      "**Identifying core issues**\n\nI'm getting ready to outline important problems in the analysis. One critical issue is the lack of rigorous justification for Lemma 11.5 concerning uniform bounds. Then, Theorem 7.6 relies heavily on analytic continuation, yet crucial steps are only sketched. The proof for Theorem B misses essential integrability conditions. Also, the converse of Theorem A constructs unbounded functions without fully addressing their injectivity. Lastly, the factorization of geometric balls isn't universally verified, which raises concerns about its applicability.\n\nNow, I'll draft the JSON entries to detail these problems."
    ],
    "token_usage": {
      "input": 47690,
      "thinking": 2560,
      "output": 889
    }
  },
  {
    "entry_id": 121,
    "retraction_id": "2106.14209v3",
    "paper_id": "2106.14209v2",
    "retraction_comment": "The paper builds on the wrong statement in the paper \"The quantum twistor bundle\" Theorem 4.2. Therefore the C*-algebra investigated in the present paper is not the one for the quantum symplectic sphere",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Use of discontinuous characteristic functions in defining φ⁻¹",
        "Location": "Theorem 3.4, definition of φ⁻¹ mapping P_{v_{n+1}} and P_{v_j} via χ₁",
        "Explanation": "The authors define projections in C(S⁴ⁿ⁻¹_q) by applying the characteristic function χ_{ {1} } to y_i^*y_i even though 1 is an accumulation point of Spec(y_i^*y_i) (e.g. 1–q^{2k}→1). χ_{ {1} } is not continuous on the spectrum, so χ_{ {1} }(y_i^*y_i) need not lie in the C*-algebra. This invalidates the construction of φ⁻¹."
      },
      {
        "Problem": "Unjustified claim that the π representation is faithful",
        "Location": "Lemma 3.1",
        "Explanation": "Lemma 3.1 invokes a single irreducible *-representation π of C(S⁴ⁿ⁻¹_q) and asserts it is faithful. Irreducible representations always have nontrivial kernels (primitive ideals), so one cannot conclude faithfulness without further argument (e.g. direct sum over all irreducibles). The subsequent spectrum analysis and use of π to distinguish elements fails if π is not actually faithful."
      },
      {
        "Problem": "Spectrum computations refer only to π-image, not to full algebra",
        "Location": "Lemma 3.2",
        "Explanation": "Lemma 3.2 describes the joint spectrum of the elements y_i^*y_i by computing the spectrum under the (unjustified) faithful representation π. Even if the π-spectrum has the stated form, without genuine faithfulness one cannot conclude that the abstract spectrum in C(S⁴ⁿ⁻¹_q) agrees. All conclusions about functional calculus and spectral projections are therefore unsound."
      },
      {
        "Problem": "Typographical and notational confusion in the definition of π",
        "Location": "Lemma 3.1 (first line)",
        "Explanation": "The statement reads π: C(S⁴ⁿ+¹_q) → B(ℓ²(ℕⁿ×ℤ)) even though the paper is about C(S⁴ⁿ⁻¹_q). This suggests a mis‐indexing of the sphere and casts doubt on the correctness of the representations and ensuing isomorphism."
      },
      {
        "Problem": "Reliance on π and ρ to prove φ and φ⁻¹ are inverses",
        "Location": "Proof of Theorem 3.4",
        "Explanation": "The authors conclude φ∘φ⁻¹=id and φ⁻¹∘φ=id by checking these equalities only under the (claimed) faithful representations π and ρ. Without independent verification on generators or a genuine faithfulness proof, these computations do not imply equality of the maps in the abstract C*-algebras, leaving the isomorphism unestablished."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 11059,
      "thinking": 4032,
      "output": 693
    }
  },
  {
    "entry_id": 122,
    "retraction_id": "1608.08317v2",
    "paper_id": "1608.08317v1",
    "retraction_comment": "This paper has been withdrawn by the author due to a crucial error in equation 13",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Lack of orthogonality among σ-SCF states",
        "Location": "Sec. II D (paragraph 3)",
        "Explanation": "σ-SCF does not impose orthogonality constraints on the determinants generated for different ω values. Real electronic eigenstates are orthogonal, so non-orthogonal σ-SCF states can mix or swap character along PES’s, leading to ambiguous state assignment and spurious crossings—undermining the claim that σ-SCF can robustly locate all excited states."
      },
      {
        "Problem": "No guarantee of true minima in determinant-restricted variance optimization",
        "Location": "Sec. II B (eqns. 7–8)",
        "Explanation": "While full Hilbert-space variance minimization has local minima at eigenstates, restricting Ψ to single Slater determinants invalidates that proof. The SCF steps diagonalizing the ‘variance Fock matrix’ only solve stationarity conditions and can converge to saddle points or maxima, so there is no assurance the algorithm finds the intended excited-state minima."
      },
      {
        "Problem": "Severe spin-contamination without correction",
        "Location": "Sec. IV A (eqns. 17–18 and related discussion)",
        "Explanation": "Unrestricted σ-SCF singlet solutions exhibit ~50% triplet contamination (γ_T ≈ 0.5), yet no spin-projection or adaptation is applied. This contaminates energies and densities, invalidating quantitative comparisons to FCI and compromising any spin-specific excited-state properties derived from these solutions."
      },
      {
        "Problem": "Misapplication of the Hellmann–Feynman theorem to energy and variance derivatives",
        "Location": "Sec. V (eqns. 19–22)",
        "Explanation": "The derivation assumes the Hellmann–Feynman theorem holds for the variance operator and then uses it to relate non-smoothness in σ-SCF energy to discontinuities in ⟨H²⟩. However, the variance operator depends explicitly on the wavefunction energy E, so the standard HF theorem does not apply. Consequently, the conclusions about PES continuity and its correlation with HF variance lack a valid foundation."
      },
      {
        "Problem": "Unproven coverage of all excited states by ω-scanning in determinant space",
        "Location": "Sec. II A (eqns. 1–4) and Sec. II C (step 1)",
        "Explanation": "The guarantee that each electronic eigenstate corresponds to a local minimum of Ω(ω) relies on a full variational space. Restricting to single determinants breaks that guarantee—states of strong multi-reference character may not manifest as minima for any ω. Without proof of completeness or robustness of the ω-scan, the claim that σ-SCF can in principle find all excited states is unsound."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 25071,
      "thinking": 3456,
      "output": 660
    }
  },
  {
    "entry_id": 123,
    "retraction_id": "1805.10733v3",
    "paper_id": "1805.10733v2",
    "retraction_comment": "The result (eq. 3) is not correct; therefore, latter part which is derived from this result is not correct",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Misidentification of C as a physical thermodynamic cost by neglecting bath entropy and far-from-equilibrium operation",
        "Location": "Page 3, after Eqs. (12)–(13) and accompanying paragraph",
        "Explanation": "The paper defines C via system‐only entropy changes (Eq. 13) and invokes Δσ≈0 (Δσ=Δσ_bath+Δσ_sys≈0) to argue C represents thermodynamic cost. However, sensory adaptation in E. coli is maintained far from equilibrium with continuous energy dissipation in the methylation cycle. Neglecting bath entropy production invalidates the identification of C with actual physical energy or entropy costs, undermining the core efficiency metric."
      },
      {
        "Problem": "Mathematical inconsistency in the derivation of the statistical speed metric",
        "Location": "Page 2, Eqs. (7)–(9)",
        "Explanation": "Equations (7)–(9) conflate ds²/dt² with (ds²/dt) and assert ds²/dt²=(√(ds²/dt))², confusing first and second time derivatives. This flawed manipulation invalidates the definition of ds/dt as the statistical speed, which is pivotal for the subsequent Cauchy–Schwarz bound and efficiency formulation."
      },
      {
        "Problem": "Overreliance on Gaussian approximation and linearization despite inherent nonlinear dynamics",
        "Location": "Page 1–2, Methods section and Eqs. (1)–(2)",
        "Explanation": "The analysis assumes the joint distribution of kinase activity and methylation level remains Gaussian under a linear Langevin model. In reality, receptor methylation exhibits saturation and cooperative nonlinearities, and a step change in ligand can induce transient non‐Gaussian distributions. These non‐Gaussian effects would invalidate both the Fisher‐metric formulas and the KL‐based ds² expressions used throughout."
      },
      {
        "Problem": "Use of a time‐dependent efficiency η(τ) that yields arbitrary, non‐comparable values",
        "Location": "Page 3–4, Eqs. (16)–(17) and Figs. 3–5",
        "Explanation": "Defining efficiency as η(τ)=L(τ)²/[τ C(τ)] makes η a monotonically decaying function of the chosen observation window τ, even for identical underlying dynamics. Consequently, comparing η across different noise levels or stimulation strengths conflates differences in timescale with genuine efficiency effects, calling into question claims of robustness."
      },
      {
        "Problem": "Lack of empirical or numerical validation against known energy dissipation",
        "Location": "Discussion and Conclusion sections",
        "Explanation": "The proposed cost C and efficiency η are never benchmarked against ATP consumption rates or heat dissipation data in E. coli chemotaxis. Without such validation, it remains unclear whether the information‐geometric cost measure corresponds to any real thermodynamic quantity, undermining the biological relevance of the conclusions."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 8695,
      "thinking": 3264,
      "output": 684
    }
  },
  {
    "entry_id": 124,
    "retraction_id": "1912.00541v2",
    "paper_id": "1912.00541v1",
    "retraction_comment": "As several people have pointed out to me, the last sentence of Lemma 5.3 is not justified. This likely counts as a fatal flaw that invalidates the main theorem (Theorem 5.4). I would like to thank those who took the time to read the preprint and send me their feedback. I hereby retract the claimed result",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Unjustified global extension from local homomorphisms (Proposition 4.1)",
        "Location": "Page 8, Proposition 4.1",
        "Explanation": "The authors claim a bijection β:k^G→Hom(G,Δ_r) by pulling back the atlas of Δ_r to define a k-coloring on G from any graph homomorphism ι:G→Δ_r. But ι only guarantees pairwise consistency of adjacent r-balls, not coherence over arbitrary cycles in G. No argument (e.g. a finite-intersection or gluing lemma) is given to ensure these local patches globally piece together to a single coloring on all of G. This undermines the fundamental correspondence between colorings and graph homomorphisms used throughout."
      },
      {
        "Problem": "Ambiguity in the “possible to move” edge relation (Definition 3.1)",
        "Location": "Page 5, Definition 3.1",
        "Explanation": "Edges in the generalized de Bruijn graph are defined by an existential condition: there must exist some global k-coloring realizing two local patches at distance a. Different global extensions may exist for different pairs of patches, and there is no control over transitivity or uniqueness. This makes the edge relation non-constructive and potentially non-coherent when gluing multiple edges, casting doubt on any argument that relies on finite graph homomorphisms faithfully modeling the shift space."
      },
      {
        "Problem": "Composition of models need not equal the canonical projection (Lemma 5.1)",
        "Location": "Page 10, Lemma 5.1",
        "Explanation": "The proof asserts that if ψ: A_r→Δ_q models f^{-1} and φ: Δ_q→Δ_p models f, then φ∘ψ is \"the restriction of the natural projection π_{r,p} to A_r\". In reality φ∘ψ is merely some model of the identity map on A, not necessarily the canonical projection. Without uniqueness of models, one cannot conclude that the induced map on immersions φ_*∘ψ_* agrees with (π_{r,p})_*, so the subsequent injectivity argument fails."
      },
      {
        "Problem": "False claim that q-compatibility implies injectivity on homomorphisms",
        "Location": "Page 11, Theorem 5.4, penultimate paragraph",
        "Explanation": "They introduce a canonical homomorphism φ:(Δ_q,ψ^*α)→Δ_q from Proposition 3.5 and assert that because φ is q-compatible, the induced map φ_*:Hom(G,(Δ_q,ψ^*α))→Hom(G,Δ_q) is injective. But q-compatibility only guarantees compatibility of labels up to radius q, not that φ is vertex-injective or preserves distinct immersions. No argument is provided that φ_* cannot identify two different immersions, so the key contradiction (injective φ_* with eventually noninjective iterate) collapses."
      },
      {
        "Problem": "Inconsistent choice of scales for models of f and f^{-1}",
        "Location": "Pages 9–10, Lemmas 5.1 and 5.3",
        "Explanation": "Both Lemma 5.1 and Lemma 5.3 require choosing radii r>q>p so that models of f and f^{-1} exist and are q-compatible. However, Proposition 4.4 only guarantees a model at some (potentially different) pair of scales for each map, and there is no mechanism to synchronize these choices. The circular dependency in selecting r,q,p for both directions means one cannot ensure all required compatibility conditions simultaneously, invalidating the constructions of full models."
      }
    ],
    "think_summary": [
      "**Identifying logical issues**\n\nThe user is looking to find up to five critical errors in a paper claiming to prove surjunctivity for all groups, which is an extremely unlikely assertion since it would resolve several significant conjectures simultaneously. I need to examine the potential flaws in definitions or logic as follows. First, I’ll consider how they define the generalized de Bruijn graph and whether the term \"possible to move\" is well-defined, as it may involve ambiguous sets. I'll also review Proposition 3.5 regarding the canonical map.",
      "**Examining bijections and homomorphisms**\n\nThe focus here is on how they rely on α′ being bijective. Since the atlas of ∆r is defined as identity, we can establish that α′ is bijective. In Proposition 4.1, a bijection β is defined, connecting k^G to Hom(G, ∆r). However, G being infinite and ∆r finite complicates things. They assume α′ leads to a unique mapping in ∆r. I need to check if this assumption about edge preservation holds. Ultimately, we need to validate whether they correctly demonstrate that edges are preserved in the homomorphism.",
      "**Identifying critical problems**\n\nThis reasoning focuses on various critical issues in a paper that relies on local-to-global extensions in graph theory. Firstly, Proposition 4.1's bijection fails because it overlooks the necessity for global coherence of local patches. Secondly, the definition of \"possible to move\" leads to an ambiguous edge relation in the graph. Thirdly, the assumption about the model composition not being proven can undermine injectivity claims. Fourth, the proof about the canonical map lacking injectivity on vertices raises concerns. Lastly, there's a circular dependency in the model framework that may not ensure consistency."
    ],
    "token_usage": {
      "input": 9928,
      "thinking": 6400,
      "output": 837
    }
  },
  {
    "entry_id": 125,
    "retraction_id": "2002.11860v4",
    "paper_id": "2002.11860v3",
    "retraction_comment": "Mistake in Lemma 3 changing the announced rate. Withdrawing while fixing the error",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Theorem 2’s convergence claim only establishes a subsequence convergence (lim inf of the Frank–Wolfe gap equals zero) rather than full convergence of the iterates or the gap to zero, yet the paper asserts full convergence to a stationary point.",
        "Location": "Section 3.3 (Theorem 2) and its proof in Appendix F",
        "Explanation": "The proof shows ∑ₜ γₜ ĝₜ is finite and ∑ₜ γₜ diverges, which implies lim inf ĝₜ=0 but not ĝₜ→0. Concluding that the entire sequence (gₜ) converges to zero and hence wₜ→a stationary point is unfounded without further argument (e.g. monotonicity or stronger regularity)."
      },
      {
        "Problem": "Use of the stochastic Frank–Wolfe gap estimator ĝₜ as a stopping criterion is unsupported: the paper bounds its deviation from the true gap but never shows ĝₜ itself converges or decays.",
        "Location": "Section 2.3 (Proposition 1) and related discussion",
        "Explanation": "Proposition 1 (informal) bounds |gₜ−ĝₜ| but does not establish that ĝₜ→0. Without a guarantee on ĝₜ’s decay, using ĝₜ as a practical stopping rule may be misleading, especially in the non-convex setting."
      },
      {
        "Problem": "Claim of strictly faster worst-case rates than prior unit-batch methods is too broad: for small t the O(n^{7/2}/t²) term can dominate and be worse than O(n/t).",
        "Location": "Abstract and Section 4 (Table 1 and surrounding text)",
        "Explanation": "The derived rate O(√n/t + n^{7/2}/t²) only improves over O(n/t) once t≫n^{5/2}. For practical t smaller than this threshold, the second term makes the bound weaker, contradicting the blanket statement of uniformly faster worst-case rates."
      },
      {
        "Problem": "Theorem 1’s stated dependence on n in the higher-order term is incorrect—exponent of n in the H₀ term is misstated, indicating a mistake in the rate derivation.",
        "Location": "Theorem 1, Equation (11)",
        "Explanation": "Carrying through the bounds in Lemmas 2–3 yields an O(n^{5/2}/t²) dependence for the term involving the initial gradient error H₀, but Theorem 1 reports O(n^{7/2}/t²). This mismatch suggests an algebraic error in aggregating the finite-sum bounds."
      },
      {
        "Problem": "Definition of diameter Dₚ in Equation (10) is ill-posed: xᵢᵀ(u−v) is a scalar, so its p-norm across components is undefined or vacuous.",
        "Location": "Section 3.1, Equation (10)",
        "Explanation": "The paper writes Dₚ = maxᵢ, u,v∈C ‖xᵢᵀ(u−v)‖ₚ, but xᵢᵀ(u−v)∈ℝ, so the vector p-norm is meaningless. This ambiguity undermines subsequent Lipschitz and diameter-based bounds."
      }
    ],
    "think_summary": [
      "**Analyzing a mathematical error**\n\nI’m identifying an error in Theorem 1 regarding the dependence on n in the location equation. The exponent in the second term is incorrectly stated as n^{7/2} when it should be n^{5/2}. Similarly, there's an inconsistency with the term 48n^{7/2}LD1D2, which should also be adjusted. Their bound for Bt seems accurate, but the expression for the sum includes a missing factor. Correctly, the n dependence in the H0 term has significant implications, although the main term is still accurate.",
      "**Identifying logical flaws**\n\nI'm noticing several critical errors in their reasoning. First, they misstate that g_t converges fully when it only shows that liminf g_t equals zero. This is a significant flaw. They claim convergence to a stationary point in Theorem 2 but only demonstrate subsequence convergence. In Theorem 1, I see an error in the exponent for H0, where it should be n^{5/2} instead of n^{7/2}. Additionally, they have some issues with their bounds and definitions, leading to broad and perhaps unsound claims.",
      "**Clarifying errors in definitions**\n\nI’m noticing a possible mix-up in their definitions regarding Dp. They may have intended D2 to be max_{u,v∈C} ||X(u-v)||_2 instead of how they currently defined it. The notation in (10) seems incorrect because x_i^T(u-v) is a scalar, making the p-norm undefined across different values. Additionally, I need to highlight some critical errors that invalidate their conclusions, focusing on five specific points related to convergence and definitions that could impact the logic significantly."
    ],
    "token_usage": {
      "input": 18508,
      "thinking": 3648,
      "output": 778
    }
  },
  {
    "entry_id": 126,
    "retraction_id": "2109.13007v2",
    "paper_id": "2109.13007v1",
    "retraction_comment": "We found a gap in the claim 1 (we can not solve it)",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Flawed boundary tangency argument in Proposition 1",
        "Location": "Page 4, Proof of Proposition 1, boundary case",
        "Explanation": "The authors attempt to show that the two unit normals η₁ and η₂ at a boundary contact point are linearly dependent by decomposing tangent spaces of Σ₁ and the sphere. Their dimension counts are inconsistent (they write TₚSⁿ = Span{e₁,…,e_{n−2},η₁,η₂}, which is (n+1)-dimensional), and the subsequent algebraic steps (deriving ⟨η₁,η₂⟩=0 and reaching a contradiction) rest on this incorrect decomposition. The normal–tangent splitting for free‐boundary hypersurfaces is mishandled, invalidating the boundary‐case use of the tangency principle."
      },
      {
        "Problem": "Misuse of tangency principle for free-boundary hypersurfaces",
        "Location": "Page 2–3 (Theorem 1) and throughout Section 3",
        "Explanation": "Theorem 1 (Fontenele–Silva) is formulated for compact minimal hypersurfaces in ℝⁿ⁺¹ without boundary. The paper applies it directly to free-boundary hypersurfaces meeting Sⁿ, without verifying the additional angle‐ or boundary‐contact hypotheses required for boundary tangency. As a result, each application of this maximum/tangency principle to boundary intersection points is unjustified."
      },
      {
        "Problem": "Unjustified symmetry in applying Claim 1 to Σ₂",
        "Location": "Page 6, beginning of Claim 2",
        "Explanation": "Claim 1 is proved only for the first hypersurface Σ₁. In Claim 2, the authors invoke it for Σ₂ to assert that an arbitrary equatorial semi-disk D⁻ meets Σ₂⁺, but no argument or remark is given that the same proof works verbatim for Σ₂. This leap assumes symmetry without justification, undermining the foundational intersection step in the Claim 2 argument."
      },
      {
        "Problem": "Intersection of Σ₂⁺ with D⁻ occurs only on the boundary equator",
        "Location": "Page 6, Claim 2",
        "Explanation": "They deduce from D⁻∩Σ₂⁺≠∅ that Σ₂⁺ meets the interior of D⁻, yet D⁻ = D ∩ H⁺[−ν] lies entirely in the negative ν-half-ball while Σ₂⁺⊂H⁺[ν]. Hence any intersection can occur only on the common boundary D ∩ D[ν], i.e. the equatorial plane I[p₁,p₂]. This fact weakens the claim that there is a genuine interior intersection point to initiate the rotation/tangency argument."
      },
      {
        "Problem": "Missing one-sidedness condition in final tangency argument",
        "Location": "Page 7, end of Claim 2",
        "Explanation": "When rotating Σ₂ and Σ₁ into first contact at p₀, the authors invoke the tangency principle without verifying that one hypersurface locally stays on one side of the other at the contact point. Neither a local ordering nor a barrier property is established near p₀ (which may lie on ∂B), so the conclusion that the two rotated hypersurfaces coincide is unsupported."
      }
    ],
    "think_summary": [
      "**Considering global oversight**\n\nI'm reflecting on whether the user might be overlooking the possibility that Σ1 and Σ2 could coincide without being rotations of each other, and perhaps they just share a hemisphere. But, you know what? That's not the main focus here. The user is mainly interested in identifying up to five critical problems, so I’ll steer my attention toward that and help them out with a solid list of significant issues."
    ],
    "token_usage": {
      "input": 9204,
      "thinking": 9088,
      "output": 776
    }
  },
  {
    "entry_id": 127,
    "retraction_id": "1305.3218v2",
    "paper_id": "1305.3218v1",
    "retraction_comment": "This paper has been withdrawn by the author. Due to the dependence of f(\\sigma) on z, Lemma 5.3 is incorrect",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Incorrect circuit reconstruction by FLATTEN",
        "Location": "Section 6, Algorithm FLATTEN",
        "Explanation": "The transformation C₀ → C₀₀ with root r(C₀₀)=∨_{α∈C₀}(α(C₀)∧C₀) assumes that any sub‐circuit output α(C₀)=1 implies the root C₀=1. In a general De Morgan circuit, a node α feeding into an AND gate whose other input is 0 can be 1 without forcing the root to 1. Thus C₀₀ can compute a different function than C₀, invalidating the flattening step."
      },
      {
        "Problem": "Unproven existence of fixed‐point negative tail",
        "Location": "Section 5.2, Steps 1–4",
        "Explanation": "The proof hinges on selecting a negative tail z such that z′=z under the elimination mapping z→z′ derived from splitting and blocked‐edge elimination. No combinatorial fixed‐point argument or counting lemma is provided to guarantee such a z exists, so the crucial step of eliminating contradictions may fail."
      },
      {
        "Problem": "Uniform split y across all nodes not guaranteed",
        "Location": "Section 3.3 and 4.4",
        "Explanation": "The argument selects a single split y=(y₁,…,y_q) claimed to be valid for every extension generator g at every node α. However, the set of valid y depends on local sparsity properties at each α, and there is no proof that the intersection of those validity conditions over all exponentially many nodes remains non‐empty."
      },
      {
        "Problem": "Informal continuous argument in double‐mark sparsity bound",
        "Location": "Lemma 2.6",
        "Explanation": "The proof uses a Lagrange multiplier to minimize ∑x_i² subject to ∑x_i fixed, treating x_i as continuous. Since marks are discrete counts, this continuous relaxation need not yield a valid bound on the actual integer mark distribution, undermining the derived sparsity relation."
      },
      {
        "Problem": "Shift method merging negative literals can reintroduce contradictions",
        "Location": "Section 5.2–5.3, LOCALSHIFT2",
        "Explanation": "In the non‐monotone construction, terms t(σ) mix positive and negative literals and rely on masking and splitting to avoid {e,¬e}. The merging of B‐terms across AND/OR gates is not shown to preserve absence of both e and ¬e, so the final shift may still contain contradictory literals, breaking the intended counterexample."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 66532,
      "thinking": 2880,
      "output": 615
    }
  },
  {
    "entry_id": 128,
    "retraction_id": "1502.02299v3",
    "paper_id": "1502.02299v2",
    "retraction_comment": "This paper has been withdrawn by the authors due to a sign error in the last equation of system (2.11). In turn, this implies a change of sign of the last equation in the linearized system (3.1) as well. The linear three annuli property for solutions to the new system (3.1) is no longer valid",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Assumption of radial parametrization of the crack without justification",
        "Location": "Introduction (just before Theorem 1.2), equation (1.3)",
        "Explanation": "The paper assumes that, under David’s Hausdorff‐closeness result, the jump set Su in B1 can be written as {r(cos α(r),sin α(r)): r∈(0,1)} with a single‐valued C∞ function α. David’s theorem only guarantees a connected C1 arc close to a radius, not that it intersects each circle of radius r exactly once or that it admits a radial graph parametrization. This unproven monotonicity in r is essential for the entire reparametrization and PDE setup."
      },
      {
        "Problem": "Unjustified uniform C^k convergence in the blow‐up rescaling Lemma",
        "Location": "Section 2.1, Lemma 2.1",
        "Explanation": "Lemma 2.1 asserts that uρ→Rad and αρ→0 in C^k on annuli as ρ→0, citing only Bonnet’s blow‐up and a higher‐differentiability reference. No uniform control of all derivatives up to order k is provided, yet Corollary 2.2 relies on this to linearize the PDE. Without a detailed proof of these uniform C^k bounds, the subsequent perturbative analysis is unsupported."
      },
      {
        "Problem": "Compatibility condition for Ventsel boundary operator not verified",
        "Location": "Section 3.3, equation (3.14)",
        "Explanation": "To solve the boundary‐value problem with Ventsel condition, the authors restrict the data g to the subspace W defined by ∫0^{2π}g(φ) cos(φ/2) dφ=−π g(0). However, there is no verification that the actual boundary data arising from the linearization of (2.11) satisfy this compatibility condition. If the condition fails, the spectral analysis and decomposition in Corollary 3.7 cannot be applied."
      },
      {
        "Problem": "Insufficient control of nonlinear remainders in the three‐annuli argument",
        "Location": "Section 6, equation (6.13) and surrounding",
        "Explanation": "In proving the nonlinear three‐annuli property, the terms I_j and I_j^{(2)} are estimated to vanish as j→∞, but the bounds mix small parameters ε_j and norms of α, ϑ, v without a uniform hierarchy. The key estimate lim j ∥I_j∥_{L^2}=0 depends on dividing by ε_j and also uses decay of ∂φ v_j, yet no uniform estimate ensures these quotients tend to zero. This gap undermines the convergence to the linearized system."
      },
      {
        "Problem": "Final graphical conclusion in Theorem 1.1 is handwavy and unquantified",
        "Location": "Section 8, last paragraph of the proof of Theorem 1.1",
        "Explanation": "The authors conclude that Su∩B_{1/4}(y0) is a graph in the standard coordinate frame because the tangent ν1 must be “close” to (1,0). No quantitative estimate is given to ensure that the small C^{1,κ} deviation from a radius suffices to guarantee global single‐valuedness over a fixed smaller ball. A rigorous argument or explicit smallness threshold is missing."
      }
    ],
    "think_summary": [
      "**Identifying critical errors**\n\nI'm tasked with finding the five most critical errors in the conclusions of a paper, focusing on significant issues rather than minor ones. I'll create a JSON list structured with \"Problem,\" \"Location,\" and \"Explanation.\" I'll need to carefully read through the paper to spot these errors. For instance, there might be gaps in proofs, reliance on an unproven result, issues with the self-adjointness of an operator, and concerns about boundary condition changes. Let’s dive in!",
      "**Analyzing critical errors**\n\nThere might be potential sign errors in the derivation. For the Ventsel boundary condition, we need to verify if the boundary term in the relevant equation holds, based on defined conditions in W. The \"three annuli\" theorem relies on a functional definition needing more robust estimates for nonlinear terms, which could be uncontrolled. There are concerns over boundary conditions needing higher regularity. Also, in deriving another theorem, there could be bugs with the bounds needed. My goal is to focus on the main logical flaws in the proofs of Theorems 1.1 and 1.2 that could invalidate the conclusions.",
      "**Identifying critical flaws in theorems**\n\nI need to find major issues in the theorems presented, starting with Theorem 1.2, which assumes the radial parameterization of a crack without sufficient justification. They rely on a result that doesn't guarantee monotonic intersections, leading to flawed assumptions. Similarly, Lemma 2.1 lacks a solid argument for convergence, while the proof of the Ventsel boundary condition's compatibility isn't clear. The proof for Theorem 1.1 also fails to adequately prove graphical representation. Those are my main focus points for significant errors.",
      "**Summarizing critical issues**\n\nI think I've pinpointed five major issues worth noting for the user: \n\n1. The radial parameterization is unproven.\n2. There’s no proof for uniform C^k convergence in Lemma 2.1.\n3. The compatibility condition in equation (3.14) might not be satisfied.\n4. The explanation of the smallness of remainder terms in Section 6 lacks rigor.\n5. The concluding argument in Theorem 1.1 feels handwavy.\n\nI’ll format these as JSON entries for clarity."
    ],
    "token_usage": {
      "input": 28594,
      "thinking": 1664,
      "output": 801
    }
  },
  {
    "entry_id": 129,
    "retraction_id": "1901.07856v7",
    "paper_id": "1901.07856v6",
    "retraction_comment": "The proof of the the second inequality of Lemma 7 is wrong",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Misapplication of Esperet–Parreau bound to independent random coloring",
        "Location": "Subsection 3.2, Lemma 7",
        "Explanation": "The paper uses Lemma 1 (Esperet–Parreau) to assert that in a random global edge coloring each edge independently avoids 2(Δ−1) forbidden colors with probability (1−2(Δ−1)/K), and then takes the product over edges to bound the probability of a strongly proper (proper + 4-acyclic) coloring. However, the existence of adjacent‐edge and 4-cycle constraints couples the events across edges, so one cannot simply multiply per-edge probabilities. This invalid independence assumption undermines the claimed bound Qˆ_n ≤ 1−[1−2/(2+ε)]^m, and hence the tail bound on repetitions of MainAlgorithm."
      },
      {
        "Problem": "Flawed analytic‐combinatorics derivation for geometric decay",
        "Location": "Subsection 3.3, Equations (9)–(12) and subsequent paragraph",
        "Explanation": "To show that ∑_{|F|=n}Pr[V_F] decays geometrically, the authors set q=(Δ−1)/K=1/(2+ε)<1/2 but then invoke the regime q>1/2 to argue the singularity ρ>1. The calculations for the characteristic equation τφ'(τ)/φ(τ)=1 are only carried out at q=1/2 (ε=0) yielding ρ=1, and the claim that for q>1/2 one obtains ρ>1 contradicts the actual parameter q<1/2. There is no justification that in the intended parameter range the generating‐function singularity lies beyond 1, so the geometric bound on P_n is unsubstantiated."
      },
      {
        "Problem": "Unsound coupling between EdgeColor and ColorVal",
        "Location": "Subsection 3.2, Lemma 6 and preceding coupling description",
        "Explanation": "The paper couples the original EdgeColor algorithm with a simplified validation algorithm ColorVal by matching their random choices on a feasible forest, then infers P_n ≤ P̂_n and Q_n ≤ Q̂_n. But EdgeColor's recoloring phases depend on the evolving coloring state and on conditional events (whether a parity-homochromatic cycle exists), whereas ColorVal blindly recolors according to the forest without conditioning. This mismatch breaks the coupling: the distributions of phase lengths and success events are not aligned, so the inequalities relating P_n and P̂_n lack justification."
      },
      {
        "Problem": "Lack of proof of almost‐sure termination under correct dependencies",
        "Location": "Section 2.2, description of EdgeColor and MainAlgorithm",
        "Explanation": "While Lemma 3 shows at most m root‐level Recolor calls, there is no rigorous argument that the recursive Recolor calls cannot lead to an infinite loop with positive probability once properness and 4-acyclicity are enforced. The claimed tail bounds presuppose correct handling of dependencies, but given the flawed coupling and independence assumptions, there is no valid proof that EdgeColor (or MainAlgorithm) terminates almost surely or in polynomial time."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 8500,
      "thinking": 2368,
      "output": 727
    }
  },
  {
    "entry_id": 130,
    "retraction_id": "1912.10027v2",
    "paper_id": "1912.10027v1",
    "retraction_comment": "We have found some errors in the methods, in particular two of the methods are not A-stable",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Lack of combined IMEX stability analysis",
        "Location": "Section 4 (page 14)",
        "Explanation": "The paper only analyzes linear stability for the explicit part (G=0) and the implicit part (F=0) separately. It does not investigate the two-variable stability region in (Δtλ_F,Δtλ_G) for coupled stiff/non-stiff eigenvalues. Without this combined stability analysis, there is no guarantee that the proposed IMEX-EIS+ schemes will remain stable for realistic problems where both F and G modes are present simultaneously."
      },
      {
        "Problem": "Invalid bound on the propagation operator Q_n in the stiff regime",
        "Location": "Lemma 1 and its proof (Section 2.2, pages 6–7)",
        "Explanation": "The error analysis assumes ∥Q_n∥ ≤ 1 + cΔt by bounding Δt·(AFF'_n + AGG'_n). However, for stiff G-terms G'_n can have large eigenvalues 𝒪(1/ε), so Δt·AGG'_n need not be small. This invalidates the key stability estimate ∥Q_n∥≤1+O(Δt) and thereby undermines the discrete Duhamel argument and the global error bound."
      },
      {
        "Problem": "Unverified zero-stability for the constructed GLMs",
        "Location": "Assumption on D in Section 2.1 (page 5)",
        "Explanation": "The paper asserts that requiring D to be rank-one with D·1=1 guarantees zero-stability. In general GLM theory, zero-stability depends on the full spectrum of the amplification matrix, not just on D. No verification is provided that the new IMEX-EIS+ coefficient sets satisfy the necessary root-condition for zero-stability, so the subsequent convergence and super-convergence claims may not hold."
      },
      {
        "Problem": "Uncontrolled conditioning of the post-processing filter",
        "Location": "Section 3.2 (page 12)",
        "Explanation": "The post-processor is built by inverting a Vandermonde-type matrix T without analyzing its condition number. If T is nearly singular, the filter Φ=T·diag(… )·T⁻¹ can be ill-conditioned, amplifying round-off errors and possibly destroying the claimed p+2 convergence in finite precision. No criterion or numerical check is given to ensure the practical invertibility or stability of T."
      },
      {
        "Problem": "Neglect of mixed F–G higher-order terms in the local error expansion",
        "Location": "Equations (12)–(13) and the derivation of τⁿ (Section 2.1, pages 4–5)",
        "Explanation": "The expansion of the local truncation error separates pure F and pure G derivative contributions but omits mixed derivatives like ∂^jF/∂u^{j−k}∂G^k. These mixed terms can contribute at the same order in Δt and may violate the stated order-conditions (14). Ignoring them could invalidate the form of τⁿ and thus compromise the entire error-inhibiting and post-processing framework."
      }
    ],
    "think_summary": [
      "**Identifying critical problems**\n\nThe user is looking for a JSON format listing the top five critical problems from a specific paper. I need to read it closely to find errors and conclusions that compromise the results. \n\nI’m noticing some potential issues:  \n1. In Lemma 1, there's a neglect of terms affecting the results.  \n2. The discrete Duhamel principle raises concerns about equation dependencies.  \n3. Proofs regarding the derived accuracy could be problematic.  \n4. Risks related to stability in post-processing need attention.  \n5. Their approach to stability analysis might need a more comprehensive review.",
      "**Analyzing stability issues**\n\nI’m examining the linear stability of additive methods, which seems to hinge on how well the explicit and implicit parts are coupled. It’s concerning that the analysis only looks at separate components without considering the stable regions as a whole. \n\nThere are other potential problems: the measurements in Table 1 might not accurately represent stability regions. Also, they assume quite restrictive conditions for zero-stability and may ignore critical concepts like the Dahlquist barrier. Errors in their truncation can arise from mixing derivatives, which might not hold under all conditions."
    ],
    "token_usage": {
      "input": 25397,
      "thinking": 3136,
      "output": 742
    }
  },
  {
    "entry_id": 131,
    "retraction_id": "1502.05353v3",
    "paper_id": "1502.05353v2",
    "retraction_comment": "This paper has been withdrawn due to some errors. The main error is the wrong spin function of Eq. (5), which makes equations (13) and (14) incorrect, and in turn puts doubts on the final conclusions",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Mischaracterization of direct exchange sign and mechanism",
        "Location": "Section 2, Equations (13)–(14)",
        "Explanation": "The authors derive a positive J_d from the bare Coulomb exchange integral and interpret this as stabilizing antiparallel (antiferromagnetic) alignment. In fact, the direct Coulomb (Hund’s‐rule) exchange generally favors parallel spins (ferromagnetism), while antiferromagnetism in Mott insulators arises from virtual superexchange. The sign convention and physical interpretation of J_d are therefore incorrect, undermining the central claim that direct exchange can explain the antiferromagnetic coupling in cuprates."
      },
      {
        "Problem": "Neglect of electronic screening and many‐body renormalization in exchange integral calculation",
        "Location": "Section 3, Equations (17)–(18) and Table 1",
        "Explanation": "The direct exchange integral is computed using unscreened atomic d–p orbitals in vacuum, without accounting for strong dielectric screening, band‐structure effects, or electron–electron correlations present in the solid. In real cuprates the Coulomb integrals are heavily screened and renormalized, which would reduce J_d by orders of magnitude and invalidate the quantitative comparison to the indirect superexchange J = 4τ²/U."
      },
      {
        "Problem": "Over‐simplified one‐particle anti‐bonding wavefunction model",
        "Location": "Section 2, Equations (2)–(4)",
        "Explanation": "The anti‐bonding orbital is constructed by mixing isolated atomic Cu 3d and O 2p functions with simple two‐level theory, omitting orthogonality to the rest of the valence band, crystal‐field splittings, multi‐orbital covalency, and the large on‐site Coulomb repulsion. Such a single‐particle Ansatz is not a reliable basis for evaluating two‐electron exchange integrals in a strongly correlated charge‐transfer insulator."
      },
      {
        "Problem": "Inconsistent addition of direct exchange to the t‐J Hamiltonian",
        "Location": "Section 4, Equation (27)",
        "Explanation": "The t–J model is rigorously derived from the Hubbard model under the no‐double‐occupancy projection. Simply adding an extra 'direct exchange' term between neighboring spins without re‐deriving the low‐energy effective theory leads to double counting of Coulomb interactions and violates the projected Hilbert‐space constraints. A consistent treatment would require a fresh Schrieffer–Wolff transformation including both direct and superexchange pathways."
      },
      {
        "Problem": "Oversimplified energy‐counting for critical doping",
        "Location": "Section 4, pages 19–20 (cases A–C)",
        "Explanation": "The critical doping δ_c is estimated by equating static bond‐breaking magnetic energies and a bare kinetic bandwidth 4τ, neglecting key many‐body effects such as quasiparticle weight renormalization, spin‐wave fluctuations, temperature dependence, and the reconstructed Fermi surface in the antiferromagnetic phase. This simplistic bond‐counting approach cannot reliably predict the true δ_c in cuprates."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 27811,
      "thinking": 2112,
      "output": 728
    }
  },
  {
    "entry_id": 132,
    "retraction_id": "1808.08722v2",
    "paper_id": "1808.08722v1",
    "retraction_comment": "We find the polarization degrees of freedom of the squeezed light had not been treated properly in our analysis, which would affect our results significantly especially in the case of dissipative quantum walks. A new analysis is currently underway. We thank [REDACTED-NAME] for help in clarifying these issues",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Neglect of exponentially decaying success probability in dissipative QWs",
        "Location": "Section II.B, around Eq. (13)–(17)",
        "Explanation": "Each step of the ‘dissipative’ quantum walk implements a projection onto the diagonal polarization, which succeeds with probability ≤1/2 per step. After N steps the overall success probability scales like (1/2)^N, yet the manuscript neither quantifies nor addresses this probabilistic heralding. Without a viable post-selection or amplification strategy, the proposed scheme cannot realistically generate the codewords, invalidating the practicality of the conclusions."
      },
      {
        "Problem": "Error-correction analysis assumes ideal infinite comb codewords and ignores truncation",
        "Location": "Appendix B, Eqs. (B1)–(B3)",
        "Explanation": "The probability P_no_error is computed by projecting onto ideal GKP states |s,t⟩ with infinitely many peaks. However, the actual dQW codewords are finite superpositions over n∈[−N,N], so peaks outside this range are absent. This truncation significantly increases uncorrectable shifts, but is omitted in the integration over s,t, leading to overly optimistic error-correction performance."
      },
      {
        "Problem": "Approximation of squeezed coherent states as orthonormal in normalization and envelope derivation",
        "Location": "Section II.B, Eqs. (21)–(22)",
        "Explanation": "In deriving the Gaussian envelope for the position-space wavefunction and normalization Z_N, the displaced squeezed states |n r⟩ are treated as mutually orthogonal. For finite squeezing (r) and displacement spacing √π, adjacent squeezed states have non-negligible overlap, invalidating the Stirling-based normalization and the resulting ∆x, ∆p relationships. This undermines the claimed correspondence with standard GKP codewords."
      },
      {
        "Problem": "Non-unitary walk alters logical amplitudes without a compensation protocol",
        "Location": "Section II.B, Eqs. (17)–(19)",
        "Explanation": "The mapping from the input polarization amplitudes (α,β) to the encoded qubit coefficients (α₀,β₀) depends on the ratio γ=√(Z_{N+1}/Z_N), arising from the dissipative walk. This non-linear, state-dependent distortion breaks the one-to-one correspondence with arbitrary logical inputs. The paper does not propose how to pre-compensate or correct for this amplitude skew, calling into question the scheme's ability to encode arbitrary qubit states faithfully."
      },
      {
        "Problem": "Gaussian envelope approximation invalid for small N due to binomial distribution deviations",
        "Location": "Section II.B, discussion around Fig. 3",
        "Explanation": "The analysis treats the binomial weights w_N(n) of the dQW codewords as if they form a Gaussian envelope even for moderate N (e.g. N=8). In reality, the finite-N binomial distribution exhibits skewness and kurtosis deviations from a Gaussian, especially in the tails. Relying on the Gaussian approximation to compare performance at N=8 misestimates both ∆p and the error-correction efficacy, undermining the quantitative conclusions drawn from Fig. 4."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 10857,
      "thinking": 2176,
      "output": 753
    }
  },
  {
    "entry_id": 133,
    "retraction_id": "2406.11623v4",
    "paper_id": "2406.11623v3",
    "retraction_comment": "Some errors appeared in the article that seem difficult to correct. For example, the Green function $G_R(o,x)$ for the geodesic ball $B(R)$ was misunderstood to satisfy the Dirichlet boundary condition on the geodesic sphere $\\partial B(R)$, however, this is not the case",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Misapplication of Borel’s lemma and differentiation in the proof of the Calculus Lemma",
        "Location": "Section 4.1, Theorem 4.2 (proof, display (4))",
        "Explanation": "The authors set S(r)=∫_{Δ(r)}g_r\\,k\\,dv and compute S′(r)=4r F_R(r)∫_{∂Δ(r)}k dσ. They then assert without justification that\n\n    d/dr [ (1/(4rF_R(r))) S′(r) ] = ∫_{∂Δ(r)}k dσ,\n\nand apply Borel’s lemma twice to the resulting “u(r)”.  In fact one has\n\n    (1/(4rF_R(r))) S′(r)=∫_{∂Δ(r)}k dσ,\n\nso differentiating again does *not* reproduce the same boundary integral, and the two successive applications of Borel’s lemma (which requires the function to be non‐decreasing) are not valid.  This destroys the key estimate\n\n    ∫_{∂Δ(r)}k\\,dπ_r ≤ …(∫_{Δ(r)}g_r k dv)^{(1+δ)/2}\n\non which all the logarithmic derivative estimates and hence the Second Main Theorem rest."
      },
      {
        "Problem": "Unjustified use of Sard’s theorem for the domains Δ(r)",
        "Location": "Section 2.1, paragraph invoking Sard’s theorem",
        "Explanation": "The construction of Δ(r) relies on level sets of the Green function G_R(o,x), and the authors invoke Sard’s theorem to assert that for almost every r the boundary ∂Δ(r) is a smooth submanifold.  Sard’s theorem requires that G_R be C^∞, but the only regularity shown (via heat‐kernel representation) is that G_R is C^2 away from the pole.  Without C^∞ regularity one cannot conclude that almost all level sets are smooth, so the boundary integrals and normal derivatives used throughout are not justified."
      },
      {
        "Problem": "Ambiguous definition of Ric(Ω) and flawed current inequality",
        "Location": "Section 5.1, just before equation (8)",
        "Explanation": "The paper writes\n\n    ddc[log ξ] ≥ f^* c₁(L,h) − f^* Ric(Ω) + R − [Red(f^*D)]\n\nwhere Ω=c₁(L,h)^n is a top‐degree form on X and Ric(Ω):=−ddc log Ω is used as the curvature form of K_X.  But Ω is a (n,n)‐form, not a Hermitian metric on K_X, and the identification of Ric(Ω) with the Chern form of K_X is never justified.  Hence the fundamental inequality leading to (8) is ill‐posed."
      },
      {
        "Problem": "Logarithmic Derivative Lemma depends on the flawed Calculus Lemma",
        "Location": "Section 4.2, Lemma 4.9 and Theorem 4.10",
        "Explanation": "Both Lemma 4.9 and Theorem 4.10 invoke Corollary 4.3 (the boundary‐integral estimate) to control terms of the form\n\n    ∫_{∂Δ(r)} log⁺ |∇ψ|/|ψ| dπ_r.\n\nSince Corollary 4.3 rests on Theorem 4.2—which has been shown above to be incorrect—the entire logarithmic derivative machinery collapses, removing the main analytic tool needed to prove the Second Main Theorem."
      },
      {
        "Problem": "Failure to show that Δ(r) exhausts M in the global (R=∞) case",
        "Location": "Section 2.1, last paragraph (and global discussion in Section 4.2)",
        "Explanation": "In the global (R=∞) situation the domains are defined by\n\n    Δ(r)= {x : G(o,x)>A₁∫_r^∞dt/V(√t)}.\n\nFor the Nevanlinna functions to make sense one needs Δ(r)↗M as r→∞, i.e. G(o,x)→0 at infinity.  The paper never proves G(o,x)→0 (or that the threshold ∫_r^∞dt/V(√t)→0 fast enough), so it is not established that these Δ(r) eventually cover arbitrarily large compact sets of M.  Without exhaustion, the global Second Main Theorem and Picard‐type corollaries are unsupported."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 20238,
      "thinking": 7552,
      "output": 1072
    }
  },
  {
    "entry_id": 134,
    "retraction_id": "2108.09325v2",
    "paper_id": "2108.09325v1",
    "retraction_comment": "Several sections, particularly Section 5, contain an error interpreting the mutual inclination directly as the obliquity",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Neglect of octupole-level dynamics when deriving obliquity evolution",
        "Location": "Section 2, Equations (1) and (4)",
        "Explanation": "The analytic relation between e_max and i_min (Eqn. 1) assumes the quadrupole approximation and test-particle limit, neglecting octupole-order terms. In real systems with non-zero outer eccentricity (e_out≳0.3) and comparable masses, octupole effects can qualitatively alter Kozai–Lidov cycles (e.g. induce orbital flips, chaotic evolution) and thus change the resulting inclination distribution. Ignoring these contributions may render the predicted polar obliquity outcomes invalid for a large fraction of plausible companion configurations."
      },
      {
        "Problem": "Assumption that final spin–orbit angle equals the minimum mutual inclination",
        "Location": "Section 2, paragraph after Eqn. (4)",
        "Explanation": "The study assumes that tidal dissipation during high-eccentricity migration freezes in the orbital orientation at i_min (the minimum mutual inclination in a Kozai cycle). However, tidal evolution involves complex coupling between orbital angular momentum, spin, and dissipative processes (including stellar tides and spin–orbit coupling) that can drive additional inclination damping or excitation. The simplification i_final≈i_min may therefore mispredict the true distribution of final obliquities."
      },
      {
        "Problem": "Potential error in the definition and normalization of the GR precession parameter ε_GR",
        "Location": "Section 2, Equation (2)",
        "Explanation": "The dimensionless parameter ε_GR governing the strength of GR-induced precession appears to differ (coefficient ‘8’) from standard expressions in the literature (e.g. factors of 3 and dependence on (1−e_out^2)^(3/2)). An incorrect normalization or missing dependence could significantly shift the boundaries of the so-called 'GR-reduced regime', altering which systems are predicted to undergo polar Kozai cycles."
      },
      {
        "Problem": "Unjustified assumptions in population synthesis of companions and initial inclinations",
        "Location": "Section 3, Population synthesis description",
        "Explanation": "The simulated companion populations assume isotropic mutual inclinations and fixed mass–semi-major axis distributions (Bryan et al. 2016; Duquennoy & Mayor 1991) without accounting for observational biases, dynamical stability beyond simple criteria, or correlations between eccentricity and inclination. These oversimplifications may overestimate the fraction of systems in the GR-reduced HEM regime and thus the efficiency of producing polar orbits."
      },
      {
        "Problem": "Predicted obliquity distribution does not quantitatively match observations",
        "Location": "Section 5, Discussion; Figure 4",
        "Explanation": "The mechanism predicts a roughly uniform final spin–orbit angle distribution between 50°–130°, centered at 90°, whereas the observed sample (Albrecht et al. 2021) is sharply peaked near 100° with σ∼20°. Without a quantitative comparison including selection effects, detection biases, and confidence intervals, the model’s ability to explain the observed 'perpendicular' population remains unsubstantiated."
      }
    ],
    "think_summary": [
      "**Analyzing distribution discrepancies**\n\nI've observed a peak around 100°, which contradicts the expectation of a flat distribution. It seems questionable whether the model can produce the observed peak without proper evidence supporting it. They mention a center at 90° based on predictions, while the actual observed center is at 100°. This mismatch raises concerns, especially since they overlook normalization and selection effects. They also assume isotropy in mutual inclinations, but that may not hold if inclinations from planet scattering are involved. There are several critical issues to consider, like neglecting octupole effects.",
      "**Identifying key issues in equations**\n\nEquation (1) is derived under the quadrupole approximation, but skipping octupole contributions could significantly affect inclination and eccentricity evolution. This leads to potential inaccuracies in the predicted obliquity distribution. \n\nNext, the assumption that final mutual inclination equals the minimum in the GR-reduced Kozai cycle overlooks tidal dissipation dynamics, which could distort obliquity estimates. \n\nThere's also uncertainty with the dimensionless GR parameter that may shift regime boundaries. \n\nI noticed isotropic initial inclinations and simplified companion distributions might lead to overestimations. Lastly, the predicted obliquity distribution differs notably from observations, questioning the validity of the model."
    ],
    "token_usage": {
      "input": 16691,
      "thinking": 2176,
      "output": 730
    }
  },
  {
    "entry_id": 135,
    "retraction_id": "1705.00151v2",
    "paper_id": "1705.00151v1",
    "retraction_comment": "We apologize that in the results and algorithms of Section 4.1, Proposition 4.1 and Lemma 4.2, there are some missing conditions and assumptions on the hypergraphs. Hence we want to withdraw the manuscript. Moreover, we want to work out more results, and merge this manuscript together to write a publishable paper",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Misapplication of Soulé’s torsion bound to arbitrary hypergraph homology",
        "Location": "Section 7, Equation (7.1) and Algorithms 9–10",
        "Explanation": "The bound cited from C. Soulé (\"Perfect forms and the Vandiver conjecture\") concerns torsion in very specific arithmetic complexes (e.g. Voronoi complexes for GL(n,Z)), not general simplicial complexes. There is no justification that the same inequality Sup{p | Torₚ Hₙ(K)≠0} ≤ b(n)/a(n) holds for the arbitrary associated simplicial complexes or embedded homology groups of hypergraphs, so the claimed finite prime bound is invalid."
      },
      {
        "Problem": "Flawed existence argument in Proposition 4.3 Step 2 when dim(H)=n",
        "Location": "Section 4, Proposition 4.3, Step 2",
        "Explanation": "The construction of H' adds n+2 distinct m-hyperedges τᵢ of size m+1 containing prescribed n-simplices σᵢ. When the hypergraph dimension N equals n, VH has only n+1 vertices, so one cannot form n+2 distinct n-hyperedges of size n+1, invalidating the proof in the case m=N=n."
      },
      {
        "Problem": "Unjustified homotopy invariance of embedded homology in Lemma 5.5",
        "Location": "Section 5.4, Lemma 5.5",
        "Explanation": "The lemma asserts that a homotopy equivalence f:K→K′ relative to K∩H′ induces an isomorphism on embedded homology H_*(K∪H′)→H_*(K′∪H′). However, neither the paper nor the cited results in [4] establish embedded homology’s functoriality under such relative homotopy equivalences, so the key step underlying Algorithm 5 and Proposition 5.4 lacks a firm foundation."
      },
      {
        "Problem": "Algorithm 5 implicitly requires constructing the full associated simplicial complex KH",
        "Location": "Section 5.4, Algorithm 5",
        "Explanation": "Deciding whether a hyperedge σ is “simplicial-like” (i.e. St_{K_H}σ⊆H) and testing if Link_{K_H}σ is a cone both require knowledge of all simplices in the closure of H under subsets. Building KH is exponential in the worst case, so the claimed efficiency of the collapse heuristic is undermined by this hidden exponential step."
      },
      {
        "Problem": "Overlooked vertex-sharing issue in path-component decomposition",
        "Location": "Section 5.3, Algorithm 4",
        "Explanation": "The paper decomposes H(n) into 'disjoint' path-connected components and then asserts Hₙ(H)=⊕ₖHₙ(H(n,k)). This relies on the components having no shared vertices so that their associated complexes are topologically disjoint. While path-connectedness in terms of hyperedges prevents hyperedges from touching across components, it does not obviously forbid components from sharing isolated vertices, which could spoil the direct-sum decomposition in homology."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 19506,
      "thinking": 6720,
      "output": 738
    }
  },
  {
    "entry_id": 136,
    "retraction_id": "1701.02653v2",
    "paper_id": "1701.02653v1",
    "retraction_comment": "This paper has been withdrawn to an error in Proposition 8 when moving from the quenched to the annealed measure. Thus, it is not a straightforward adaptation of the theorem cited",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Overstatement of reversibility in Lemma 11",
        "Location": "Lemma 11 (page 6)",
        "Explanation": "The authors claim that a unimodular random graph with finite expected degree is both stationary and reversible for the edge‐driven random walk.  In fact, unimodularity alone guarantees stationarity but not reversibility: to obtain reversibility one must bias the law of the root by its degree.  This misstatement does not affect the stationarity argument used later, but the assertion of reversibility is incorrect as stated."
      },
      {
        "Problem": "Insufficient justification of the coupling/measurability construction",
        "Location": "Section 2.2 (page 3–4)",
        "Explanation": "The paper sketches a recursive scheme to couple the Poisson arrow processes across all isomorphism classes to ensure joint measurability of (G,ρ) and the dynamics.  However, no formal proof is given that this construction yields a measurable map from the space of rooted graphs to the path space of the processes, nor that the resulting P and P^G_ρ are well‐defined conditional probabilities."
      },
      {
        "Problem": "Informal supermartingale argument in Proposition 12",
        "Location": "Proposition 12 (page 6–7)",
        "Explanation": "The bound on E[|ζ^(ρ)_t|] relies on a coupling that ignores arrows into the distinguished path and then claims the resulting ‘voter‐cluster size’ is a supermartingale with unit initial mass.  The boundary killing mechanism at the path is handled only heuristically, with no verification of the required optional‐stopping conditions or integrability to justify the supermartingale bound."
      },
      {
        "Problem": "Unverified extension of site‐recurrence criterion to random graphs",
        "Location": "Proposition 8 (page 4)",
        "Explanation": "The authors invoke [BFGG+16, Proposition 1.1] to assert that ∫_0^∞ p_t dt=∞ implies site recurrence, and then claim (without additional proof) that the same argument carries over to random graphs via Lemma 7.  But the original result relies on uniform integrability and tail‐$\\sigma$–fields for a fixed graph; its applicability under the joint law of (G,ρ) and the dynamics is not justified."
      },
      {
        "Problem": "Neglected integrability check in MTP application",
        "Location": "Lemma 9 (page 5)",
        "Explanation": "The mass‐transport argument sums the function F(G,ρ,x) over all vertices x and interchanges expectation and summation via Fubini.  Although cluster sizes are almost surely finite at fixed time, the paper does not explicitly verify that E[∑_x F(G,ρ,x)]<∞, which is needed to justify the interchange and the use of MTP."
      }
    ],
    "think_summary": [
      "**Evaluating Proposition 8 and MTP Application**\n\nI’m considering whether Proposition 8 applies to random graphs and referencing BGFG+16. The key step involves Lemma 9 and the application of MTP to find the probability P(|ζ^(ρ)|=n). There may be a potential issue with how MTP sums over vertices since F depends on G, ρ, and x. I need to check if the sums are finite. It seems that as long as F is positive and their expectations remain finite, MTP might indeed apply accurately here.",
      "**Reviewing Lemmas and Corollaries**\n\nAlright, so I'm considering Lemma 10, which indicates that Jensen's inequality holds—great! For Lemma 11, I'm referencing Aldous and Lyons regarding unimodular random graphs. They state that such graphs with finite expected degree are stationary and reversible, but their joint distribution is degree-biased. It's interesting that unimodularity implies stationarity but doesn’t guarantee reversibility without degree bias. I need to confirm this with the corollaries from AL07, especially focusing on how they relate stationarity and reversibility under certain conditions. More checking is needed!",
      "**Analyzing Lemmas and Clusters**\n\nI'm reviewing Lemma 9, which discusses using Fubini to swap sum and expectation for unimodular graphs. They need to address integrability, but since ζ cluster sizes seem integrable, it’s a bit tricky with infinite clusters. Even if a cluster is infinite, the focus is on finite sizes, keeping sums manageable. I'm intrigued whether P(cluster finite) could be less than 1, and while infinite configurations are possible, they aren’t considered. It looks like they’ve made solid assumptions, especially regarding the rates and expectations related to jumps and arrows in the model. A lot to digest, but looking good!",
      "**Identifying Critical Problems**\n\nI'm analyzing some key issues in the work, particularly focusing on measure continuity, where they argue that if \\( P(A)=0 \\) then \\( P^G_ρ(A)=0 \\) almost surely. This reasoning seems valid since they mention \\( A \\) being a tail event linked to joint laws. That's good! However, I see possible problems: 1) The claim in Lemma 11 about reversibility may be overstated; 2) There's a lack of rigorous justification for coupling in Section 2.2; 3) Proposition 12’s supermartingale argument isn’t fully addressed; 4) The extension of BFGG+16's Proposition 1.1 to random graphs may not hold; and 5) The MTP applications in Lemma 9 need further verification on integrability conditions. I’ll structure this as JSON now!"
    ],
    "token_usage": {
      "input": 6655,
      "thinking": 6528,
      "output": 661
    }
  },
  {
    "entry_id": 137,
    "retraction_id": "2011.07585v2",
    "paper_id": "2011.07585v1",
    "retraction_comment": "The article contains wrong idea. There was mistake in the H3 assumption",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Inner method DSGD does not satisfy the required linear convergence + variance bound (equation 3)",
        "Location": "Section 3.3 (application of property (3) to DSGD)",
        "Explanation": "The Catalyst framework demands that the inner solver M satisfy a bound of the form E[h(z_t)–h*] ≤ C(1–a)^t(h(z_0)–h*) + Bσ^2η for each t. The convergence result for DSGD from [2] is only an average‐in‐time guarantee (a weighted sum of errors) under decaying step sizes, not a per‐iterate linear convergence in expectation. Hence DSGD cannot be used as M in Catalyst as claimed, invalidating the acceleration analysis."
      },
      {
        "Problem": "Inconsistent definition and use of the surrogate function in the decentralized setting",
        "Location": "Section 3.3, equations (8)–(10)",
        "Explanation": "The paper defines H˜_k(X) on the matrix of node iterates X but then replaces it by H_k(x) on the average x without proving that optimizing H˜_k with DSGD is equivalent to optimizing H_k. The strong convexity and smoothness constants (κ+μ) used in the surrogate do not carry over transparently to the decentralized problem, so the step sizes and convergence rates derived are unjustified."
      },
      {
        "Problem": "Flawed proof of Lemma 2 (condition H3) leading to uncontrolled error accumulation",
        "Location": "Section 2, Lemma 2",
        "Explanation": "The transition from a bound on E[H_k(x_k)–h*_k] to E[f(x_k)] ≤ E[h*_k]+δ_k improperly absorbs the term f(x_{k−1})−f*, and δ_k is defined recursively without showing it remains small. There is no correct control of the approximation errors δ_k across outer iterations, so the claimed surrogate‐solve accuracy fails."
      },
      {
        "Problem": "Omission of network parameters in inner‐solve iteration complexity",
        "Location": "Section 4.1 (proof of assertion (6))",
        "Explanation": "When deriving the number of inner iterations t_k for solving each surrogate, the authors treat the Catalyst constants C and a as independent of the network mixing parameter p and delay τ. In reality, the linear convergence rate and constants for DSGD depend explicitly on p and τ, so the inner‐solve complexity should also reflect these dependencies. Omitting them leads to an overly optimistic total complexity."
      },
      {
        "Problem": "Claimed accelerated rate is actually worse than the original DSGD rate",
        "Location": "Section 3.3, equation (11) versus the bound on page 6",
        "Explanation": "The original DSGD complexity from [2] scales as Õ(σ̄^2/(μnε) + √(L(ζτ+σ√(pτ))/(μp√ε)) + Lτ/(μp)), i.e. the variance/network term is O(ε^(–1/4)). The new bound (11) has that term scaling as O(ε^(–1/2)), which is strictly slower. Thus, the method loses, rather than gains, acceleration in the variance‐dominated regime, contradicting the paper’s main claim."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 15486,
      "thinking": 2688,
      "output": 763
    }
  },
  {
    "entry_id": 138,
    "retraction_id": "1803.09392v2",
    "paper_id": "1803.09392v1",
    "retraction_comment": "This paper is withdrawn as the proof of Lemma 2.4 is incorrect",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Unjustified existence of the auxiliary set S satisfying (S1)–(S3)",
        "Location": "Section 2, just before Definition 2.1",
        "Explanation": "The argument relies on choosing a finite G-stable set of places S satisfying three non-trivial conditions (S1)–(S3) simultaneously: (S1) demands S-class number 1 for every intermediate field; (S2) demands the existence of global units z,m effecting a free OFG-submodule Fr controlling L; (S3) demands local analytic conditions for the exponential map at all v∈Sf. No proof or reference is given that such an S always exists even in the tame case. Failure of that existence would void the construction of the key modules Je(1), Fr and hence Proposition 2.6."
      },
      {
        "Problem": "Passage from K₀(ℤG) to Cl(ℤG) omits justification",
        "Location": "Proposition 2.6 and the proof of Theorem 1.5 (Section 3)",
        "Explanation": "Proposition 2.6 is stated in the Grothendieck group K₀(ℤG) and involves a ‘–[F:ℚ](ℤG)’ term, but the proof of Theorem 1.5 concludes Ω(N/F,2)=(A_{N/F}) in the locally free class group Cl(ℤG) without explicitly showing that subtracting a free summand [ℤG] vanishes in Cl(ℤG). The identification of trivial summands in passing from K₀ to Cl is assumed rather than proved."
      },
      {
        "Problem": "Undemonstrated equality of classes under central scaling by α",
        "Location": "Remark 2.2 and proof of Theorem 1.5",
        "Explanation": "They define L=α·A_{N/F} for a non-unit α∈O_F chosen to clear denominators, then assert (L)=(A_{N/F}) in Cl(ℤG). However, multiplication by a non-unit central element can change the isomorphism class of a locally free ℤG-module, and no argument is given that this operation yields the same class in Cl(ℤG)."
      },
      {
        "Problem": "Over-reliance on [6] without verifying its hypotheses",
        "Location": "Lemma 2.4 (Section 2)",
        "Explanation": "The proof of Lemma 2.4 is said to be “identical to [6, Lemma 2.2]” except for a minor remark. But [6] treats wildly ramified quaternion extensions of ℚ under very specific local conditions. There is no check here that all of [6]’s hypotheses (especially on residue characteristics and group structure) hold in the general tame N/F setting."
      },
      {
        "Problem": "Notation inconsistency (‘K’ vs ‘F’) in the final argument",
        "Location": "Section 3, proof of Theorem 1.5",
        "Explanation": "The proof concludes “Ω(N/K,2)=(A_{N/K})”, but the theorem and rest of the paper uniformly use N/F. This typo obscures which base field is intended and raises concerns about care in the final deduction."
      }
    ],
    "think_summary": [
      "**Identifying logical inconsistencies**\n\nThe user is focusing on critical errors and unsoundness in the provided paper structure, aiming for up to five points in a JSON list. From my analysis, there are issues regarding Theorem 1.5 and Proposition 2.6, specifically concerning the relationship between classes and wild terms. For example, substituting L=α·AN/F raises questions about the implications for class groups. There's also a lack of clarity on definitions in K0(ZG) compared to Cl(ZG), which needs addressing.",
      "**Analyzing module relationships**\n\nI'm looking at the definition of S, which includes all wildly ramified and archimedean components. In a tame extension, there are no wild elements, so Sf_wild is empty. According to Proposition 2.6, we find Ω = (L)-[F:Q](ZG). They state that L = α·AN/F and claim that the classes are equal; however, α being a possible non-unit adds complexity. The multiplication by α could affect module isomorphism, particularly with respect to the endomorphism ring. This raises questions about the class equality, which ultimately relies on whether the multiplication remains ZG-linear.",
      "**Identifying typographical issues**\n\nIn Problem 5, I'm noticing a typographical inconsistency that affects clarity. Specifically, in Section 3 of the proof for Theorem 1.5, the theorem refers to N/F, but the proof incorrectly uses Ω(N/K, 2) and AN/K, indicating a potential misprint regarding K. While this seems minor, it could lead to confusion about notation. Additionally, there's an unchecked assumption in Chinburg's Proposition 2.7; it addresses infinite v but treats finite and archimedean uniformly, which is also a minor concern."
    ],
    "token_usage": {
      "input": 4191,
      "thinking": 2176,
      "output": 759
    }
  },
  {
    "entry_id": 139,
    "retraction_id": "1709.04340v2",
    "paper_id": "1709.04340v1",
    "retraction_comment": "A problem with the proofs of Propositions 2 and 3 (a gap or fault in the reasoning used to claim that the expression in (3.9) is dominated by that in (3.11)); a similar problem with the proof of Proposition $1'$ (it is hard to justify the particular application of the Bourgain-Guth reduction theory implicit in a paragraph above Proposition $1'$). Theorems 1, 2 and 3 lose their status as theorems",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Unjustified extension to Proposition 1′",
        "Location": "Section 2, Proposition 1′ (Eq. (2.8))",
        "Explanation": "The authors claim that the same linear-to-multilinear reduction used in [BW17] to prove Proposition 1 also yields Proposition 1′ without any additional loss. However, no new argument is given to control the penalty in passing from L⁴–L^{24/5} and L^{2ν}–L³ mixed norms to the stated L^q bound. The key trilinear-to-linear reduction incurs nontrivial combinatorial factors which are not estimated here. Without a detailed verification, the bound (2.8) remains unproven, undermining all subsequent conclusions that rely on this strengthened estimate."
      },
      {
        "Problem": "Omitted proof of refined solution count in Lemma 2",
        "Location": "Section 3, Lemma 2 (Eq. (3.7))",
        "Explanation": "Lemma 2 asserts a bound on the number of integer solutions to system (3.5) under the additional diameter constraint (3.6), supposedly by a slight modification of the argument in [W90]. Yet no proof or even sketch is provided, and the diameter condition can significantly alter the Diophantine geometry. Since this estimate feeds directly into Lemma 3 and ultimately into Propositions 2 and 3 (hence Theorem 2), its validity is critical. Without a justification, the new bounds for the divisor and circle problems lack a firm foundation."
      },
      {
        "Problem": "Incomplete justification of the double large sieve application",
        "Location": "Section 5, derivation of Eq. (5.12)",
        "Explanation": "The transition from (5.11) to the large sieve bound (5.12) invokes the variant double large sieve from [BW17] with only a brief summary. Crucially, the required independence and amplitude‐support conditions on the coefficients α_{k,ℓ} and β_{a/q}—which now depend on both the I(a/q) classification and parameters t₁,t₂—are not checked. It is nontrivial to verify that the correlation sums B(A,Q;V) and the multimoment A_p still satisfy the hypotheses of [BW17,(5.5)]. Failing to establish these conditions invalidates the entire First Spacing Problem and the subsequent bounds on S."
      },
      {
        "Problem": "Unverified monotonicity in Q for optimization step",
        "Location": "Section 6, discussion around Eqs. (6.6)–(6.8)",
        "Explanation": "In reducing the bound (6.6) to the single‐parameter choice Q=R, the authors assert that the composite bound is monotonically decreasing in Q because 4/q<1−ε. However, no detailed check is provided that the intricate combination of factors involving L/K and (ηK²/L) maintains this monotonicity across the entire admissible range R≤Q<Q₅, especially when mixing the cases ν≤5 and ν≥6. If the bound were not actually minimized at Q=R, the derived exponents in Eqs. (6.7)–(6.8) could be too optimistic, undermining Theorem 3."
      },
      {
        "Problem": "Unsupported application of Proposition 1′ to the sum S* in Theorem 1",
        "Location": "Section 8, derivation of Eq. (8.7)",
        "Explanation": "When estimating the exponential sum S* in the proof of Theorem 1, the authors concede that Proposition 3 does not apply, and instead invoke Proposition 1′ for the perturbed cone sum. Yet they do not verify that the geometric and frequency‐spacing conditions required by Proposition 1′ (e.g. truncated cone structure, curvature bounds) hold for the phase function F(x)=log((m+h)/M)+O(1/T). In the absence of this check, the crucial bound (8.7) on S*/M remains unjustified, so the asserted improvement on the mean square of ζ cannot be guaranteed."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 25886,
      "thinking": 2368,
      "output": 933
    }
  },
  {
    "entry_id": 140,
    "retraction_id": "1106.5292v3",
    "paper_id": "1106.5292v2",
    "retraction_comment": "This paper has been withdrawn by the author due to a crucial error in equation (5)",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Incorrect signs and coefficients in the conformal-rescaling formulas for R_{ab} and R",
        "Location": "Eqs. (5),(6) and hence in (22),(23)",
        "Explanation": "The standard formulas for how the Ricci tensor and scalar transform under g_{ab}=e^{Φ} ḡ_{ab} carry opposite signs in the ∇_a∇_bΦ and □Φ terms (and different numerical factors) from those given in (5),(6). Because all subsequent substitutions into the Einstein equations use these wrong formulas, the claimed mismatch with the Jordan-frame field equations in Sec. 3.3 is an artifact of algebraic error, not a genuine inequivalence."
      },
      {
        "Problem": "Misuse of conformal transformations as a gauge symmetry argument",
        "Location": "Sec. 4.1 (“Conformal symmetry: gauge fixed vs unfixed”)",
        "Explanation": "Conformal rescaling between Einstein and Jordan frames is a local field redefinition, not a gauge symmetry of the Jordan-frame action. Treating the Einstein-frame form as a 'gauge fixing' of an alleged Weyl symmetry in SJ is conceptually incorrect; as a result, the non-commutativity argument between gauge fixing and variation does not apply to mere field redefinitions."
      },
      {
        "Problem": "Neglect of the Gibbons–Hawking–York boundary term and inconsistent dropping of surface terms",
        "Location": "Derivation of (9) and discussion in Sec. 4.2",
        "Explanation": "A well-posed variational principle for both Einstein- and Jordan-frame gravity requires adding the appropriate GHY boundary term. The paper omits these entirely and then asserts that different surface terms remain after conformal rescaling. In fact, if one includes and correctly transforms the GHY terms, the variational procedures commute with the field redefinition and yield equivalent equations of motion."
      },
      {
        "Problem": "Analysis limited to 'pure gravity' without matter makes frame‐equivalence questions moot",
        "Location": "Beginning of Sec. 2 (“Both Einstein frame gravity and Jordan frame gravity can couple to external matter fields. In this article, however, we shall consider only pure gravity…”) ",
        "Explanation": "The usual physical inequivalence debate between frames arises when matter couples to the metric (e.g. minimally in one frame vs. non-minimally in the other). By omitting all matter couplings, the authors remove the very context in which frame differences manifest, so their conclusions cannot be extended to realistic situations."
      },
      {
        "Problem": "Algebraic inconsistency in the derived Jordan-frame field equations",
        "Location": "Comparison between (20) and (24)",
        "Explanation": "Even setting aside boundary issues, one finds that the coefficients of ∇_a∇_b ϕ, □ ϕ and ∇_aϕ∇_bϕ in (24) do not match those in the correctly varied Jordan-frame equations (20). This discrepancy traces back to earlier computational mistakes (in the curvature transforms and/or the variation), so the claimed incompatibility is unfounded."
      }
    ],
    "think_summary": [
      "**Evaluating paper errors**\n\nThe user wants me to analyze the attached paper for critical errors that could invalidate its conclusions, specifically looking for up to five problems in a specified format. The paper by Xu & Zhao discusses the differences between the Einstein and Jordan frames. It states that while the equations of motion differ after a conformal transformation, I need to check if they considered matter couplings, as they focus only on pure gravity. There might be important aspects related to the equivalence of actions that need further examination.",
      "**Refining error entries**\n\nI’m identifying critical issues in the paper, aiming for clarity and precision. For Entry 1, there's a wrong sign in the conformal transformation of the Ricci tensor and scalar, located in equations (5), (6), (22), and (23). Entry 2 highlights a misinterpretation of the conformal transformation as gauge fixing in section 4.1. Entry 3 notes improper treatment of boundary terms, especially the Gibbons–Hawking–York term. Entry 4 points out the absence of matter coupling, crucial for frame equivalence, and Entry 5 mentions an algebraic error in deriving the Jordan frame equations. I'll structure these in JSON format."
    ],
    "token_usage": {
      "input": 5905,
      "thinking": 2496,
      "output": 715
    }
  },
  {
    "entry_id": 141,
    "retraction_id": "1903.00526v2",
    "paper_id": "1903.00526v1",
    "retraction_comment": "An error occurs in Section 5. Post-measurement results in the RTO experiment are improperlystated to directly apply to the entangled measurement state itself. This puts the conclusions stated in the abstract into question",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Conflation of improper and proper mixtures leading to claimed collapse at entanglement",
        "Location": "Section 3, Equations (3)–(4), page 6",
        "Explanation": "The paper treats the reduced density operators rA and rB obtained by tracing out one subsystem as representing actual ('ontological') mixtures and asserts that collapse occurs upon entanglement. In standard quantum theory these are 'improper' mixtures that do not correspond to definite outcomes in individual runs. Decoherence alone does not transform the global pure state into a proper ensemble or select a single outcome, so the claimed collapse at the moment of entanglement is unjustified."
      },
      {
        "Problem": "Semantic redefinition of product states does not address outcome selection",
        "Location": "Section 2, pages 4–5",
        "Explanation": "The core claim—that the tensor‐product state |A1⟩|B1⟩ must be read as 'A1 if and only if B1' rather than 'A1 and B1'—is purely interpretational. Recasting product states as correlations does not resolve how and why a particular branch is realized in an individual measurement, nor does it dispense with the need for a physical collapse or an interpretation that selects one outcome. Thus the measurement problem remains."
      },
      {
        "Problem": "Unjustified assertion that entanglement’s nonlocality single‐handedly selects one outcome",
        "Location": "Section 3, paragraph beginning 'Returning to the measurement analysis…', page 7",
        "Explanation": "The paper claims that the nonlocal correlations inherent in entanglement 'ensure a single outcome occurs while the other outcomes don’t,' yet provides no dynamical or interpretational mechanism for this selection. Standard quantum mechanics with unitary evolution does not single out one branch; additional postulates or interpretations are required. This leap invalidates the purported solution."
      },
      {
        "Problem": "Misuse of decoherence and thermodynamic irreversibility to claim fundamental collapse",
        "Location": "Section 3, final two paragraphs, page 7",
        "Explanation": "Appealing to thermodynamic irreversibility 'for all practical purposes' (FAPP) conflates practical unpredictability with an objective, irreversible physical process. While environmental decoherence and amplification destroy observable phase coherence, they do not produce a true wave‐function collapse or explain the transition from quantum superposition to a single definite record at the fundamental level."
      },
      {
        "Problem": "Overinterpretation of RTO interferometry as dictating semantics of product states",
        "Location": "Section 2, page 5",
        "Explanation": "The experiments show that local reduced states lack phase coherence while nonlocal correlations exhibit interference, in full agreement with textbook quantum mechanics. They do not force a reinterpretation of the mathematical structure of the tensor product or impose a new logical reading ('if and only if') on product states. The semantic shift is neither necessitated by nor supported strictly by the data."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 9332,
      "thinking": 1280,
      "output": 676
    }
  },
  {
    "entry_id": 142,
    "retraction_id": "1503.07411v2",
    "paper_id": "1503.07411v1",
    "retraction_comment": "This paper has been withdrawn due to a gap in the proof of Proposition 2.19",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Misapplication of Corti’s inequality in Proposition 2.19",
        "Location": "Proposition 2.19, page 11–12",
        "Explanation": "The authors invoke Corti’s Theorem 3.12 to bound multiplicities of the cycle Z at centers p_i, but do not verify the necessary hypotheses: namely that the movable linear system H has no fixed components meeting the centers improperly, and that the pair (X,1/μ H) is klt away from the centers. In the non-Gorenstein setting with quotient singularities, these conditions can fail, invalidating the key inequalities used to conclude f must be square."
      },
      {
        "Problem": "Unjustified extension of the Bertini involution to a pseudo link that untwists maximal centers",
        "Location": "Lemma 2.15, page 9",
        "Explanation": "The construction of the fiberwise involution σ_Γ relies on the generic fiber involution, yet no argument is given to ensure σ_Γ extends to a pseudo link of type II_h^dP that untwists arbitrary movable systems (as required by Definition 2.8). In particular, its effect on the quasieffective threshold μ and fiber coefficient ν of H is not established, so one cannot conclude it reduces (μ,ν) as needed for the inductive 'untwisting' argument in §2.4."
      },
      {
        "Problem": "Use of intersection theory on a weighted hypersurface without justifying Cartier conditions",
        "Location": "Lemma 2.12, page 6",
        "Explanation": "The argument produces a Weil divisor Ξ on a surface S⊂P(1,1,1,2) and computes (Γ·Ξ) and (–K_S·Ξ) by naive intersection counts. However, in P(1,1,1,2) divisors of degree 1 need not be Cartier, and neither is Ξ shown to be Q-Cartier. Without verifying that these intersections are well-defined in the orbifold sense, the inequality (Γ·Ξ)≥(–K_S·Ξ) may be invalid."
      },
      {
        "Problem": "Exclusion criterion for maximal singularities applied without checking relative Picard conditions",
        "Location": "Lemmas 2.21–2.22, page 13",
        "Explanation": "Lemmas 2.21 and 2.22 extend the curve‐exclusion criterion to arbitrary Mori fiber spaces X/S, but the proof tacitly assumes that the movable system H is relatively free over S and that the extraction φ: Y→X has relative Picard number 1 over S. These assumptions are essential in the original argument to ensure negativity of (H–μK_Y) on the exceptional curves, and they are not verified here, undermining the exclusion of singular points as weak maximal centers."
      },
      {
        "Problem": "Picard number computation for the toric image Q_n may fail due to non-quasi-smoothness",
        "Location": "Lemma 3.2, page 14–15",
        "Explanation": "The authors assert that Q_n is a quasi-smooth hypersurface in a simplicial toric variety and invoke Roan’s result to conclude ρ(Q_n)=1. But Q_n has a two-dimensional singular locus along the contracted plane Π of type ℙ²×1/n(1,1), which may prevent Q_n from being a V-submanifold or quasi-smooth in the sense required. Without establishing these properties, the Picard number argument is invalid, and subsequently the extremality of π: X_n→ℙ¹ is in doubt."
      }
    ],
    "think_summary": [
      "**Identifying critical problems**\n\nThe user is asking me to identify up to five major issues in a math article that could invalidate its conclusions. I need to focus on serious errors rather than minor ones. I can't verify all proofs due to time constraints, but I can look for common critical issues like incorrect lemma usage or circular reasoning.\n\nPotential problems include whether they’ve adequately addressed non-Cartier divisors in Lemma 2.12. Additionally, Lemma 2.14 may be questionable due to assumptions about multiplicity, and Proposition 2.19 might misapply Corti's inequality.",
      "**Analyzing mathematical assumptions**\n\nThey use Cox's proposition, but I’m questioning whether Qn is indeed Q-factorial, as they argue it's toric through a specific morphism. The kernel appears to be of rank 1. Regarding the K-condition, they assert that K2 implies it, which seems true. In Lemma 3.3, they state that if f=0 is nonsingular, the fibers must also be smooth. However, singular cases arise when certain parameters are zero, raising contradictions with their claims about del Pezzo fibrations. \n\nRegarding terminal and Q-factorial status, they rely on cDV classification, which I need to verify. There's also a mention of curves being contracted or positioned correctly, ensuring they have enough variations. I’ll need to see if they adequately meet the criteria outlined in Lemma 2.21."
    ],
    "token_usage": {
      "input": 17269,
      "thinking": 1920,
      "output": 826
    }
  },
  {
    "entry_id": 143,
    "retraction_id": "1602.03949v2",
    "paper_id": "1602.03949v1",
    "retraction_comment": "This paper has been withdrawn by the author due to the different description of second-order correlation",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Undefined ghost imaging reconstruction algorithm and O(x,y)",
        "Location": "Section 2 (after Eq. 1)",
        "Explanation": "The paper never specifies how the reconstructed image O(x,y) is computed from the measured bucket and pixel signals (e.g., what correlation or normalization is used). Without an explicit reconstruction formula, the experimental SNR values cannot be reproduced or independently validated, calling into question the reported improvements."
      },
      {
        "Problem": "Use of a single CMOS camera for both bucket and spatial detectors",
        "Location": "Section 2, Experimental Setup (Fig. 1)",
        "Explanation": "Both the bucket detector and the imaging detector are realized by summing regions of the same camera chip, so electronic readout noise, fixed-pattern noise, and cross-talk are common to both signals. Such shared noise sources can introduce artificial correlations that inflate the measured SNR enhancements attributed to filtering."
      },
      {
        "Problem": "Assumption of uncorrelated background from halogen lamp",
        "Location": "Section 2, paragraph describing mixing of pseudo-thermal and halogen light",
        "Explanation": "The background thermal source is delivered via a fiber illuminator, which typically produces spatial speckle and finite temporal coherence. This speckle can generate its own intensity correlations, violating the assumption that the halogen lamp contributes only uncorrelated noise. Unaccounted background correlations could skew the reported SNR improvements."
      },
      {
        "Problem": "Nonstandard and potentially misleading SNR definition",
        "Location": "Equation (1), Section 2",
        "Explanation": "The SNR is defined using the sum of the binary object pattern S(x,y) as the 'signal' and the root-mean-square error between S(x,y) and O(x,y) as the 'noise', resembling a PSNR metric. This metric ignores actual intensity levels and background offsets, making comparisons across different filter bandwidths ambiguous and potentially overstating the filtering benefit."
      },
      {
        "Problem": "Neglect of filter effects on pseudo-thermal source coherence properties",
        "Location": "Section 2, Experimental Setup describing filter wheel",
        "Explanation": "While filters are described as 'transparent at 532 nm', their finite bandwidth can alter the temporal and spatial coherence of the pseudo-thermal light (e.g., changing speckle grain size and coherence time). Such changes can independently affect ghost imaging performance, confounding the attribution of SNR gains solely to background suppression."
      }
    ],
    "think_summary": [
      "**Evaluating potential flaws**\n\nThe user is asking me to identify critical errors in a paper on improving SNR in ghost imaging with narrowband filters. I need to scrutinize for theoretical, experimental design, or analysis mistakes. \n\nOne possible issue is the definition of SNR, which seems unconventional: they define SNR using pixel sums rather than actual signal intensity. This resemblance to peak signal-to-noise ratio could lead to misinterpretation, especially since they don't consider background offset. Also, there's a claim stating that narrowing the filter increases SNR.",
      "**Scrutinizing potential oversights**\n\nI'm wondering about the correlation between pseudo-thermal light coherence time and the findings. They integrate short snapshot times but consider a background that's wideband. The comparison of a 0.1 nm pseudo-thermal source to an 80 nm background suggests an improvement in SNR, but they don't address how filtering might decrease light's coherence length or the loss of power from the filters. It looks like they assume the laser is narrow enough, yet the filter's FWHM can still impact pseudo-thermal light. Lastly, there seems to be some confusion about how they treat the camera with signal arms interacting with the filter."
    ],
    "token_usage": {
      "input": 4606,
      "thinking": 2496,
      "output": 565
    }
  },
  {
    "entry_id": 144,
    "retraction_id": "2203.01307v2",
    "paper_id": "2203.01307v1",
    "retraction_comment": "Lemma 2.1 is true for Heisenberg type groups, but in general not for M_tivier groups, cf. Eq. (2.4) of M_ller and Stein [MS94]",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Divergent geometric series in the final summation",
        "Location": "Section 6, Proof of Proposition 6.1, Step (5)",
        "Explanation": "In Step (5) one estimates\n\n  ||g(1)_≤||_p^p  ≲  ∑_{ℓ=0}^ι (2^{ℓ(d₁−d₂)/q + 2ιd₂/q + γi d₁/q})^p ||F^(ι)||₂^p ||f||_p^p,\n\nbut since d₁>d₂ and γ>0 the exponent (d₁−d₂)/q > 0, so the sum over ℓ grows like 2^{ι(d₁−d₂)/q} rather than decaying.  One cannot then extract a factor 2^{-ε ι} as claimed, and the geometric series diverges rather than converges.  This breaks the final L^p–bound and invalidates the conclusion of Proposition 6.1."
      },
      {
        "Problem": "Mis-scaled cutoff in the multiplier localization",
        "Location": "Section 6, Proof of Proposition 6.1, Step (2)",
        "Explanation": "They introduce an even bump ψ supported in |λ|∈[1/8,8] and then decompose\n\n  F^(ι)(λ)  =  F^(ι)(λ)·ψ(λ)  +  F^(ι)(λ)·(1−ψ(λ)).\n\n—but F^(ι) is supported at |λ|∼2^ι, so for large ι the product F^(ι)(λ)·ψ(λ) vanishes identically.  The intended localization was in a dyadic window around λ∼2^ι (i.e. ψ(λ/2^ι)), not around λ∼1.  As written, the decomposition annihilates all but finitely many ι and the subsequent use of the Mikhlin–Hörmander theorem on F^(ι)(1−ψ) is vacuous."
      },
      {
        "Problem": "Incorrect scaling in the definition of the Hermite operator H_μ",
        "Location": "Section 5, beginning of proof of Proposition 5.1",
        "Explanation": "They define the “rescaled Hermite operator” by\n\n  H_μ  =  −Δ_z  +  |z|²/|μ|²,\n\nwhereas the correct operator arising from the twisted Laplacian L_μ is\n\n  H_μ  =  −Δ_z  +  |μ|²·|z|².\n\nThis inversion of the μ–scaling invalidates the application of the weighted estimate from [CO16, Prop. 3.3] and thus the weighted Plancherel estimate (5.1)."
      },
      {
        "Problem": "Neglect of the μ=0 (central‐frequency) contribution",
        "Location": "Proposition 2.2 and throughout the U‐truncation in (1.4)",
        "Explanation": "All formulae for F(L,U) and the truncation along spec U (i.e. the decomposition in ℓ via χ_ℓ(L/U)) integrate only over μ≠0 and set F_ℓ(λ,ρ)=0 for ρ=0.  This omits the entire μ=0 part of the spectral decomposition (the trivial central character), which in general contributes a nontrivial Euclidean sub–Laplacian on g₁.  Without handling that piece separately, the operator decomposition and estimates are incomplete."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 21706,
      "thinking": 7360,
      "output": 832
    }
  },
  {
    "entry_id": 145,
    "retraction_id": "2112.10980v2",
    "paper_id": "2112.10980v1",
    "retraction_comment": "There is an error in the proof of the co-primality statement in Proposition 6. The author has constructed examples of knots with integer surgeries so that the orders of the groups generated by these knots and their surgery duals have a non-trivial common factor, so in fact the co-primality statement in Proposition 6 is false",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Misapplication of Gainullin’s Dehn surgery characterization",
        "Location": "Introduction, page 1, paragraph 3",
        "Explanation": "The paper applies Gainullin’s Theorem 8.2 to conclude that a nullhomologous knot κ in a Heegaard Floer L-space Σ(K) must bound a disk (hence be the unknot), but Gainullin’s result is proved only for knots in integral homology sphere L-spaces. In general Σ(K) is merely a rational homology sphere, so the theorem does not guarantee the stated surgery characterization of the unknot in this setting."
      },
      {
        "Problem": "Ambiguous definition of self-intersection number in a manifold with boundary",
        "Location": "Section 2, Proposition 6 (page 3–4)",
        "Explanation": "The argument identifies the self-intersection ι(Σˆ_K,Σˆ_K) of the closed surface Σˆ_K in the 4-manifold W with the integer p arising in the long exact sequence, but gives no construction of a transverse pushoff or justification that the intersection form on H₂(W) is well-defined when ∂W≠∅. Without a careful treatment of relative homology or the normal bundle framing near ∂W, the equality |p|=|K||K*| is not rigorously established."
      },
      {
        "Problem": "Unjustified splitting H₂(W,∂W)≅H₂(W)⊕H₁(W)",
        "Location": "Section 2, just after equation (2) (page 3)",
        "Explanation": "The paper invokes the Universal Coefficient Theorem to assert H₂(W,∂W)≅H₂(W)⊕H₁(W), but does not verify the vanishing of higher Ext terms or check that H₃(W) has no torsion that could contribute to Ext². This splitting is critical to identify the map A and compute the order of its image, yet remains unproven."
      },
      {
        "Problem": "Incorrect assumption on the effect of −2–surgery on the meridian m",
        "Location": "Section 2, Theorem 7 (page 4)",
        "Explanation": "Theorem 7’s proof begins by performing −2–surgery on the meridian m of K in Y and asserting the result is Y#RP³. This holds only if m is null-homologous in Y. When K represents a nontrivial torsion class in H₁(Y), its meridian may also represent torsion, so −2–surgery need not produce RP³ or a connected sum with Y, invalidating the subsequent computation of |H₁(W)|."
      },
      {
        "Problem": "Failure to rule out torsion in H₂(W)",
        "Location": "Section 2, Proposition 4 and subsequent discussion (page 2–3)",
        "Explanation": "Proposition 4 concludes H₂(W)≅ℤ by constructing a single generator [Σˆ_K], but does not exclude the possible existence of torsion in H₂(W). If H₂(W) had a nontrivial torsion summand, the entire analysis of the intersection form, exact sequences, and the conclusion that H₂(W) is infinite cyclic would break down."
      }
    ],
    "think_summary": [
      "**Identifying critical problems**\n\nThe user is looking for the five most critical problems in this paper as a structured JSON list. I’ll need to focus on significant errors that may compromise the conclusions, while overlooking minor typos. The main claim proposes that certain alternating knots cannot have cosmetic crossings, needing to show one knot is null-homologous. I’ve outlined specific propositions and theorems that may contain errors, particularly in calculations and logical justifications. I’ll want to address miscalculations and ensure the justification step aligns accurately.",
      "**Questioning justification of arguments**\n\nI suspect there's a critical flaw in the use of Poincaré-Lefschetz duality and the universal coefficient trick. It seems they overlook the justification for certain torsion elements and make assumptions about intersection numbers that may not hold. The surfaces involved might not be disjoint or properly embedded, affecting the validity of their calculations. In Proposition 6, the identification of the self-intersection number lacks rigorous support. They need to consider the boundaries and the definition of intersection pairings, especially in non-simply-connected manifolds.",
      "**Examining critical assumptions**\n\nThey need to focus on relative classes rather than absolutes, as the self-intersection in manifolds with boundaries remains ambiguous. The conclusion about coprimeness of |K| and |K*| depends on certain integers existing, which could be influenced by their dependence on torsion surfaces. A significant point is that Theorem 2 claims the knot is null-homologous, but Theorem 7 only applies under specific conditions. The calculations of the determinants are tricky, and while many determinants for knots tend to be odd, there are instances where they are even. This challenge throws into question certain assumptions about surgeries, particularly in the handling of knot invariants."
    ],
    "token_usage": {
      "input": 5003,
      "thinking": 6400,
      "output": 758
    }
  },
  {
    "entry_id": 146,
    "retraction_id": "2006.16461v2",
    "paper_id": "2006.16461v1",
    "retraction_comment": "withdrawn due to an error in Lemma 4.1",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Unjustified transfer of bypass from meridian disk to torus boundary",
        "Location": "Proposition 3.8",
        "Explanation": "The authors assert that any boundary-parallel dividing curve on the convex meridian disk D induces an interior bypass on the torus boundary ∂M. While Proposition 2.17 guarantees a bypass half-disk on D, there is no rigorous argument showing that this lifts to a valid bypass attachment along ∂M. Without a clear construction or verification of the required Legendrian arc on ∂M, the key inductive step in reducing n fails."
      },
      {
        "Problem": "Over–reliance on uniform bypass existence in inclusion–exclusion",
        "Location": "Lemma 4.2 (proof)",
        "Explanation": "The inclusion–exclusion argument sums over 2n ‘bypass centers’ α, implicitly assuming that each tight contact structure on (n,−p,q) admits a bypass at every one of these positions and that the union of the images of the maps Bₐ covers the entire set Tₙ. However, the existence of bypasses may vary between contact structures, and the paper does not verify that each Bₐ(Tₙ₋₁) has the same domain or that their union is exactly Tₙ. This gap undermines the validity of the derived recurrence."
      },
      {
        "Problem": "Insufficiently rigorous argument excluding adjacent bypasses",
        "Location": "Proposition 3.10",
        "Explanation": "The authors claim that two boundary-parallel bypasses centered at adjacent dividing curves cannot both exist because their joint attachment yields a homotopically trivial dividing loop on ∂M. The proof sketches an isotopy collapsing the new curve but does not rigorously show that the resulting loop violates Giroux’s tightness criterion on the torus or that no further isotopies could avoid this trivial loop. The lack of a detailed dividing-set analysis leaves this exclusion unjustified."
      },
      {
        "Problem": "Ambiguous base-case identification for n=0 in closed-form solution",
        "Location": "Section 4.2, final proof of Theorem 1.1",
        "Explanation": "To solve the recurrence, the authors define N(0,p,q) by fiat as N(1,p₀,q₀) (with (p₀,q₀) from a shifted continued fraction). They then identify this with the parameter s in the closed formula. However, there is no general proof that reducing (1,−p,q) by a bypass always yields exactly the dividing set parametrized by (1,−p₀,q₀) with the desired continued-fraction properties. Without verifying this correspondence in all cases, the matching of initial conditions may fail and invalidate the closed-form expression."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 15932,
      "thinking": 6272,
      "output": 630
    }
  },
  {
    "entry_id": 147,
    "retraction_id": "1811.02204v3",
    "paper_id": "1811.02204v2",
    "retraction_comment": "Some arguments in the proof of Thm. 2.3.3 are erroneous. One of the faulty arguments lies in the estimate on the first line of page 24. The author mistakenly treats the orthogonal decomposition with respect to the unweighted inner product as the one with respect to the weighted one. Contents which are free from irreparable errors are contained in arXiv:1912.08076",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Use of the twisted Bochner–Kodaira inequality with only partial control of negative curvature terms",
        "Location": "Section 2.3 (eqns (2.3.2)–(2.3.3))",
        "Explanation": "In the construction for higher-codimension lc-centres the curvature form Θ acquires a negative summand –Λ(ψ)i∂ψ∧∂ψ.  The paper asserts that this can be controlled on the support of θ′_ε and made arbitrarily small as ε→0, but never produces a global semi-positive lower bound on Θ.  Yet the L² ∂-solution via the Bochner–Kodaira technique requires Θ ≥ 0 (or Θ ≥ –small × ω) everywhere on the complete manifold X°, and a uniform estimate of its negative part.  Without a rigorous uniform positivity or explicit bound one cannot justify the fundamental L² existence step in Theorem 2.3.3."
      },
      {
        "Problem": "Rigor of the ∇(1,0)-Bochner–Kodaira formula on the Poincaré-type metric in Lemma 2.3.4",
        "Location": "Lemma 2.3.4 (proof, eq (2.3.6))",
        "Explanation": "To bootstrap integrability of a local (n,0) form across {z₁=0}, the author introduces a ‘complete’ Kähler metric ω=ω_e + i dz₁∧d\bar z₁/|z₁|² and then applies a ∇(1,0)-Bochner–Kodaira identity.  But this metric has genuine Poincaré singularities along {z₁=0}, and the standard derivation of that formula assumes a smooth metric.  No justification is given that all integration by parts and curvature terms remain valid under these singularities, so the extension argument across the divisor lacks a firm analytic foundation."
      },
      {
        "Problem": "Unjustified identification of multiplier ideals under log-resolution",
        "Location": "Section 2.1 (eq (2.1.1))",
        "Explanation": "The key reduction to the snc case rests on the claimed equality\n  K_X_e ⊗ π*L ⊗ R^(–1) ⊗ 𝓘(π*φ_L – φ_R + m π*ψ)\n= π*(K_X⊗L)⊗E⊗𝓘(φ_L+mψ)\nas coherent sheaves on X_e.  The construction of R as 'the maximal divisor making π*φ_L+m_1ψ–φ_R psh' is never justified, nor is it shown that π₁₎ preserves jumping numbers or that higher direct images of these ideals vanish.  Without a precise algebro-analytic proof one cannot guarantee that one may work entirely on the snc resolution and then descend sections and L² estimates back to X."
      },
      {
        "Problem": "Ill-definedness of the lc–measure outside the local snc model",
        "Location": "Definition 1.3.1 and Section 3",
        "Explanation": "The central 'lc–measure' on log-canonical centres is defined as a delicate ε→0 limit of weighted integrals on X.  In Section 3 the limit is computed only under the assumption that φ_L and ψ pull back to an snc divisor on some chart.  However, the main theorems are stated for arbitrary neat analytic singularities (with only a reference to a blow-up argument).  There is no general proof that the limit exists, is independent of the choice of local trivialisation or of the extension f_e, nor that it descends from the resolution back to X, so the very weight against which the L² norms are measured may be ill-defined in the claimed generality."
      },
      {
        "Problem": "Multiple limit and subsequence arguments lack uniform control and may fail to produce a valid holomorphic extension",
        "Location": "End of Section 2.2 and Section 2.3 (passage ε→0, ε′→0, α→0, b→0, A→∞, B→1⁺)",
        "Explanation": "The construction of the final extension F requires taking several nested limits (first ε→0 to kill error terms, then subsequences for weak convergence, then α→0, b→0, A→∞, B→1⁺ to optimize constants) and invoking Montel/Fatou at each stage.  No uniform estimate is provided that survives all these passages simultaneously, nor an explicit diagonal argument to produce a single sequence along which F converges holomorphically and still satisfies the required congruence modulo the higher‐jump ideal.  Without such a uniform construction the existence and L²‐estimate for the final F remain unproven."
      }
    ],
    "think_summary": [
      "**Analyzing critical errors**\n\nThe user wants me to check an attached paper for significant issues that could invalidate its conclusions, while ignoring minor problems. I should focus on the introduction, statements, proofs, and methodology to pinpoint serious flaws or gaps. My final output will include up to five critical problems formatted as a JSON object, detailing the problem, its location in the paper, and an explanation. This will help ensure the user can properly address any major concerns.",
      "**Identifying critical errors**\n\nThe user seems to be analyzing the lc-measure's validity, noting that its correctness outside the specific case isn't proven without certain assumptions. They're emphasizing the need to highlight critical errors that could invalidate conclusions. Key points include the use of complete metrics and controlling negative curvature, as the assumptions around b could lead to blow-ups when b approaches zero. They also raise concerns about whether L2 existence holds with altered metrics. It seems there are significant issues related to completeness and assumptions that may undermine the paper's structure.",
      "**Identifying critical problems**\n\nI'm sketching out potential critical issues within the paper. Entry one highlights the application of the Bochner-Kodaira inequality, noting that the curvature form's negativity isn't justified for the inequality's requirements. Entry two mentions Lemma 2.3.4, which lacks rigorous justification regarding the validity of the Bochner-Kodaira formula for the singular metric. Entry three points out that the control of the divisor linked to multiplier ideal sheaves isn't adequately justified. Entry four raises doubts about the independence of lc-measure definitions. Entry five discusses the limit processes and convergence that aren't rigorously proven, which creates uncertainty about the final results. I'll summarize these in JSON format now."
    ],
    "token_usage": {
      "input": 41330,
      "thinking": 4416,
      "output": 1120
    }
  },
  {
    "entry_id": 148,
    "retraction_id": "1303.6535v2",
    "paper_id": "1303.6535v1",
    "retraction_comment": "Crucial flaw in proof of Theorem 3. The argument only gives a lower bound, not purity as claimed (the latter most likely does not hold except for in small rank)",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Unjustified Hodge–structure on Ext groups",
        "Location": "Page 1, second paragraph under “Ext•(∆v, ∆w)=…”, and immediately following",
        "Explanation": "The paper treats the vector‐space isomorphism Ext•(∆v,∆w)≅Hc•(Cv∩Cw) as an isomorphism of (mixed) Hodge structures, but neither [RSW] nor [BGS] establish that this identification is Hodge‐compatible. Category O and its Verma modules are a priori algebraic/D-module objects, not Saito Hodge modules, so one cannot transfer Deligne’s mixed Hodge structure from Hc to Ext without a separate construction or argument in the framework of mixed Hodge modules."
      },
      {
        "Problem": "Misapplication of [RSW] (modular Koszul duality) in characteristic 0 Hodge setting",
        "Location": "Page 1, citation “By [RSW, Proposition 4.2.1] and [BGS, Proposition 3.5.1]”",
        "Explanation": "[RSW] develops Koszul duality for modular perverse sheaves (coefficients in characteristic p), not for rational Hodge modules on the complex flag variety. Invoking it to identify Ext in characteristic 0 and to import Hodge structures is not justified."
      },
      {
        "Problem": "Invalid base case for purity induction",
        "Location": "Page 2, start of proof of Theorem 4 (“If w is the longest element, this is [M, Theorem 32]”) ",
        "Explanation": "Mazorchuk’s Theorem 32 computes dimensions of Ext^1(∆v,∆w0), but makes no reference to any Hodge or weight structure. It cannot serve as a base case for purity‐of‐weight arguments without first endowing those Ext groups with a bona fide Hodge structure and proving it pure."
      },
      {
        "Problem": "Typographical/logical error in Corollary 5, case (iii)",
        "Location": "Page 2–3, Corollary 5, third bullet (“if vs > w and vs ≤ ws”)",
        "Explanation": "The condition “vs > w and vs ≤ ws” is self-contradictory (since ws<w implies vs<ws<w), so the recursion omits the intended case vs>v and vs≤ws. As stated, this case never applies, invalidating the completeness of the recursive formula."
      },
      {
        "Problem": "Unjustified preservation of purity under long exact and Künneth sequences",
        "Location": "Pages 1–2, repeatedly in the proofs of Corollaries 2–3 and Theorem 4",
        "Explanation": "The arguments assume that long exact sequences in compactly supported cohomology of affine varieties and the Künneth isomorphism are strictly compatible with mixed Hodge structures so as to preserve or detect purity in each degree. Neither the necessary strictness assertions nor compatibility with Tate twists in this setting are established."
      }
    ],
    "think_summary": [
      "**Identifying logical contradictions**\n\nI see a real logical flaw in the conditions laid out in (iii). It’s contradictory to have vs > w and vs ≤ ws with ws < w since that implies vs < ws < w, which makes vs > w impossible. They likely meant to state vs > v instead. This misstatement in Corollary 5 really matters because it affects the formula and recursion. Additionally, the surprising dimension equality could fail in type B — that's a consideration, but I think it might work out fine.",
      "**Questioning algebra purity assumptions**\n\nThey conclude that Ext^i isn't pure, leading to algebra not being generated in degrees 0 and 1. They reference Boe's work about negative signs in KL R-polynomials affecting the Hodge Euler polynomial's positivity. However, they assume that if algebra generates in degrees 0 and 1, higher Ext must be pure, which is incorrect. For Koszul algebras, while degrees 0 and 1 provide some structure, it doesn’t guarantee all higher products remain pure. This creates problems with their final assertion regarding purity in higher Ext groups.",
      "**Highlighting Hodge structure assumptions**\n\nThe paper assumes that Ext groups have rational Hodge structures through geometric realization, but it fails to establish that these isomorphisms are compatible. The Ext groups in category O are seen as purely algebraic, without proof that they originate from mixed Hodge modules, making weight and purity statements questionable. The identification of Ext with cohomology does not maintain Hodge structure, and without embedding into the category of mixed Hodge modules, subsequent arguments about purity lack a solid foundation, exposing a critical flaw in the paper."
    ],
    "token_usage": {
      "input": 2106,
      "thinking": 4160,
      "output": 716
    }
  },
  {
    "entry_id": 149,
    "retraction_id": "1204.0036v3",
    "paper_id": "1204.0036v2",
    "retraction_comment": "This paper has been withdrawn by the authors because Lemma 11, for p>1, is incorrect",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "The algorithm is never explicitly specified, in particular the choice of step sizes Δt is only asserted to exist but not constructively given",
        "Location": "Section 1.3 (description of Smale’s algorithm reconsidered)",
        "Explanation": "All of the complexity bounds depend on being able to choose time–step increments Δtk so that the projective Newton continuation stays within the quadratic convergence region. While prior work is cited, no explicit, uniform rule for selecting these Δtk is provided in this paper. Without a constructive step‐size strategy one cannot claim a realizable polynomial‐time algorithm, only an existential bound."
      },
      {
        "Problem": "Use of the coarea formula on maps with boundary and noncompact domains without justification",
        "Location": "Section 3 (Proof of Theorem 1, around the application of the co‐area formula to Φt)",
        "Explanation": "The paper applies the coarea (or pushforward) formula to the restriction Φt: H(d)×P(Cⁿ⁺¹)→V, yet H(d) is noncompact and the parameter t∈[0,1] has boundary, and Φt may fail to be proper near t=0,1. No verification is given that all regularity and properness hypotheses of the coarea theorem are satisfied, making the change of variables potentially invalid."
      },
      {
        "Problem": "Extremely intricate and unverified normal‐Jacobian computation in Proposition 3",
        "Location": "Section 3.1 (Proposition 3)",
        "Explanation": "A central step rewrites NJΦt(f,ζ) as t^{2n}·Jac Ĥt(ζ)·NJπ₁(h,η). This delicate determinant calculation involves multiple adjoints and block‐matrix decompositions; any sign or exponent error in the t^{2n} factor would invalidate the subsequent integration and the entire representation in Theorem 1. No independent check or simpler argument is provided."
      },
      {
        "Problem": "Transversality and stratification arguments omit verification of properness and singular loci",
        "Location": "Section 2 (Proof of Proposition 1, Lemmas 2 and 3)",
        "Explanation": "The paper claims that the set where the homotopy fails is a stratified algebraic subset of positive codimension by arguing transversality of certain bundle maps. However, no check is given that these maps are proper or that the usual Thom–Mather conditions hold on the projective spaces involved. Without this, one cannot rule out that exceptional strata might have positive measure, undermining the claim that the homotopy is defined almost everywhere."
      },
      {
        "Problem": "Gamma‐function bounds yield super-polynomial growth in N, contrary to claimed polynomial average-cost",
        "Location": "Section 4.2 (Proposition 4 and Theorem 3)",
        "Explanation": "The final average‐cost bound involves ratios Γ(N+1)/Γ(N+1−q) with q=3/2, which scale like N^{3/2}, and further gamma‐ratios from Theorem 2 contribute additional factors growing as (N−n)^{p/2}. Thus the bound is O(N^{const·p}), not truly polynomial in the input size N for all regimes, contradicting the paper’s assertion of a uniformly polynomial average cost unless one restricts to ranges where N remains bounded relative to D."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 28515,
      "thinking": 4288,
      "output": 772
    }
  },
  {
    "entry_id": 150,
    "retraction_id": "2111.11437v6",
    "paper_id": "2111.11437v5",
    "retraction_comment": "There is an error. In section 3.4, the author identifies $Hom_Q(M, _M)$ with the set $Mat_{r \\times r}$ through the decomposition of M into a direct sum of indecomposable modules, and discusses nilpotent matrices in $Mat_{r \\times r}$. This is misleading because here we do not have a natural ring structure on $Mat_{r \\times r}$",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Unjustified reduction to ‘superdiagonal-only’ matrices in Lemma 3.8",
        "Location": "Section 3.4, Lemma 3.8 and Remark 3.9",
        "Explanation": "The proof treats the maps a, b as strictly upper-triangular Jordan blocks with 1’s only on the first superdiagonal and then solves af–fb=0 by an elementary matrix argument. However, in general τ-orbit representations a and b may have nonzero entries beyond the first superdiagonal (representing longer AR–paths). The author only asserts these extra entries do not affect the dimension of Ker(ra,b), but gives no justification for ignoring them. If these entries are nonzero they introduce additional linear constraints on f and can alter the kernel dimension, invalidating the key dimension count used throughout."
      },
      {
        "Problem": "Unclear definition and existence of the ‘dual representation’ M*",
        "Location": "Definition 2.2",
        "Explanation": "The paper defines M* as the unique orbit in D Ext¹_Q(M,M) dominating all others under degeneration order, but does not prove such a maximal open orbit exists or is unique for arbitrary Dynkin types. Later results (notably Lemma 3.10) hinge on using M* to construct the special element a∈Hom_Q(M,τM). Without a precise construction or proof of uniqueness, it is unclear that the chosen a coincides with the determinantal-module map used in the cluster setup, potentially invalidating the match between Ext computations and quantum minor properties."
      },
      {
        "Problem": "Over–strong equivalence between Ext¹_Λ vanishing and surjectivity of ra,b and rb,a",
        "Location": "Proposition 4.5 (equation 4.14) and its use in Theorem 4.6",
        "Explanation": "While [1, Proposition 9.1] gives a short exact sequence relating Ext¹_Λ(M,N) to Coker ra,b and Coker rb,a*, the paper asserts Ext¹_Λ(M,N)=0 if and only if both ra,b and rb,a are surjective. This neglects the possibility of nontrivial Hom_Λ(M,N) or Hom_Λ(N,M) terms interfering with the exact sequence or of higher-order obstructions. The argument assumes the exact sequence splits cleanly and that vanishing of the cokernels suffices, but does not check the necessary Tor or higher Ext vanishing hypotheses that would justify the iff statement."
      },
      {
        "Problem": "Unproven bijection between Ext¹_Λ vanishing and dual canonical basis quasi-commutation",
        "Location": "Theorem 5.2",
        "Explanation": "The main theorem equates b_λ b_κ∈q^ℤ B* (quasi-commuting quantum minors in the dual canonical basis) with Ext¹_Λ(M_fλ,M_fκ)=0. Although the paper cites earlier cluster results, it does not supply a direct proof that vanishing of this particular Ext¹ over the preprojective algebra exactly characterizes dual canonical basis multiplication. In particular, the logical step from cluster-categorical rigid combinatorics to membership in the dual canonical basis is only sketched by reference and may fail without additional monoidal or positivity arguments."
      },
      {
        "Problem": "Type–A multisegment combinatorics (Lemma 4.7) assumed for all Dynkin types",
        "Location": "Lemma 4.7",
        "Explanation": "Lemma 4.7 gives explicit formulas for m(λ,κ), p(λ,κ), and R(λ,κ) in terms of multisegment data, but its proof relies on the segment model specific to type A quivers. Yet the paper claims to treat arbitrary Dynkin quivers. The combinatorial description of Kostant partitions and the segment-indexing break down in types D and E, so the counting arguments and resulting bijections in Lemma 4.7 do not generalize and so cannot underpin the general theorems stated earlier."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 27217,
      "thinking": 2368,
      "output": 850
    }
  },
  {
    "entry_id": 151,
    "retraction_id": "0912.4084v3",
    "paper_id": "0912.4084v2",
    "retraction_comment": "This paper has been withdrawn by the author. Paper is withdrawn. On review the paper contributes nothing of significance. The runtime analysis of the algorithms presented, while correct in terms of number of operations, does not represent the complexity of the algorithms in terms of \"bits input\". A naive mistake in reasoning",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Complexity measured in the magnitude of n rather than its bit‐length makes the algorithms exponential time in the conventional input size",
        "Location": "Page 2, end of Section 1 (paragraph starting “The complexity of factoring…”)",
        "Explanation": "The paper repeatedly measures time–space cost as a polynomial in the value of n (e.g. O(n^1.5 log n)), but the standard for factoring algorithms is polynomial in the number of bits of n (≈ log n). A bound that is polynomial in n can be exponential in log n, so the claimed 'polynomial' complexity does not imply tractability on the usual input size measure."
      },
      {
        "Problem": "Unjustified and likely incorrect bound on the inner loop of Algorithm 1",
        "Location": "Page 3, Section 2.1.1 (derivation of Loop 2 complexity)",
        "Explanation": "The analysis asserts that the inner 'while (r >= x)' loop iterates O(√n (ln √n + y)) times based on a harmonic‐series argument, but no rigorous proof is given that the recurrence on r and y yields the claimed logarithmic behavior. In the worst case (e.g. n prime), r may remain O(√n) each iteration, giving up to O(√n) inner steps per outer step and thus O(n) total, not the asserted polylog factor."
      },
      {
        "Problem": "Algorithm 1 does not correctly handle prime inputs and misidentifies trivial factors",
        "Location": "Page 4, Section 2.1.2 (Prime Factoring description)",
        "Explanation": "When n is prime, the process terminates at x = 1 with factors (1,n). The algorithm treats '1' as a valid factor and relies on external primality checks, but 1 is not prime and the method does not include a correct prime‐test or exclusion of the unit, undermining its correctness for prime inputs and its use in recursive factoring."
      },
      {
        "Problem": "Use of ArrayList.Contains in the sieve process incurs unaccounted linear‐time scans",
        "Location": "Page 6–7, Section 3.2.2 Loop 5 and Section 3.2.3 Filter Process",
        "Explanation": "Both in testing membership of j in p.N (Loop 5) and in the filter process (Loop 7), the implementation calls ArrayList.Contains, which is O(m) in the size of the list. This cost is omitted from the complexity bounds, but since m can grow to Θ(n^0.5) per prime, the actual time becomes at least quadratic in √n per check, invalidating the claimed O(n^2.5) overall."
      },
      {
        "Problem": "Divisibility test by repeated addition in Filter Process is grossly inefficient and mis‐analyzed",
        "Location": "Page 7, Section 3.2.3 (description of Filter Process)",
        "Explanation": "The filter tests p divides n by looping 'while (p.m < x) p.m += p.v', i.e. summing p until exceeding x, which takes O(n/p) steps. For p=2 this is O(n), and iterated over O(√n) primes gives O(n^1.5) just for the filter, yet the paper asserts only O(n^2.5) and ignores the linear‐add cost per iteration, making the stated bound erroneous."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 13940,
      "thinking": 2176,
      "output": 788
    }
  },
  {
    "entry_id": 152,
    "retraction_id": "2106.04691v2",
    "paper_id": "2106.04691v1",
    "retraction_comment": "Theorem 1.7 is established only under a restrictive hypothesis, not the generality that the authors assert",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Unjustified properness of the local map f used to define the analytic equivalence relation",
        "Location": "§2.2(c)",
        "Explanation": "The proof of Theorem 1.7 hinges on constructing, for each point b∈B, a neighborhood O and a holomorphic map f:O→C^d whose fibers agree with those of the period map and such that f is proper on O. The sketch in §2.2(c) merely asserts that certain ‘horizontal’ period‐matrix coordinates and exponentials thereof yield a proper map, but does not establish that f^{-1}(K) is compact in O for every compact K⊂C^d. Without a clear argument that f is proper, one cannot conclude that the induced equivalence relation is proper or invoke Grauert’s quotient theorem, so the existence of the analytic quotient \\hat P_T and hence Theorem 1.7 is left unsupported."
      },
      {
        "Problem": "Neglect of the semisimple part of quasi-unipotent monodromy",
        "Location": "Remark 1.16; §§2.2, 3.1",
        "Explanation": "Throughout the construction the authors assume, for exposition, that all local monodromy operators are unipotent (Ni), deferring the semisimple parts γ_{i,s}. In reality the monodromy in a general variation is only quasi‐unipotent, and the action of the semisimple factors can obstruct the single‐valuedness and holomorphic extension of the period‐matrix coefficients and their exponentials. No mechanism is provided to handle γ_{i,s}—for instance, whether one must pass to branched covers or take roots of the τ_j—so it is unclear that the local period‐matrix representations, the proper map f, or the finiteness and algebraicity arguments survive in the full quasi‐unipotent case."
      },
      {
        "Problem": "Insufficient justification of algebraicity of the Stein factor \\hat P_T",
        "Location": "Proof of Theorem 1.7, step (iii)",
        "Explanation": "In step (iii) the authors invoke Grauert to obtain that \\hat P_T is a compact complex analytic (Moishezon) space and then cite Artin’s GAGA to conclude that the morphism Φ_T extends as an algebraic map of algebraic spaces. However, Artin’s criterion requires detailed verification of formal algebraization conditions (normality, effectivity of formal deformations, coherence of the structure sheaf) which are not checked. Without ensuring that \\hat P_T satisfies these additional hypotheses, one cannot assert that it underlies an algebraic space or that the extension morphism is algebraic."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 39645,
      "thinking": 4224,
      "output": 617
    }
  },
  {
    "entry_id": 153,
    "retraction_id": "1308.2817v2",
    "paper_id": "1308.2817v1",
    "retraction_comment": "The paper has been withdrawn because Eq.(4) is incorrect (isospin CG coefficients have been omitted). The corrected results change some of the discussion for 48Ca while the conclusions for 208Pb are hardly effected. A revised manuscript is under preparation",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Unvalidated assumption of exact isospin symmetry between (p,p′) and (p,n) reactions in heavy nuclei",
        "Location": "Page 3, around Eq. (4)",
        "Explanation": "The derivation of the unit‐cross‐section ratio in Eq. (4) assumes perfect isospin symmetry and identical spin–isospin matrix elements for (p,p′) and (p,n) probes. In heavy systems like 208Pb, Coulomb distortions, medium modifications, and different reaction mechanisms can break this symmetry, leading to significant deviations in the ratio and hence in the extracted B(M1) strengths."
      },
      {
        "Problem": "Application of GT unit cross section parametrization without full uncertainty propagation",
        "Location": "Page 3, Eq. (5)",
        "Explanation": "The mass‐dependent fit for the (p,n) GT unit cross section at 297 MeV carries a quoted ~9% uncertainty in both prefactor and exponent. This parametrization is applied directly to 295 MeV (p,p′) data via Eq. (4) without propagating its errors. Such an omission can induce a >20% systematic error in the final B(M1) values."
      },
      {
        "Problem": "Use of DWBA angular distributions with large model dependence for extrapolation to 0°",
        "Location": "Page 2, discussion of Fig. 2",
        "Explanation": "Cross sections measured at finite angles are extrapolated to 0° using theoretical angular distributions from DWBA calculations. Prior studies report up to ~50% variation in absolute cross sections depending on the effective interaction. The present work does not quantify how this model uncertainty affects the extrapolated σ(0°) and resulting B(M1)."
      },
      {
        "Problem": "Neglect of isoscalar spin and orbital M1 contributions in converting to electromagnetic strength",
        "Location": "Page 4, Eq. (7)",
        "Explanation": "In Eq. (7) the conversion B(M1στ)→B(M1) assumes only the isovector spin term contributes. While orbital and isoscalar spin terms are small, they are nonzero—especially in heavy nuclei with complex configurations—and their omission can bias the derived electromagnetic transition strengths by up to 10–20%."
      },
      {
        "Problem": "Lack of direct validation of the method for 208Pb",
        "Location": "Page 4, Application to 208Pb (Fig. 4)",
        "Explanation": "The key relation between (p,p′) and (p,n) is tested only for 48Ca. For 208Pb no corresponding (p,n) data exist for the spin‐M1 analog states, so the extraction of M1 strength above 8 MeV relies entirely on unverified assumptions. This undermines the claimed discovery of additional strength in that region."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 7035,
      "thinking": 1664,
      "output": 659
    }
  },
  {
    "entry_id": 154,
    "retraction_id": "1412.0982v2",
    "paper_id": "1412.0982v1",
    "retraction_comment": "This paper has been withdrawn by the authors. As pointed out to us by [REDACTED-NAME], [REDACTED-NAME] and [REDACTED-NAME], Theorem 3.1 is incorrect, namely, the zero locus should be larger than that in Theorem 3.1. We are sincerely grateful to them for their valuable comments. Nevertheless, the metrics we constructed have positive sectional curvature almost everywhere on the Gromoll-Meyer sphere and on the homotopy (not diffeomorphic) RP^7",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Unjustified extension of Gray–O’Neill formulas to the semi‐Riemannian setting",
        "Location": "Proposition 2.2",
        "Explanation": "The authors assert without proof that the Gray–O’Neill curvature formulas remain valid for semi‐Riemannian submersions with bi‐invariant (possibly indefinite) metrics. Validity of those formulas requires nondegeneracy of the horizontal and vertical distributions and a careful sign analysis; this step is nontrivial and lapses may render the curvature formula incorrect."
      },
      {
        "Problem": "Incomplete necessity argument for nonnegative curvature",
        "Location": "Proposition 2.3, necessity proof",
        "Explanation": "To show that (Sp(2), g(r₁,r₂)) acquires negative curvature when r₁+r₂>2, the authors exhibit special vectors solving certain quadratic equations. The existence of these solutions (and the ensuing vanishing of all but the last curvature term) is asserted via a 'long but straightforward calculation' without checking parameter ranges; this algebraic construction may fail or overlook cases, undermining the sharpness of the r₁+r₂≤2 criterion."
      },
      {
        "Problem": "Potential omission of zero–curvature planes in the base submersion analysis",
        "Location": "Section 3 (proof of Theorem 3.1, Cases II and III)",
        "Explanation": "The classification of horizontal 2‐planes with vanishing sectional curvature on Σ⁷ relies on an ad hoc case‐by‐case analysis of algebraic entries. The arguments in Cases II and III assume without exhaustive justification that no further degeneracies occur, leaving open the possibility of unnoticed zero‐curvature directions in intermediate regions."
      },
      {
        "Problem": "Erroneous lower bound in the key conformal‐deformation estimate",
        "Location": "Lemma 4.2, inequality (4.11)",
        "Explanation": "The authors claim −4F·h(F)≥tπF², where h(F)=−(π/4)t sin(πF/2)/(1−t cos(πF/2)).  In fact one obtains −4F h(F)=tπF sin(πF/2)/(1−t cos(πF/2))≤tπF²/(1+t)<tπF², so the stated bound fails. This mistake propagates through (4.8) and invalidates the uniform positive lower bound needed to conclude positive curvature."
      },
      {
        "Problem": "Use of a merely transnormal, not isoparametric, function in the conformal step",
        "Location": "Section 4 (conformal deformation via F)",
        "Explanation": "The deformation factor is built from F which is only shown to be transnormal (∥∇F∥²=4(1−F²)), but not isoparametric on (Sp(2),g_r) for t>0.  Many Hessian and curvature identities employed rely on the stronger isoparametric condition; without it, the derived curvature estimates may no longer hold."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 16244,
      "thinking": 7232,
      "output": 716
    }
  },
  {
    "entry_id": 155,
    "retraction_id": "1402.3490v2",
    "paper_id": "1402.3490v1",
    "retraction_comment": "This paper has been withdrawn by the authors due to a crucial error of the combination rule",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Ill-defined handling of non-exclusive frame elements in the combination rule",
        "Location": "Section 3 (Definition 1 & Definition 3, Eq. 12); Example 1",
        "Explanation": "The authors allow frame elements to overlap but then use ordinary set intersection (B1 ∩ B2) to drive the combination rule. In Example 1 they treat “High ∩ Medium” as a primitive focal element, yet “High ∩ Medium” is neither an element of Θ nor a subset of Θ under the formal definition. This mismatch between intended semantics and set-theoretic operations renders the combination rule ill-defined when exclusiveness is dropped."
      },
      {
        "Problem": "No guarantee of rule applicability—normalization may fail",
        "Location": "Section 3 (Definition 3, Eq. 13)",
        "Explanation": "The D numbers combination rule defines KD = (1/(Q1Q2)) Σ_{B1∩B2=∅} D1(B1)D2(B2), then normalizes by 1–KD. Unlike Dempster’s requirement K<1, the authors impose no condition ensuring KD<1. When information completeness degrees Q1, Q2 are small or conflicts large, KD can be ≥1, making 1–KD zero or negative and the rule inapplicable or producing nonsensical results."
      },
      {
        "Problem": "Lack of proof for fundamental algebraic properties of the combination operator",
        "Location": "Section 3 (after Definition 3)",
        "Explanation": "Critical properties such as commutativity, associativity, and closure under combination are neither stated nor proved. Without these, results from multi-source fusion are order-dependent or undefined, undermining the operator’s utility in practical reasoning systems that combine more than two sources."
      },
      {
        "Problem": "Failure to acknowledge and differentiate from prior non-exclusive frameworks",
        "Location": "Section 1 (Introduction)",
        "Explanation": "Generalizations of Dempster–Shafer theory that relax exclusiveness and completeness—most notably the Transferable Belief Model (TBM) and the Dezert–Smarandache Theory (DSmT)—are not discussed. The claimed novelty of D numbers is therefore unsubstantiated, as key ideas have been explored and axiomatized elsewhere."
      },
      {
        "Problem": "Absence of axiomatic or semantic justification for D numbers and combination rule",
        "Location": "Section 3",
        "Explanation": "While D numbers and their combination are introduced by analogy to mass functions and Dempster’s rule, no underlying axioms, coherence criteria, or decision-theoretic semantics are provided. This ad hoc presentation lacks the formal foundation necessary to establish soundness or guide correct application."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 14399,
      "thinking": 1792,
      "output": 633
    }
  },
  {
    "entry_id": 156,
    "retraction_id": "1504.06694v3",
    "paper_id": "1504.06694v2",
    "retraction_comment": "This paper has been withdrawn by the author due to a logical fallacy was made in transition from equation (46) to equations (47)-(50)",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Invalid parametric construction in Section 2: the method of deriving new solutions from the n=4 case is circular and arbitrarily skips n=6 in order to recover the known n=7 solution, and it offers no genuine mechanism for n≥8.",
        "Location": "Section 2, equations (8)–(18)",
        "Explanation": "The author sets up a diophantine 6x−4y=2 based solely on the n=4 solution and uses its general solution to \"derive\" the n=5 and n=7 cases, but this process is entirely circular (it assumes known solutions to get known solutions), fails at n=6, and provides no justification or extension to n≥8."
      },
      {
        "Problem": "Unjustified discriminant argument: rewriting the quadratic for t leads to a 'necessary' perfect‐square condition but no exclusion of new solutions.",
        "Location": "Section 2, equations (21)–(22)",
        "Explanation": "From 6t²+5t+1−n!/4!=0 the paper derives discriminant Δ=1+24·5·6·…·n and asserts it must be a perfect square ending in 1 for n≥8, yet offers no proof that 1+24·(n!/4!) cannot be a square. Moreover, this restatement is algebraically equivalent to the original equation and contributes no new information."
      },
      {
        "Problem": "Invalid polynomialization by treating the constant 5 as a variable x and comparing coefficients.",
        "Location": "Section 3, equations (41)–(46)",
        "Explanation": "The author replaces the fixed integer 5 with a symbolic x and writes an alleged polynomial identity in x equating coefficients on both sides. Since the original equality holds only at x=5 (not as an identity in x), comparing coefficients is mathematically unjustified and yields no valid constraints on integer solutions."
      },
      {
        "Problem": "Erroneous inference of irrational coefficient a_{n/2−2} from S_{n−4}(n−4)=1.",
        "Location": "Section 3.1 & 3.2, equations (45) and (50)",
        "Explanation": "Even accepting the flawed polynomial approach, the claim that the top‐degree Stirling number term forces the corresponding a‐coefficient to be irrational is baseless: it arises from the invalid coefficient comparison and does not reflect any legitimate obstruction to m² being of the prescribed form."
      },
      {
        "Problem": "Overall unsubstantiated claim that no solutions exist for n≥8.",
        "Location": "Abstract and Conclusion",
        "Explanation": "The paper’s main theorem contradicts the long‐standing open status of Brocard’s problem but rests entirely on circular arguments, incomplete case analyses, and invalid algebraic manipulations, providing no rigorous number‐theoretic proof to rule out further solutions."
      }
    ],
    "think_summary": [
      "**Identifying errors in a proof**\n\nThe user wants to identify critical errors in a paper claiming that 1 + n! = m^2 has no solutions for n ≥ 8, which is an unresolved problem. They accurately highlight that known solutions exist only for n = 4, 5, and 7, indicating their conclusions are dubious. \n\nFirstly, in Section 2, the authors present an intuitive approach but rely on an arbitrary transformation of the diophantine equation. Their method for deriving solutions lacks generality and raises questions about the validity of the proof. I'll dig deeper for more critical problems.",
      "**Summarizing critical problems in the proof**\n\nIn Section 3, treating the constant 5 as a variable is a major issue, leading to invalid polynomial comparisons that don't apply universally. The conclusion drawn from Stirling numbers misuses previous invalid arguments. Overall, the claim of no solutions for n ≥ 8 is unproven and misrepresents an open problem. \n\nHere’s a breakdown of up to five critical problems referencing their locations and explanations, pointing out the flawed reasoning and methods throughout the sections. This should help clarify where the arguments fall short.",
      "**Summarizing a critical error**\n\nIn Entry 5, the proof claiming nonexistence for n ≥ 8 reveals a significant problem because it lacks a nontrivial number-theoretic argument. It fails to recognize that this is an open problem. The entire conclusion is either circular or invalid, meaning there’s no real progress made here. It contradicts the nature of the open problem itself. I'll format this finding along with others in a JSON list for clarity."
    ],
    "token_usage": {
      "input": 3783,
      "thinking": 2432,
      "output": 667
    }
  },
  {
    "entry_id": 157,
    "retraction_id": "1502.02090v5",
    "paper_id": "1502.02090v4",
    "retraction_comment": "This paper has been withdrawn by the author due to a crucial error in equation 3.15",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Introduction of an undefined variation form ω in the proof of Theorem 3.6, without specifying its properties or showing it satisfies the necessary gauge condition (d*_A ω=0) or integrability conditions, yet using it to conclude [*F⁷_A, ψ_A]=0.",
        "Location": "Section 3.2, proof of Theorem 3.6 (around equation (3.16))",
        "Explanation": "The argument invokes a one-parameter variation η = tψ_A ± t^{3/2} ω in the second variation inequality (3.13) but never defines ω, checks that ω lies in the Coulomb gauge or in L²∩L⁴, or verifies that Π⁷₂(d_A ω)=0. Without these, the limiting argument as t→0 is invalid and the key commutator vanishing cannot be deduced."
      },
      {
        "Problem": "Misuse of the adjoint operator Λ_{Ω̄} to define ψ_A in the Calabi–Yau case: wedging a (0,1)-form with the (0,3)-form Ω̄ on a threefold yields zero, so its adjoint cannot produce a meaningful map from Ω^{0,2} to Ω^{0,1}.",
        "Location": "Section 4, definition of ψ_A following equation (4.3)",
        "Explanation": "The paper defines ψ_A by Λ_{Ω̄}(ψ_A)=F^{0,2}_A, where Λ_{Ω̄} is said to be the adjoint of L_{Ω̄}(η)=Ω̄∧η. But on a Calabi–Yau 3‐fold Ω̄∧η≡0 for any (0,1)-form η, so L_{Ω̄}=0 and its adjoint is trivial. This renders the construction of ψ_A and all subsequent steps based on it ill-posed."
      },
      {
        "Problem": "Failure to justify that the ad(E)-valued 1-form ψ_A is parallel in the Calabi–Yau case: only d_Aψ_A=0 and d*_Aψ_A=0 are shown, but no Weitzenböck argument is provided to conclude ∇_Aψ_A=0.",
        "Location": "Section 4, proof of Theorem 4.4, immediately before the Riemann-holonomy argument",
        "Explanation": "The reduction-of-holonomy conclusion relies on showing ∇_Aψ_A=0 so that [∇_i,∇_j]ψ_A gives R_{ij}\\cdotψ_A. However, the paper never establishes full covariant constancy of ψ_A from its harmonicity; the link from d_Aψ_A=d*_Aψ_A=0 to ∇_Aψ_A=0 is omitted."
      },
      {
        "Problem": "Insufficient justification for the G2-form decomposition identity dβ = (1/4)*((dⁱ⁴₇ β)∧φ) + d¹⁴₂⁷β used in Lemma 3.5, and the consequent claim that the Λ³₁-component of d_AF⁷_A vanishes.",
        "Location": "Section 3.2, Lemma 3.5, equation (3.7) and its application",
        "Explanation": "The paper cites a decomposition of dβ for β∈Λ²₁₄ without defining the operators dⁱʲ_q or proving the identity. The step Π³₁(d_AF¹⁴_A)=0 is asserted 'easily' but depends on these undeveloped operators and on vanishing mixed terms. This gap undermines the derivation of d*_Aψ_A=0."
      },
      {
        "Problem": "Lack of control over integrability and boundary behavior of the variational forms on noncompact manifolds: the assumptions F_A∈L⁴ and YM(A)<∞ do not guarantee that ψ_A lies in the required domain (C¹∩L²∩L⁴) for the second variation, nor that boundary terms vanish in integration by parts.",
        "Location": "Section 2, discussion following equation (2.4), and throughout Sections 3 and 4",
        "Explanation": "The extension of the second variation inequality to noncompact M relies on cutoff functions and decay assumptions, but no estimates are provided showing ψ_A (defined from F_A) has sufficient decay or regularity to justify passing to the limit without boundary contributions. This gap affects both the G2 and Calabi–Yau arguments."
      }
    ],
    "think_summary": [
      "**Evaluating critical problems**\n\nThe user wants me to identify key issues in the paper titled \"Yang-Mills connections on G2-manifolds and Calabi-Yau 3-folds\" by Teng Huang. I need to carefully analyze the methods and proofs, focusing specifically on the main theorems.\n\nIn Theorem 1.1, the definition of stability for the Yang-Mills connection appears unclear. They describe \"stability\" based on the second variation being nonnegative, but I find it questionable if these variations consistently adhere to necessary gauge preservation. I'll need to pinpoint similar concerns in other theorems.",
      "**Examining variation concerns**\n\nIn section 3, the authors consider a variation of the form A + t ψ_A, but I need to consider whether ψ_A is compactly supported. They assume certain approximations for noncompact manifolds, yet they require ψ to belong to specific function spaces. \n\nIf F_A is in L^4, I'm curious if ψ_A calculated from F_7 would also fall into appropriate spaces. This boils down to whether having L^4 for a 2-form implies L^2 for a 1-form, and I need to check their assumptions about finite Yang-Mills energy on noncompact manifolds."
    ],
    "token_usage": {
      "input": 10046,
      "thinking": 7552,
      "output": 1033
    }
  },
  {
    "entry_id": 158,
    "retraction_id": "1611.05964v2",
    "paper_id": "1611.05964v1",
    "retraction_comment": "Algorithm 1 is inefficient since line 2 is processed n 3 times need to be changed There are inconsistent notations throughout the manuscript [REDACTED-NAME] are not defined",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Non-convexity and lack of convergence guarantee",
        "Location": "Section III.D",
        "Explanation": "The proposed iterative reweighted algorithm tackles a non-convex weighted tensor nuclear-norm minimization, yet no convergence analysis or sufficient conditions are provided. Without theoretical guarantees, the method may diverge or stall at poor local minima, invalidating claims of consistent performance improvement."
      },
      {
        "Problem": "Incorrect proximal operator for weighted nuclear norm",
        "Location": "Section III.A & III.C (Algorithm 2 steps 7–9)",
        "Explanation": "The algorithm simply weights the singular values and then applies a uniform shrinkage (SVT), but the true proximal operator of a weighted nuclear norm is singular-value‐specific shrinkage by λ·w_i. The paper’s heuristic (weight-then‐threshold) does not correctly solve the weighted minimization, so the method does not actually minimize the intended objective."
      },
      {
        "Problem": "Flawed definition of weighted tensor nuclear norm",
        "Location": "Definition 9, Equation (24)",
        "Explanation": "The ‘weighted tensor nuclear norm’ is ambiguously defined (uses “min” instead of a summation) and no properties (e.g. convexity, unitary invariance) are established. Arbitrary per-singular-value weights generally break nuclear-norm convexity, so the foundation of the entire reweighting strategy is unsound."
      },
      {
        "Problem": "Unclear experimental protocol for tensor rank",
        "Location": "Section IV (Experiments)",
        "Explanation": "The paper reports results for synthetic ranks 20–50 on real videos of unknown true rank, but never explains how these low-rank versions were generated or enforced. Without describing data synthesis or truncation procedures, the experiments cannot be reproduced and the rank-variation claims are meaningless."
      },
      {
        "Problem": "Unfair and non-transparent baseline comparisons",
        "Location": "Section IV (Comparison with GTNN, TMAC, TC)",
        "Explanation": "No details are given on how competing methods’ parameters (e.g. regularization weights, stopping criteria) were chosen or tuned. Absent a clear, standardized setup, it is impossible to know whether the reported gains arise from the proposed method or from suboptimal baseline implementations."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 26597,
      "thinking": 3008,
      "output": 524
    }
  },
  {
    "entry_id": 159,
    "retraction_id": "2206.04913v2",
    "paper_id": "2206.04913v1",
    "retraction_comment": "I really apologize the audience for this withdrawal. The last section has some errors, because the proof of Lemma 4.2 is not true. Also other sections should be improved",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Mischaracterization of faces in L²(I) in the proof of Theorem 3.5",
        "Location": "Theorem 3.5, proof (page 11)",
        "Explanation": "The argument distinguishes two types of faces of the simplicial complex L²(I) by invoking vertices S₁² and faces of the form {S_{ℓ₁}S_{ℓ'₁},…,S_{ℓᵢ}S_{ℓ'ᵢ}}.  But by Definition 3.4 (Step 2) all square exponents b=2e_i are removed from the vertex set V, so vertices like S₁² do not exist in L²(I).  Thus the case analysis uses non‐existent faces, and the exhaustive classification of faces needed to apply Lemma 2.3 fails, invalidating the computation of β_{i,2di}(R/I²).  "
      },
      {
        "Problem": "Potential loss of exactness in the simplicial resolution after vertex removal in Definition 3.4",
        "Location": "Definition 3.4, Step 2 (page 12)",
        "Explanation": "Step 2 removes any chosen representative cᵢ for which there exists cⱼ with S_{cⱼ} | S_{cᵢ}.  While this prunes non-minimal generators, it can break the acyclicity conditions required for the complex Lᵗ(I) to support a free resolution.  No verification is given that the induced subcomplex remains exact, so the use of Lᵗ(I) for computing or bounding Betti numbers (in Section 3) is unfounded without further proof."
      },
      {
        "Problem": "Unjustified exactness of the sequence in Lemma 4.2",
        "Location": "Lemma 4.2 (page 15)",
        "Explanation": "The short exact sequence decomposing Iᵗ into powers of subideals I_{m−s}, I_{m−s+1},…,I_m is asserted without checking kernel–image relations.  In particular, injectivity of φ and surjectivity of ψ in general monomial ideals of degree d is non-trivial and no proof is given that Im φ = Ker ψ.  Subsequent inductive bounds in Proposition 4.3 and Corollary 4.5 rely on this exactness and are thus unsound."
      },
      {
        "Problem": "Misapplication of Peeva’s minimality criterion in Proposition 3.10",
        "Location": "Proposition 3.10, proof (page 14)",
        "Explanation": "The argument uses Theorem 7.5 of [25] to deduce that vanishing of β_{s,r}(Iᵗ) forces the existence of a trivial subcomplex in the chain complex, hence a face of the supporting complex of degree r in dimensions s−1 or s+1.  However, Peeva’s criterion applies to minimal graded free resolutions, not arbitrary simplicial resolutions, and the correspondence between trivial summands and missing faces is not established here.  This gap invalidates the vanishing/non-vanishing conclusions."
      },
      {
        "Problem": "Overextension of vertex decomposable graph results to hypergraphs in Corollary 3.8",
        "Location": "Corollary 3.8 (page 13)",
        "Explanation": "Corollary 3.8 asserts that for any (C₂,C₅)-free vertex decomposable hypergraph H one has reg(R/I(H)) = (s.i.m)'_H.  Vertex decomposability and forbidden-cycle arguments are well-studied in graphs but do not straightforwardly extend to hypergraphs without additional shellability or link-decomposability conditions.  No verification is provided that the same combinatorial–algebraic correspondences hold in the hypergraph setting, so the equality may fail in general."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 18036,
      "thinking": 7040,
      "output": 886
    }
  },
  {
    "entry_id": 160,
    "retraction_id": "1705.06716v2",
    "paper_id": "1705.06716v1",
    "retraction_comment": "This study needs many major modifications. Majority of the study includes mistakes. For example, all the plots and the numbers that are generated using ALPGEN MC generator in the tables are not correct. In addition, the selected factorization and renormalization scales do not define the correct form of the interaction. Even the title of the study must be changed",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Ad-hoc tuning of renormalization and factorization scales to match data",
        "Location": "Section 3 (page 6, paragraphs 1–2)",
        "Explanation": "The authors multiply their default scale choices by arbitrary factors (×4 for Z+jets and ×2 for W+jets) solely to force agreement with ATLAS measurements. This tuning undermines the predictive power of the calculation and is not theoretically motivated; meaningful theoretical uncertainties should be assessed by standard scale variations rather than data-driven rescaling."
      },
      {
        "Problem": "Omission of jet–parton matching and merging procedure in Alpgen",
        "Location": "Section 3 (page 6, paragraph 1)",
        "Explanation": "Alpgen requires a matching/merging algorithm (e.g., MLM) to avoid double-counting between matrix-element jets and parton-shower jets. The paper does not describe any matching procedure, calling into question the reliability of the W/Z+multi-jet predictions, especially at high jet multiplicities."
      },
      {
        "Problem": "Mischaracterized and incomplete theoretical uncertainties",
        "Location": "Tables 2 and 4 (pages 3–5)",
        "Explanation": "The quoted errors (labeled “stat.”) on NNLO cross sections are unphysically large Monte Carlo integration uncertainties (up to ∼4%), while no PDF or scale variation uncertainties are provided. Without a proper assessment of perturbative and PDF errors, the claimed agreement and projected 10% increase at 14 TeV are not quantitatively supported."
      },
      {
        "Problem": "Inconsistent phase-space definitions when comparing generators and data",
        "Location": "Table 6 (page 7)",
        "Explanation": "The paper compares Alpgen 0-jet predictions (with jet pT>30 GeV) directly to MCFM inclusive-boson fiducial cross sections and ATLAS measurements without ensuring identical lepton, jet, and missing-energy cuts. These mismatched definitions invalidate the direct comparisons."
      },
      {
        "Problem": "Use of leading-order Alpgen for high jet multiplicities without NLO corrections",
        "Location": "Section 3 (page 7–8) and Figures 3–4",
        "Explanation": "Relying on LO Alpgen for predictions up to 6 jets yields theoretical uncertainties that grow with jet multiplicity. The observed fluctuations and non-monotonic behavior at 5–6 jets reflect this uncontrolled uncertainty, rendering quantitative conclusions about W/Z+multi-jet rates at 14 TeV unreliable."
      }
    ],
    "think_summary": [
      "**Matching Location and Explanation**\n\nI'm aiming to match locations with their corresponding explanations in a structured way. I'll create five entries that include a problem statement, location, and explanation, formatted in JSON. I want to ensure that each entry is critical and provides valid information. I wonder if there are any erroneous units I need to address, but I'm not expecting major issues. I’ll finalize this structure to maintain clarity and accuracy for the user."
    ],
    "token_usage": {
      "input": 22390,
      "thinking": 1152,
      "output": 595
    }
  },
  {
    "entry_id": 161,
    "retraction_id": "1604.05964v2",
    "paper_id": "1604.05964v1",
    "retraction_comment": "equation no. 16 17 and 18 have flaws, result of which final outage derivation is not converging",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Unrealistic interference model for secondary transmissions",
        "Location": "Section III.A, eq (6) and surrounding text",
        "Explanation": "The analysis assumes that at most one secondary base station can interfere on any given resource block (I_one,Si), completely neglecting the aggregate interference from all other secondary transmitters (both within and beyond the cluster). In a HetNet with many secondary nodes, this underestimates interference power and invalidates the outage results."
      },
      {
        "Problem": "Sensing performance (P_D, P_F) never enters the SINR/outage derivation",
        "Location": "Section III.A, after eq (6)",
        "Explanation": "Although the paper introduces probability of detection (P_D) and false-alarm (P_F) in the model description, neither P_D nor P_F appears in any SINR or outage expression. By treating every missed detection as certain interference, the analysis ignores the true random access behavior induced by imperfect sensing, making the outage formulas unsound."
      },
      {
        "Problem": "Incorrect separation of dependent expectations/integrals",
        "Location": "Section III.A, eqs (11)–(14)",
        "Explanation": "The derivation splits the joint expectation over (r1, r_m, k) into a product of two separate double-integrals, each integrating over r1 independently. Since r1 appears in both the primary and secondary interference Laplace terms, these integrals are not separable. This mathematical error corrupts the final outage probability expressions."
      },
      {
        "Problem": "Potentially wrong conditional PDF for the m-th nearest base station",
        "Location": "Section III.A, eq (8)",
        "Explanation": "The stated form of f_{r_m|r_1}(r_m|r_1) appears to use (m–1)! in the denominator and an exponent m–1 on (r_m^2–r_1^2), whereas the standard order-statistic result would involve (m–2)! and exponent m–2. This mismatch alters the distance distribution for the canceled interferers and thus the Laplace functional of the residual interference."
      },
      {
        "Problem": "Noise variance arbitrarily set to zero without justification",
        "Location": "Section III.A, after eq (12)",
        "Explanation": "The analysis drops the thermal noise term (σ=0) to focus on interference-limited operation, but offers no network parameters or regime justification. In many HetNet scenarios (especially at cell edges), noise can still be significant; neglecting it without justification undermines the generality of the outage results."
      }
    ],
    "think_summary": [
      "**Analyzing formula inconsistencies**\n\nI’m noticing another issue: they've dropped the noise term (σ=0) without justification, yet their results include noise, which seems contradictory. They refer to \"cell edge users\" at a rate of 0.05% on the CDF, using percentages but also a fraction (0.05%). It's a bit ambiguous. There’s also a potential error with the 2F1 hypergeometric function — they mix terms and confuse their notation. However, their formula holds if s equals Θ r1^α. So, I think the corrections align properly, but it’s a bit tricky to follow.",
      "**Identifying integral errors**\n\nIn equation (14), they keep r1 in the Laplace transform but integrate over both r1 and rm. This separation of integrals is problematic since it incorrectly treats r1 as independent in both integrals. They need to recognize that r1 is involved in both, which means it shouldn't be separated. The result is a double counting of r1, rendering their final expression mathematically incorrect. This issue impacts all subsequent equations, making their derivations flawed. Additionally, they seem to overlook certain probabilities in their analysis, which is another error in their approach."
    ],
    "token_usage": {
      "input": 11658,
      "thinking": 3648,
      "output": 586
    }
  },
  {
    "entry_id": 162,
    "retraction_id": "1010.6286v5",
    "paper_id": "1010.6286v4",
    "retraction_comment": "This paper has been withdrawn by the author due to the map described in Thorem 2.4 is not injective",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "The definition of the ‘couple product’ is ambiguous and lacks a clear presentation—generators, relations, and the subgroup description are not precisely specified, making the construction ill‐defined.",
        "Location": "Definition 2.2 (Page 4)",
        "Explanation": "The paper introduces a new group construction G₁ ⋊⋉ₛ … ⋊⋉ₛ Gₖ by describing partitions of generators and a generating set for a subgroup of ∏Gᵢ, but never spells out the relations or how the subgroup sits inside the product. Without a precise presentation or embedding criterion, all subsequent uses of this ‘couple product’ are unsupported."
      },
      {
        "Problem": "Key properties of the ‘couple product’ in Proposition 2.3 are asserted without proof and may fail in general.",
        "Location": "Proposition 2.3 (Pages 4–5)",
        "Explanation": "Properties such as symmetry, associativity, and the existence of natural projections are stated as exercises, yet these depend critically on the (ill‐defined) construction of the couple product. No verification is given that the proposed subgroup indeed satisfies these properties, undermining later arguments that rely on them."
      },
      {
        "Problem": "The main embedding Theorem 2.4 is not rigorously established—injectivity and relation preservation of the constructed map are unproven.",
        "Location": "Theorem 2.4 (Page 5)",
        "Explanation": "The author defines an initial homomorphism Ψ and then modifies it along each edge by forming purported couple products, but never proves that (1) non‐commuting generators remain non‐commuting exactly when required, (2) commuting relations among non‐adjacent vertices are preserved, or (3) the final map is injective. The entire embedding argument hinges on unverified properties of the couple product."
      },
      {
        "Problem": "Assertion that ⟨σ₁²,σ₂²⟩≅F₂ in Bₙ is stated for n≥2 but only holds for n≥3.",
        "Location": "Section 2.1, paragraph following Theorem 2.1 (Page 3)",
        "Explanation": "The paper claims that in Bₙ with n≥2 the subgroup generated by σ₁² and σ₂² is free of rank 2. For n=2 there is only one generator σ₁, so σ₂ does not exist. The result really requires n≥3 (in fact PB₃≅F₂), so the hypothesis n≥2 is incorrect."
      },
      {
        "Problem": "The proof of Corollary 3.3 on homomorphisms from odd‐connected Artin groups misapplies the exponent‐sum argument and omits justification that φ(s)=φ(t).",
        "Location": "Corollary 3.3 (Page 7)",
        "Explanation": "The author argues that if s,t satisfy an odd‐length braid relation then any homomorphism φ into A_Γ must identify them, using an exponent‐sum map to Z^{|Γ|}. However, the connection between the odd braid relation and the impossibility of mapping ⟨φ(s),φ(t)⟩ onto Z or F₂ in A_Γ is not made precise, and the contradiction via exponent sums is not rigorously derived."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 7657,
      "thinking": 3136,
      "output": 761
    }
  },
  {
    "entry_id": 163,
    "retraction_id": "0811.2430v4",
    "paper_id": "0811.2430v3",
    "retraction_comment": "The author revised the article and considers that the proof is not rigorous. The main counter-argument is that one should not draw conclusions from a truncated wave-function",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Failure to symmetrize initial state",
        "Location": "Section 2, Equation (1)",
        "Explanation": "The paper treats the two identical particles from different sources as initially distinguishable, using a factorized product state in Eq. (1). However, identical particles must obey symmetrization (bosons) or antisymmetrization (fermions) globally at all times. Omitting this fundamental requirement invalidates subsequent derivations of symmetrization arising only upon loss of path information."
      },
      {
        "Problem": "Assumption of fixed relative phase between independent sources",
        "Location": "Section 2, preceding Eq. (1) and Eqs. (10)–(11)",
        "Explanation": "The interference terms cos φ in Eqs. (10) and (11) rely on a well-defined relative phase φ between particles from two independent sources. In practice, separate sources are incoherent and φ is random, so any interference averages to zero. Without phase locking, the predicted modulation of coincidence rates cannot be observed, undermining the experiment's feasibility."
      },
      {
        "Problem": "Neglect of spatial–temporal wavepacket overlap requirement",
        "Location": "Section 2 experimental setup; Discussion",
        "Explanation": "Quantum interference and exchange effects require overlapping wavepackets in space and time (e.g. at beam-splitters). The proposed geometry keeps particles in distinct regions V and E that never meet or overlap, precluding any two-particle interference. Thus the predicted symmetry (antisymmetry) signatures rely on nonexistent overlap, invalidating conclusions about nonlocal symmetrization."
      },
      {
        "Problem": "Misinterpretation of symmetrization as dynamic nonlocal 'agreement'",
        "Location": "Discussion section",
        "Explanation": "The text suggests that identical particles 'agree' on symmetry at a distance without interaction. In reality, symmetrization is a static kinematic constraint in Fock space, not a dynamical process requiring communication. This conceptual misunderstanding undermines the interpretation of results as evidence for nonlocal exchange-driven correlations."
      },
      {
        "Problem": "Residual distinguishability due to spin labeling for fermions",
        "Location": "Section 2, spin selection using Stern–Gerlach; Discussion",
        "Explanation": "While Stern–Gerlach filters for, say, spin-up, it also entangles spin and path, preserving which-particle labels via their distinct spinor–spatial wavefunctions. The particles thus remain distinguishable, so complete indistinguishability—and hence antisymmetric interference—cannot be achieved as assumed."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 2557,
      "thinking": 1856,
      "output": 592
    }
  },
  {
    "entry_id": 164,
    "retraction_id": "2201.05255v2",
    "paper_id": "2201.05255v1",
    "retraction_comment": "Our definition of the higher Toda brackets is not effective, i.e. not well defined, in the category of pointed spaces",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Well-definedness of the stable bracket hΣm~θ~i with respect to choice of representative",
        "Location": "Section 10, Definition 10.3 and Lemma 10.2",
        "Explanation": "The stable bracket hΣm~θ~i is defined by picking an arbitrary representative sequence ~fℓ of the stable elements θ~ and then forming the directed system of sets A′(r,ℓ). Lemma 10.2 shows that the limit over the suspension index ℓ is independent of ℓ and of the ‘ℓ’ used for two particular representatives, but there is no general proof that the resulting limit does not depend on the choice of representative ~fℓ itself. One needs to show that for any two representative sequences (possibly at the same ℓ) the induced direct systems are isomorphic in the limit. Without that, hΣm~θ~i is not guaranteed to be well-defined."
      },
      {
        "Problem": "Missing construction and injectivity of connecting maps in the direct system for the stable bracket",
        "Location": "Section 10, Lemma 10.1(2)–(3) and Definition 10.3",
        "Explanation": "The authors form lim₍r→∞₎A′(r,ℓ) by regarding the suspension maps Σ:A′(r,ℓ)→A′(r+1,ℓ) as injections and taking the union. However, they never explicitly define these connecting maps nor prove that they are injective (or even well-defined on the nose rather than up to indeterminacy). Without a clear directed-system structure and injectivity, the operation of passing to the union may be ill-posed, casting doubt on the existence and meaning of the limit set."
      },
      {
        "Problem": "Underjustified homotopy-invariance proof for higher fk",
        "Location": "Section 3, Lemma 3.1, proofs for k≥4",
        "Explanation": "Lemma 3.1, which establishes that the unstable bracket depends only on the homotopy class of each fk, is proved in detail for k=1,2,3 and sketched for k=4. For k>4 the authors merely assert that 'the same methods' apply, without supplying the necessary inductive verifications for the increasingly intricate admissible null-homotopy sequences. This leap risks a gap: it is not clear that no new obstacles appear when moving to arbitrary higher k."
      },
      {
        "Problem": "Use of the full category TOP* without cofibration/well-pointedness hypotheses",
        "Location": "Section 2, opening paragraph",
        "Explanation": "All constructions—mapping cones, suspensions as quotients, smash products, and the homeomorphism identifications—rely on the spaces being well-pointed to ensure quotients and pushouts behave as expected. The paper extends from TOPw (well-pointed spaces) to the full TOP* without reinstating cofibration or well-pointedness assumptions. As a result, some of the key homotopy cofibration arguments may fail, undermining the foundational definitions of the Toda brackets in this more general setting."
      },
      {
        "Problem": "Potential size issues in forming unions over all admissible null-homotopy sequences",
        "Location": "Section 1, definition of {f~}m~ for n≥4",
        "Explanation": "The bracket {f~}m~ is defined as the union S_A~ of images over all admissible sequences A~ of null homotopies. There is no check that the class of all such admissible sequences is a set (rather than a proper class), nor any restriction on the homotopies to guarantee smallness. Without this, S_A~ might not be a well-defined subset in the usual set-theoretic sense."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 38623,
      "thinking": 3904,
      "output": 844
    }
  },
  {
    "entry_id": 165,
    "retraction_id": "1601.01217v3",
    "paper_id": "1601.01217v2",
    "retraction_comment": "There is an important mistake in the definition of the global smoothing operator preserving the presymplectic form",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Uncontrolled growth of derivatives in the de Rham–type smoothing operator",
        "Location": "Section 6.3, Lemma 6.12",
        "Explanation": "The estimates (6.35)–(6.36) for the new smoothing operator R^{(ω₀)}ₜ rest on uniform bounds on arbitrarily high derivatives of the chart‐wise diffeomorphisms Φₓ and their inverses. The paper merely sketches these bounds but does not show that the constants C_{p,s} depend only on p and s (and not, say, factorially on the derivative order). Without rigorous control of ∂^kΦₓ and ∂^k(Φₓ)⁻¹, the required smoothing inequalities may fail and the Nash–Moser iteration breaks down."
      },
      {
        "Problem": "Failure to verify tameness under partition‐of‐unity patching",
        "Location": "Section 6.2–6.3, definition of Sₜ and Proposition 6.9",
        "Explanation": "The global smoothing operator Sₜ is defined as the composition S_{t,1}∘…∘S_{t,m} using a finite atlas and partition of unity. The paper does not check how the constants in the estimates accumulate when patching across overlaps, nor that the resulting operator still satisfies the uniform smoothing bounds (7.13)–(7.14) with constants independent of the number of charts. This gap jeopardizes the CI–space structure needed for the global Nash–Moser scheme."
      },
      {
        "Problem": "Missing tame estimates for the Chevalley–Eilenberg homotopy operators",
        "Location": "Section 5.1, Lemma 5.1",
        "Explanation": "The construction of the homotopy operators h₀,h₁ for the Lie algebra cohomology uses Sobolev‐space arguments and Mostow–Palais equivariant embeddings, but the paper only asserts the tame bounds (5.2) in passing. A careful Nash–Moser method requires explicit control of how the C^k norms of h_j(σ) grow in terms of σ, and these are not exhibited. Without them, the quadratic convergence estimate (7.12) cannot be guaranteed."
      },
      {
        "Problem": "Overstrong Darboux chart assumption for global presymplectic form",
        "Location": "Section 6.3, equations (6.44)–(6.45)",
        "Explanation": "The paper assumes a finite cover {V_i} and local charts ϕ_i such that ϕ_i^*ω|_{V_i} is exactly the constant form ω₀. In general Darboux coordinates for a presymplectic form of constant rank only guarantee ω pulls back to ω₀ up to first order at a point, not identically on an open chart. This overstrong assumption may fail globally and invalidates the construction of the chart‐wise smoothing operator."
      },
      {
        "Problem": "Unverified small‐norm and radius‐shrink conditions in the iteration",
        "Location": "Section 4.3 together with Appendix 7 (Theorems 7.2 and 7.6)",
        "Explanation": "The abstract normal‐form theorems require precise control of how each step’s diffeomorphism and remainder shrink both in C^k–norm and in the domain radius, but the paper does not check that the push‐forward and smoothing steps preserve the necessary smallness and radius‐shrinking conditions uniformly through the iteration. Without this, the convergence argument is incomplete."
      }
    ],
    "think_summary": [
      "**Analyzing chart estimates**\n\nI see the concern about the estimates from the charts not being fully proven, especially since they rely on local cases. The user prefers to focus on critical errors that could invalidate conclusions. \n\nFirst, they assume trivialization for each chart, which is only locally guaranteed by the Darboux theorem, not for arbitrary neighborhoods. That’s something to consider. Second, the composition of smoothing operators seems to preserve the presymplectic property, as they’ve shown for each component. I think that’s a solid point!",
      "**Examining regularization operator issues**\n\nI'm looking closely at the regularization operator defined by coordinate flows for integrable vectors in Rn. They claim this operator shares certain smoothing inequalities, but I wonder if this is valid. The proof sketches resemble convolution smoothing. However, they assume bounds on derivatives without estimating the derivatives effectively. I’m concerned that if these derivatives can blow up due to non-linear dependencies, it may lead to insufficient control, breaking the inequalities and undermining abstract theorem applications. There’s also a potential issue with the de Rham smoothing operator, where flow preservation may fail."
    ],
    "token_usage": {
      "input": 36274,
      "thinking": 5824,
      "output": 792
    }
  },
  {
    "entry_id": 166,
    "retraction_id": "1412.3838v2",
    "paper_id": "1412.3838v1",
    "retraction_comment": "This paper has been withdrawn by the author due to an error in the statement according to which the volume element does not depend on the choice of the time orientation",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "False claim that the integration domain (the unit ball bundle BM) is independent of the chosen time‐orientation t",
        "Location": "Section 4.2 (page 9–10), paragraph beginning “At a given point x … the unit balls of all … are identical”",
        "Explanation": "The unit ball of each Riemannian metric g_{t,+}(x) is defined by {y|g_{t,+}(x)(y,y)≤1} and thus depends on t. Different time‐orientations t produce different osculating metrics g_{t,+}, hence different unit balls in TxM. The paper’s assertion that all these balls coincide (and are the Euclidean ball) is incorrect, invalidating the claimed t‐independence of the volume element."
      },
      {
        "Problem": "Use of the coordinate volume form dy on TxM without an intrinsic metric‐dependent measure",
        "Location": "Definition 5 and equation (20) (page 10–11)",
        "Explanation": "The prescription σ(x)=vol(Bn)^{-1}∫_{Bn(x)}|g(x,y)| dy treats dy as a fixed Lebesgue form. Under a change of base coordinates, dy rescales by det(∂x'/∂x), but the domain Bn(x) (an ellipsoid for generic g_{t,+}) does not transform to a fixed shape in the new coordinates unless one uses the Riemannian volume element of g_{t,+}. Without using the proper metric volume on the fibers, σ(x) is not a coordinate‐invariant scalar density."
      },
      {
        "Problem": "Neglect of potential singularities or divergence of det(g_ij(x,y)) within the integration domain",
        "Location": "Section 4, Definition 5 (page 11), and Section 5 Example (page 12–14)",
        "Explanation": "For general Finsler spacetimes, det(g_ij(x,y)) can blow up or fail to be smooth on directions included in the Euclidean ball Bn(x), especially near lightlike or non‐differentiable directions. Integrating |det(g)| over such regions may yield divergent or ill‐defined values. The paper assumes a continuous extension of det(g) without justifying integrability in full generality."
      },
      {
        "Problem": "Global time–orientability assumption and its overlooked impact on volume independence",
        "Location": "Introduction (page 1), multiple references in Sections 3–4",
        "Explanation": "The construction presupposes a globally defined timelike vector field t with L(t)=1. Many Finsler spacetimes do not admit a global time‐orientation. Even when local time orientations exist, the flawed claim of t‐independence (see Problem 1) means different local choices would lead to different volume forms, contradicting the paper’s main result."
      }
    ],
    "think_summary": [
      "**Evaluating critical issues**\n\nThe user is asking for up to five critical problems in a JSON format, focusing on potential errors that could undermine the paper's conclusions about Finslerian spacetimes. I need to examine each issue carefully. \n\nFor instance, there's a concern about the existence of time orientation, as not all Finsler spacetimes allow for this. The paper seems to recognize this limitation, which is a good sign. Next, if the function L isn't smooth everywhere, challenges may arise, though the paper's restrictions mitigate this. I'm curious about the calculations regarding the transitional hv-metric as well.",
      "**Analyzing volume form and metrics**\n\nI'm exploring the block diagonal metric with the determinant being det(g_ij)*det(g_t+). Now, φ_t comes out to be sqrt(|g|/g_{t,+}), leading us to Ω = |g| dx∧dy, which seems acceptable since there's no dependence on t. However, defining the volume form using |g(x,y)| over the Euclidean ball Bn raises concerns about integrability since g could be infinite or undefined for certain inputs. Their assumption about mapping from the Euclidean ball is questionable, particularly with time dependence in the metrics affecting the shape of unit balls, making me wonder about the validity of their claims.",
      "**Scrutinizing unit ball claims**\n\nI'm examining the claim that the unit ball can be considered a Euclidean sphere under positive definite metrics. However, that's not entirely accurate, since the shape of the ball changes with different coordinate frames. The assertion that all metrics produce identical unit balls at a fixed point seems false, as the metric varies with time and the definition of Bn(x) remains dependent on g_{t,+}. Additionally, they assert domain independence regarding different metrics, but that contradicts the volume's dependence on t, which is a significant flaw. Lastly, I have concerns about their integration methods and the assumptions surrounding continuity and transformation invariance.",
      "**Identifying key problems in integration**\n\nI'm uncovering several critical issues related to the volume element and integration within Finsler spacetimes. First, there's the incorrect claim that unit balls are independent of time orientation. Then, the integration domain and measure dy lack proper intrinsic definition, failing to ensure coordinate invariance. I also see potential divergences in the determinant of g_ij over the domain, and the assumption of global time orientation may overlook relevant cases. Lastly, the continuity of the chosen osculating metrics also raises concerns, though that's less critical. I'll compile these into a JSON format."
    ],
    "token_usage": {
      "input": 9193,
      "thinking": 4096,
      "output": 655
    }
  },
  {
    "entry_id": 167,
    "retraction_id": "1301.0396v3",
    "paper_id": "1301.0396v2",
    "retraction_comment": "This paper has been withdrawn by the author due to an error in Lemma 2.9",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Unjustified existence of stable ordered-union ultrafilters under CH",
        "Location": "Corollary 3.3 (page 9)",
        "Explanation": "Corollary 3.3 asserts that CH alone suffices to produce, for every block-sequence, a <ω₁-stable Milliken–Taylor ultrafilter U with FU(a¯)∈U. However, the existence of such ultrafilters is known to require additional forcing axioms (e.g. MA(σ-centred)) and is independent of ZFC+CH. CH by itself does not guarantee the existence of any Milliken–Taylor ultrafilter, so the subsequent definition of Q_β=M(U_β) may rely on non-existent objects."
      },
      {
        "Problem": "Misapplication of Eisworth’s P-point preservation criterion",
        "Location": "Theorem 3.5 and use in Section 4, cases (c) and (d) (pages 10–14)",
        "Explanation": "Theorem 3.5 requires verifying that the ground-model P-point E is not Rudin–Blass reducible to Φ(U), i.e. ¬(E ≥_RB Φ(U)). The paper never exhibits or proves such a witness for each U_β. Without an explicit argument ensuring E \n≥_RB Φ(U_β), the claim that M(U_β) preserves the ultrafilter E (and hence keeps it a P-point) is unsupported."
      },
      {
        "Problem": "Over-reliance on MA(σ-centred) for preserving block-splitting in long iterations",
        "Location": "Lemma 2.9 and its use in Section 4, property (I5) (pages 6–7, 12–13)",
        "Explanation": "Lemma 2.9 uses MA_{<2^ω}(σ-centred) to seal names and ensure preservation of countably block-splitting families. Later, in the ω₂-length proper iteration, no global MA is maintained, yet the paper invokes Lemma 2.9 and Shelah’s iterability theorem to conclude preservation of countably block-splitting at limit and successor stages. Without maintaining the required fragment of MA at each stage, the sealing arguments may fail, undermining (I5)."
      },
      {
        "Problem": "Incorrect application of Hindman’s theorem to arbitrary subsets of F",
        "Location": "Definition 4.4, clause (1) (page 13)",
        "Explanation": "Clause (1) of Definition 4.4 requires, for each ε, deciding whether an arbitrary B_ε⊆F contains or is disjoint from some FU(c₁^ε). Hindman’s theorem guarantees monochromatic FU-blocks only for finite partitions of F, not for arbitrary subsets. The step “FU(c₁^ε)⊆B_ε or FU(c₁^ε)∩B_ε=∅” cannot be achieved in ZFC without additional combinatorial hypothesis."
      },
      {
        "Problem": "Insufficient handling of non-meagre filters via ♦-bookkeeping",
        "Location": "Property (I3) in Section 4, successor of cf(β)=ω₁ case (pages 11–14)",
        "Explanation": "Property (I3) demands that at each β in S²₁ the iteration forces g_β(F)=g_β(cl(E)) for the ⋄-guessed non-meagre filter F. The paper sketches a ‘Blass–Laflamme task’ in Definition 4.4 to accomplish this, but does not verify that the chosen U_β (or its generating sequence) actually forces every finite-to-one image of F to coincide with that of E. The sealing step appears too weak to guarantee full dichotomy for arbitrary non-meagre F."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 15728,
      "thinking": 4160,
      "output": 876
    }
  },
  {
    "entry_id": 168,
    "retraction_id": "1808.04792v3",
    "paper_id": "1808.04792v2",
    "retraction_comment": "The velocities in the radiative transfer analysis for each snapshot of the simulated collapsing core are a factor of 1.732 (the square root of 3) smaller along each of the three dimensions. This error is currently being rectified",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Unrealistic initial conditions with a globally unstable uniform background",
        "Location": "Section 2.1 (Numerical simulation setup)",
        "Explanation": "The simulation assumes a uniform medium that is everywhere Jeans-unstable and collapses hierarchically from the outset. Real molecular clouds are neither globally uniform nor fully gravitationally unstable at all scales; they are highly structured by turbulence and magnetic support. This unrealistic setup produces an 'outside-in' velocity profile that may not occur in nature, undermining the paper’s key claim that standard line-profile analyses systematically underestimate infall speeds."
      },
      {
        "Problem": "Neglect of turbulent and magnetic pressure support",
        "Location": "Section 2.1 (Model assumptions)",
        "Explanation": "The model omits both supersonic turbulence and magnetic fields, yet observations and simulations show that these provide significant support against collapse in molecular clouds. Without including these effects, the collapse proceeds too rapidly and produces supersonic infall velocities throughout, leading to synthetic line profiles that are not representative of real cores and invalidating conclusions about infall measurement biases."
      },
      {
        "Problem": "Assumption of constant molecular abundances, ignoring depletion",
        "Location": "Section 2.3 (Synthetic observations — abundance prescription)",
        "Explanation": "The radiative transfer is performed with fixed abundances of HCO⁺ and N₂H⁺. In dense collapsing cores, molecules like HCO⁺ are known to freeze out onto dust grains at high densities, altering line optical depths and asymmetries. Ignoring depletion can drastically change synthetic line profiles, casting doubt on the inferred underestimation of infall speeds."
      },
      {
        "Problem": "Neglect of dust continuum emission and absorption",
        "Location": "Section 2.2 (Radiative transfer setup)",
        "Explanation": "The MOLLIE runs include only the cosmic microwave background as a continuum source, omitting dust emission and absorption that become significant at the high densities simulated. Dust continuum can fill in or modify self-absorption features, so neglecting it yields line profiles that differ substantially from observations, undermining the paper’s core comparisons."
      },
      {
        "Problem": "Enforced spherical symmetry unlike real core morphologies",
        "Location": "Section 2.1 (Simulation geometry)",
        "Explanation": "The collapse is modeled with perfect spherical symmetry, whereas observed dense cores are often filamentary or clumpy and subject to anisotropic flows. Line profiles formed in asymmetric structures can differ qualitatively, so conclusions drawn from a 1D-like spherical model may not generalize to real, aspherical cores."
      }
    ],
    "think_summary": [
      "**Structuring entries for clarity**\n\nI'm focusing on creating entries with a structured format that includes \"Problem,\" \"Location,\" and \"Explanation.\" For the location, I’ll specify sections like \"Section 2.1, initial simulation setup,\" or \"Section 2.3, abundance assumption.\" It’s important that I maintain clarity with each entry. I should aim for a JSON list containing up to five entries to keep everything organized and easy to reference. This will help in documenting my findings more effectively."
    ],
    "token_usage": {
      "input": 47655,
      "thinking": 1088,
      "output": 593
    }
  },
  {
    "entry_id": 169,
    "retraction_id": "1306.2988v2",
    "paper_id": "1306.2988v1",
    "retraction_comment": "This paper has been withdrawn by the authors. The result claiming a factor 0.56 algorithm is invalid because of a crucial bug in Claim 2 which was brought to our attention by [REDACTED-NAME], [REDACTED-NAME], and [REDACTED-NAME]",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Unproven and likely false Stability Property (Claim 1)",
        "Location": "Section 2.2, Claim 1",
        "Explanation": "The paper asserts without proof that moving a single vertex in the random order can change the current matching by at most one connected alternating path or cycle. In general non-bipartite graphs, promoting one vertex may cascade changes through multiple blossoms or augmenting paths, creating several disjoint symmetric-difference components. Without a detailed inductive argument or case analysis, this key stability claim is unjustified and may not hold, undermining all subsequent bounds that rely on it."
      },
      {
        "Problem": "Flawed counting in Lemma 7 linking non-monotone events to good-events",
        "Location": "Section 3.1, Lemma 7",
        "Explanation": "Lemma 7 constructs maps from each non-monotone event to one type-2 good-event and to t type-1 good-events, and then divides by n assuming each type-1 good-event arises from at most n non-monotone events. Neither injectivity (for type-2) nor the claimed at-most-n preimages bound (for type-1) is rigorously proved. Overlaps among mappings and dependencies between events are ignored, so the inequality relating γ(t,n) to sums of g and h may fail, invalidating the core combinatorial lower bound."
      },
      {
        "Problem": "Incorrect aggregation in the strongly factor-revealing LP (Lemma 10)",
        "Location": "Section 3.2, Lemma 10",
        "Explanation": "The reduction from LP(n) to LP(k) defines block-averaged variables x̂_i, ĝ_i, γ̂_{i,j}, but this construction does not guarantee preservation of the original monotonicity constraints x_t ≥ x_{t+1} or the inter-block γ-ordering constraints γ_{s,t} ≤ γ_{s,t+1}. Without verifying that all LP(k) constraints remain satisfied after averaging, the claimed inequality LP-OPT(k) ≤ LP-OPT(n) is unsubstantiated, so the LP lower bound on the algorithm's performance may be invalid."
      },
      {
        "Problem": "Overstated approximation guarantee (Theorem 11)",
        "Location": "Section 3.2, Table 1 and Theorem 11",
        "Explanation": "The paper claims Shuffle achieves a 0.560-approximation. Yet the numerical LP solutions plateau below 0.560 (e.g. 0.5596 at n=300) and only barely reach 0.5600 at n=400. There is no proof that LP-OPT(n)/n converges to or exceeds 0.560 for all sufficiently large n, so the round-up to 0.560 is not rigorously established."
      },
      {
        "Problem": "Invalid extension of a 4-vertex lower bound to general n in Theorem 13",
        "Location": "Section 4, Theorem 13",
        "Explanation": "The Yao’s-lemma lower bound is derived from a uniform distribution over relabelings of a fixed 4-vertex graph, yielding a 19/24 barrier for n=4. The paper applies this as a universal limit for all n, but offers no reduction from larger n to this 4-vertex case. Thus the 0.7916 upper bound holds only for the 4-vertex problem and does not imply the same bound for arbitrary-size graphs."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 18926,
      "thinking": 3520,
      "output": 793
    }
  },
  {
    "entry_id": 170,
    "retraction_id": "1901.07086v3",
    "paper_id": "1901.07086v2",
    "retraction_comment": "The claim of Main Theorem 1 is wrong. The prime counting function on the complex plan does not satisfy the inequality. The prime number Theorem contradicts the claim",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Misapplication of Proposition 2 to non-integer norms",
        "Location": "Proof of Main Theorem 1, Case I",
        "Explanation": "Proposition 2 guarantees at least (k−1) primes in the integer interval [n, kn], but the paper applies it to the Euclidean norms ||z||=√(z₁²+z₂²) and k||z||, which are generally non-integer. There is no justification that the prime‐counting result for integers extends to these real endpoints, so the core step of producing (k−1) primes by mapping lattice points’ moduli to [√(z₁²+z₂²), √((kz₁)²+(kz₂)²)] is invalid."
      },
      {
        "Problem": "Unjustified use of Chebyshev Bias as a uniform lower bound",
        "Location": "Proof of Main Theorem 1, Case I",
        "Explanation": "The argument invokes Chebyshev Bias to assert that ‘almost half’ of the primes in any finite interval are ≡1 (mod 4), hence at least (k−1)/2 of them. In reality, Chebyshev Bias is an asymptotic phenomenon about average behaviour; it does not guarantee that every interval [A,B] contains at least half its primes in a given residue class. No uniform error bounds are provided, so the claim of ≥(k−1)/2 primes ≡1 (mod 4) in the specific interval is unsubstantiated."
      },
      {
        "Problem": "Counting norms does not count Gaussian primes on the segment",
        "Location": "Proof of Main Theorem 1, Cases I and II",
        "Explanation": "The proof equates the number of rational primes p in a real interval to the number of Gaussian primes lying exactly on the straight line segment between (z₁,z₂) and (kz₁,kz₂). However, even if p appears as the norm of some Gaussian prime, that Gaussian prime need not lie on the given segment (it could be anywhere in Z[i]). There is no argument matching each counted prime to a lattice point on the segment, so the geometric conclusion does not follow from the analytic count."
      },
      {
        "Problem": "Inconsistency of the gcd(z₁,z₂)=f(k) condition in axis cases",
        "Location": "Statement of Main Theorem 1 and Case II",
        "Explanation": "The main theorem requires gcd(z₁,z₂)=f(k)>0, but in Case II one coordinate is zero (e.g. z₂=0), so gcd(z₁,0)=|z₁|, forcing z₁=f(k). The proof does not impose or justify this restriction, nor does it verify that the generalization of Bertrand’s Postulate applies to intervals [z₁,kz₁] when z₁=f(k). This inconsistency undermines the axis‐aligned case."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 6093,
      "thinking": 2240,
      "output": 677
    }
  },
  {
    "entry_id": 171,
    "retraction_id": "2204.01118v2",
    "paper_id": "2204.01118v1",
    "retraction_comment": "In the proof of Theorem 5, page 10, line 1O, a term was missing in the r.h.s, that is \\|f''\\|_p^p",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Flawed continuity argument in Proposition 10",
        "Location": "Appendix, Proposition 10",
        "Explanation": "The proof only extracts a subsequence (f∘g_{j_k}) converging to f∘g in L^p, but continuity of T_f requires that the entire sequence f∘g_j converges to f∘g in L^p whenever g_j→g. Existence of one convergent subsequence does not imply convergence of the full sequence, so Proposition 10 does not establish continuity of T_f."
      },
      {
        "Problem": "Unjustified use of the Closed Graph Theorem in Proposition 2",
        "Location": "Section 2, Proposition 2, proof",
        "Explanation": "To prove that the multiplication operator g→φg is bounded on E, the author invokes the Closed Graph Theorem.  However, only convergence in the ambient topology L^1_loc is verified for φg_n→φg, not convergence in the E-norm.  Without showing that φg_n→φg in E whenever g_n→g in E, the graph need not be closed in E×E and the boundedness claim is unfounded."
      },
      {
        "Problem": "Gap in the nonsmooth extension of Dahlberg’s theorem",
        "Location": "Section 12.3, proof of Theorem 1",
        "Explanation": "The author mollifies f to f_j=ω_j∗f and applies a localized version of Proposition 9 to each f_j on a fixed ball, but does not verify that each T_{f_j} actually maps W^{m,p}(ℝ^n)∩S_K into W^{m,p}(ℝ^n)∩S_K with the uniform bounds required by Proposition 12.  Without checking that the support and norm estimates survive the convolution, the reduction to the polynomial case is unjustified."
      },
      {
        "Problem": "Insufficient justification of the high‐order finite‐difference formula",
        "Location": "Section 10.1, Lemma 3",
        "Explanation": "The derivation of the identity Δ_h^m f(x)=∫B_m(t)∑_{|α|=m}f^{(α)}(x+th) h^α dt for f∈W^{m,p}(ℝ^n) is sketched via a one‐dimensional slice and Fubini’s theorem.  No argument is given to justify the use of pointwise Taylor expansions or Fubini interchange with only weak derivatives, so the formula may fail for general Sobolev functions."
      },
      {
        "Problem": "Unproven automatic continuity in Sobolev spaces",
        "Location": "Section 11, Theorem 9",
        "Explanation": "The paper states without proof that any T_f mapping W^{m,p}(ℝ^n) into itself is automatically continuous, citing literature but providing no argument.  Given the subtle failures of continuity in earlier L^p arguments, the claim requires at least a sketch of proof or precise reference; as stated, it is left unsupported."
      }
    ],
    "think_summary": [
      "**Considering continuity and embeddings**\n\nI'm exploring whether g→φg is continuous in the L1,loc category. It seems important that multiplication is continuous from E to L1,loc and that E can embed into L1,loc. I need to confirm that φ g_n approaches φ g in L1,loc, given φ(g_n - g) approaches 0 due to the bounded multiplication. However, I'm wondering about the implications if E isn’t continuously embedded into L1,loc. It's reassuring that they assume E to L1,loc is continuous, but I’m still cautious about the sup definition and potential issues if φ_j varies.",
      "**Examining limitations in theorems**\n\nI'm analyzing the Dahlberg theorem's proof concerning the continuity of f and its implications for the convergence of f_j as j approaches infinity. The proof assumes f is continuous, stemming from prior theorems. This leads me to question if continuity is necessary from the outset, especially when dealing with functions like constants. It's essential to clarify the conditions under which f(0) = 0 hold in relation to operators. I need to ensure I recognize the constant functions and their clearance in Sobolev spaces while addressing potential assumptions in the paper.",
      "**Identifying potential flaws in proofs**\n\nI've reviewed the mathematical arguments, and while I haven't found large mistakes, I'm concerned about the assumption regarding continuity for the composition operator in Theorem 9. They seem to imply continuity without fully justifying it, relying on prior proofs that might need additional assumptions. \n\nIn Proposition 4, the subsequence argument used to establish continuity fails to show that the entire sequence converges, which is critically important. If only a subsequence is converging, it doesn’t necessarily confirm the continuity of the operator. I should keep searching for more issues while considering the assumptions and references used throughout.",
      "**Identifying critical issues in proofs**\n\nI'm analyzing various mathematical propositions and their assumptions, focusing on continuity and compact support. For instance, I notice that g(x) is compactly supported, where f∘g remains smooth if both f and g are smooth. However, there are significant issues, particularly with the continuity proof in Proposition 10. It incorrectly relies on subsequence convergence instead of the full sequence. Potential problems include misuse of the Closed Graph Theorem and questionable assumptions about smoothness in certain propositions. I’ll summarize these problems to prepare a structured response."
    ],
    "token_usage": {
      "input": 13922,
      "thinking": 10496,
      "output": 684
    }
  },
  {
    "entry_id": 172,
    "retraction_id": "2001.10956v2",
    "paper_id": "2001.10956v1",
    "retraction_comment": "The second equation (2.22) is incorrect. The follow-up of the correct equation demands new developments, which I shall provide in a new version soon. wh",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Unjustified spectral expansion in the space of tempered distributions",
        "Location": "Proposition 4.1 (equation (4.3))",
        "Explanation": "The paper asserts without proof that the automorphic distribution B_ℓ admits the expansion (4.3) convergent in S′(ℝ²), citing only earlier work. Convergence of both the continuous integral and the discrete sum in the strong dual topology requires uniform control on their action on test functions. No estimates are given that show the integral over ν or the sum over r,ι converges in S′, so the foundational spectral decomposition used for localization and the final eigenvalue bound is not rigorously established."
      },
      {
        "Problem": "Failure to control the continuous spectrum under localization",
        "Location": "Section 5, use of Φ_N in equation (5.9) and Theorem 5.4",
        "Explanation": "The proof applies a Gaussian cut-off Φ_N(2iπE) to isolate a single discrete eigenvalue from B_disc, but makes no argument that the continuous spectrum (which covers all real parameters λ) is driven to zero by this localization. Without uniform decay estimates for the continuous spectral density around the target λ_r, contributions from the continuous part may remain of order 2^{2N}p^{Nε}, invalidating the claimed exponential separation between the main term and error terms."
      },
      {
        "Problem": "Lack of domain justification for the key operator identity",
        "Location": "Proposition 2.1 (equation (2.19))",
        "Explanation": "The identity (p^{-1/2+iπE♮} + p^{1/2−iπE♮})N = T_dist^p N is pivotal, yet its proof checks only the formal Fourier expansions of basis Hecke and Eisenstein distributions. There is no verification that this operator equality holds in the sense of tempered distributions for the full automorphic distribution algebra, nor any discussion of convergence of the underlying series. If this fundamental operator relation fails on the required domain, the entire approach collapses."
      },
      {
        "Problem": "Unspecified seminorm degree in the scaling estimate",
        "Location": "Proposition 3.1 (equation (3.27) and subsequent remark)",
        "Explanation": "The bound |⟨B₁, q h_q⟩| ≤ (q+q^{-1})^ε‖h‖ is stated with a seminorm ‖h‖ of some unspecified degree A in S(ℝ²). Later, in choosing the localization parameter β (inequality (5.20)), the argument requires precise knowledge of A to ensure an admissible range for β. Since A is never linked quantitatively to ε or the constants in Proposition 3.1, one cannot guarantee that β can be chosen to satisfy all required inequalities, breaking the final contradiction."
      },
      {
        "Problem": "Insufficient control of discrete spectral tail",
        "Location": "Proposition 4.2 (proof following equation (4.7))",
        "Explanation": "To prove convergence of the discrete part B_disc in S′, the paper appeals to the asymptotic µ_r∼√(48 r) and a crude bound φ(k)=O(|k|^C), then performs integrations by parts to suppress powers of µ_r. However, there is no uniform bound on the exponent C as r varies, nor any detailed estimate of the sum over r and k. Without a uniform decay rate in r and k, absolute convergence in S′ cannot be concluded, undermining the very decomposition used later."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 17361,
      "thinking": 6144,
      "output": 818
    }
  },
  {
    "entry_id": 173,
    "retraction_id": "1407.6492v2",
    "paper_id": "1407.6492v1",
    "retraction_comment": "This paper has been withdrawn by the main author due to the Table 1 and equation 2 errors",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Inconsistent dataset sizes in reported experiments and comparison table",
        "Location": "Table I, page 164",
        "Explanation": "Table I lists the proposed method as using 15,000 training samples and 50,000 testing samples, yet the paper states the entire dataset contains only 20,000 samples. This discrepancy indicates a data‐reporting error or misunderstanding of the experimental protocol, undermining the validity of the reported accuracy figures."
      },
      {
        "Problem": "Lack of a properly held‐out or independent test set and potential data leakage",
        "Location": "Section 3 (Practical Result and Comparative Result), page 164",
        "Explanation": "The paper does not describe how the 4-fold cross‐validation splits were generated or whether samples from the same registration forms (e.g. repeated postal codes) could appear in both training and test folds. Without clear sample partitioning and duplicate removal, there is a high risk of overlap between training and test data, leading to overoptimistic accuracy estimates."
      },
      {
        "Problem": "Invalid comparison of accuracy results across heterogeneous datasets and protocols",
        "Location": "Section 3 and Table I, page 164",
        "Explanation": "The authors compare their recognition rates to those reported in other studies without ensuring that the datasets, sample sizes, preprocessing steps, or evaluation protocols are equivalent. Such cross‐paper performance comparisons are misleading and cannot support claims of superiority."
      },
      {
        "Problem": "Missing specification and selection criterion for the parameter k in the K-NN classifier",
        "Location": "Section 2.4.4 (Classification), page 163",
        "Explanation": "Although the Euclidean distance metric is defined, the paper does not state what value of k was used in the K-NN classifier, nor how it was chosen or validated. Without this critical detail, the results cannot be reproduced, and it is unclear whether k was optimally selected or inadvertently tuned on the test data."
      },
      {
        "Problem": "Undefined behavior in transit feature computation when vertical transitions count is zero",
        "Location": "Section 2.3 (Transit Feature), page 163",
        "Explanation": "The transit feature is defined as the ratio of horizontal to vertical run‐length counts within a block. If a block has zero vertical transitions, this ratio is mathematically undefined, yet the paper does not describe how such cases are handled, potentially leading to erroneous or inconsistent feature values."
      }
    ],
    "think_summary": [
      "**Questioning dataset clarity**\n\nI'm wondering if they used a standard dataset or created their own. They reference a \"standard dataset,\" but mention it comes from different registration forms, so it's not truly identical to the cited one. They also don't clarify if the splits were random or if there are duplicates, which raises concerns about potential data leakage and overestimation of results. Plus, they mention KNN but don’t specify the k value, nor do they provide details on distance weighting or normalization—this could lead to overfitting issues.",
      "**Examining normalization effects**\n\nI’m concerned that the normalization might distort the aspect ratio, as the bounding box uses a fixed 60x30 size regardless of the original width/height ratio. This non-uniform scaling could affect different digit shapes, and while they report high accuracy, the lack of discussion on this issue is troubling. They haven't compared their results to a solid baseline, and though they claim superior accuracy with a larger dataset, differences in dataset protocols make direct comparisons flawed. Plus, KNN might struggle with classification speed due to computational costs, yet they haven’t evaluated timing or speed properly."
    ],
    "token_usage": {
      "input": 11200,
      "thinking": 2240,
      "output": 557
    }
  },
  {
    "entry_id": 174,
    "retraction_id": "1802.06370v4",
    "paper_id": "1802.06370v3",
    "retraction_comment": "This paper has been withdrawn by the author due to a crucial explanation error of redundancy",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Incorrect fundamental PDE for Hamiltonian construction",
        "Location": "Section 2, Equation (2.2)",
        "Explanation": "The key equation (2.2) is derived by inserting ẋ=p/m and ṗ=−V′(x) into d²x/dt²=Hₚₚṗ+Hₚₓẋ, but for a generic Hamiltonian H(p,x) one has ẋ=Hₚ≠p/m and ṗ=−Hₓ≠−V′ in advance. The correct consistency condition for reproducing ẍ=−V′/m is −HₚₚHₓ+HₚₓHₚ+V′/m=0, not (2.2). Using the wrong PDE invalidates the derivation of the “Hamiltonian zoo.”"
      },
      {
        "Problem": "Failure to account for time reparametrization",
        "Location": "Abstract and Section 2 (general construction)",
        "Explanation": "The paper claims the new Hamiltonians produce the same equation of motion, but in fact they reproduce Newton’s law only under a rescaled time variable t→t_λ (shown later in Section 3). Throughout Section 2 the authors implicitly assume the standard time parameter, leading to misleading assertions of dynamical equivalence without clarifying that the flows differ by a nontrivial time‐scaling."
      },
      {
        "Problem": "Ill-posed Legendre transform for non-quadratic H",
        "Location": "Section 2.4, Equations (2.33)–(2.37)",
        "Explanation": "The hierarchy of Lagrangians is obtained by Legendre transforming exponential Hamiltonians H_j(p,x), yet these H_j need not be convex in p (∂²H_j/∂p² may vanish or change sign). A globally well-defined Legendre transform requires strict convexity; without it the resulting L_j(˙x,x) can be multivalued or singular, undermining the claimed Lagrangian hierarchy."
      },
      {
        "Problem": "Circular use of energy conservation condition",
        "Location": "Section 2, Equation (2.7)",
        "Explanation": "The additional relation 0=ṗ∂F/∂p+(p/m)∂F/∂x is simply dF/dt=0 (energy conservation) for any autonomous Hamiltonian F. Invoking it to narrow down F before constructing H is tautological and does not yield genuinely new constraints beyond separability assumptions, making the method formally circular."
      },
      {
        "Problem": "Misleading dismissal of general H=f(H₀) approach",
        "Location": "Section 3, Remark 5",
        "Explanation": "The authors argue that writing H as an arbitrary function f(H₀) cannot capture the detailed time-scaling hierarchy, yet their own constructions are special cases of H=f(H₀) (exponentials, iterated exponentials). This overstates the novelty of their method and misrepresents the scope of the simpler f(H₀) ansatz."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 15546,
      "thinking": 5568,
      "output": 716
    }
  },
  {
    "entry_id": 175,
    "retraction_id": "1403.0340v2",
    "paper_id": "1403.0340v1",
    "retraction_comment": "This paper has been withdrawn by the author because of Projection measurement tacit usage (while generalized one should have been used)",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Neglect of the non-orthogonality of generalized coherent states",
        "Location": "Section 2 (Eqs. (3)–(6)) and Section 4 (Eqs. (20)–(21))",
        "Explanation": "Generalized coherent states are overcomplete and non-orthogonal in their continuous labels (q,p).  The paper nonetheless treats fixed-(q,p) GCS as if they were perfectly distinguishable orthonormal basis vectors when constructing stochastic Bell states and proposing Bell-measurements.  In reality one cannot perform the assumed perfect projective measurements on non-orthogonal GCS, so the claimed Bell bases and teleportation protocols are not physically realizable."
      },
      {
        "Problem": "Absence of a physical mechanism to prepare or measure the \"stochastic qubits\"",
        "Location": "Section 3 (Eqs. (12)–(15))",
        "Explanation": "The key object—an entangled state of a qubit and a micro-detector defined abstractly as a generalized coherent state—is never shown to arise from any interaction Hamiltonian or realistic measurement process.  Without a concrete model for how to prepare, control or read out these joint qubit–apparatus states, the subsequent claims about their use in entanglement and teleportation remain purely formal and of doubtful applicability."
      },
      {
        "Problem": "Unspecified projection onto the J=½ subspace when l=1",
        "Location": "Section 3, case b) (l=1 discussion following Eq. (14))",
        "Explanation": "Coupling a spin-½ qubit to a spin-1 detector produces total spin J=½ and J=3/2 sectors.  The authors declare that only the J=½ sector constitutes the \"stochastic qubit\" but do not explain how the J=3/2 component is physically excluded or suppressed.  In the absence of such a projection mechanism, the system actually lives in a four-dimensional Hilbert space, not a two-level qubit, invalidating the identification of the coupled system as a genuine qubit."
      },
      {
        "Problem": "Invalid teleportation measurement for two-state micro-detector case",
        "Location": "Section 6.1 (Eqs. (44)–(47))",
        "Explanation": "To teleport an arbitrary d-dimensional state one needs a maximally entangled orthonormal basis of dimension d².  Here the joint qubit+two-state detector has dimension 4, so teleportation requires 16 orthonormal Bell-type states.  Although the authors enumerate 16 formal combinations ((α,β) and ψ/ϕ types), they never prove these are orthonormal or can be perfectly distinguished.  Without a valid 16-element orthonormal basis the teleportation protocol cannot work."
      },
      {
        "Problem": "Teleportation only succeeds for special input states, contradicting claims of generality",
        "Location": "Section 6.2 (Eqs. (66)–(69))",
        "Explanation": "When working out the teleportation with a two-state detector the authors find that only input states of the restricted form (66) can be faithfully teleported; arbitrary stochastic qubit–detector states cannot.  This contradicts their earlier suggestion that one could perform Bell-based teleportation in full generality, and shows their protocol is far less powerful than claimed."
      }
    ],
    "think_summary": [
      "**Analyzing teleportation mechanics**\n\nIn this teleportation scenario, they aim to transmit a qubit-device system from Alice to Bob. Typically, teleportation utilizes a Bell basis for qubits (dim 2), but here they deal with a 4-dimensional state that necessitates 16 outcomes. They’re using a Bell basis involving two stochastic qubits but only perform a four-outcome measurement. This seems limited for teleporting a 4-dimensional state, suggesting a potential mismatch between the measurement outcomes and the state dimensions involved.",
      "**Evaluating teleportation limitations**\n\nIt seems like they are overreaching in claiming they can teleport a combined qubit-device state but only managing to transfer the qubit while ignoring the device itself. By allowing only certain states for teleportation, they contradict their earlier claims of standard teleportation. Specifically, they assume fixed measurement outputs while disregarding that continuous parameters, which cannot be perfectly distinguished, invalidate their approach. This oversight in treating coherent states as orthogonal reveals significant flaws in their teleportation argument, leading to unrealistic measurements and conclusions.",
      "**Examining teleportation assumptions**\n\nThey seem to ignore the continuous degrees of freedom while defining discrete states by fixing values for q and p, which limits their completeness. By doing so, they treat sets of states as orthonormal without validating this claim. Although they demonstrate with fixed parameters, they miss important aspects of overcompleteness. They focus only on specialized spin states, like J=1/2, while conveniently neglecting J=3/2. This raises concerns about their teleportation claims, as they fail to address the complexities of continuous labels needed for proper teleportation."
    ],
    "token_usage": {
      "input": 14578,
      "thinking": 3264,
      "output": 760
    }
  },
  {
    "entry_id": 176,
    "retraction_id": "2106.01585v2",
    "paper_id": "2106.01585v1",
    "retraction_comment": "The proof of Lemma 3.1 has a gap. While there is exponential mixing for Holder functions, the rate of the mixing depends on the Holder exponent of the function. This leads to a vicious circle",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Undefined global smoothing operator on the nilmanifold",
        "Location": "Section 2.4 (equations 2.7–2.9)",
        "Explanation": "The authors define smoothing by convolution φ_J * f on R^d but never explain how to lift this to the compact nilmanifold G/Γ (no partition-of-unity or fundamental-domain construction is given). As stated, φ_J * f is not well-defined on G/Γ, so the crucial smoothing estimates (2.8),(2.9) have no valid domain of application and the entire KAM smoothing step lacks a foundation."
      },
      {
        "Problem": "Misapplication of Euclidean Sobolev embedding to the compact manifold",
        "Location": "Theorem 2.6",
        "Explanation": "They quote the Euclidean embedding H^{k+1+d/2}(R^d)→C^k(R^d) but apply it directly to functions on G/Γ without introducing local charts, a partition of unity, or verifying uniform control of the embedding constants. Without establishing the manifold Sobolev embedding, the step upgrading distributional estimates to Hölder (and hence C^∞) regularity is unjustified."
      },
      {
        "Problem": "Circular use of uniqueness up to affine centralizer to bootstrap smoothness",
        "Location": "Remark after (3.8)",
        "Explanation": "To promote the conjugacy constructed at each finite C^{k+k′} level to full C^∞, the authors invoke Proposition 2.3 (uniqueness of conjugacy up to affine) to argue successive conjugacies agree up to an affine map and hence improve regularity. But uniqueness itself requires an a priori smooth conjugacy; using it to produce higher‐regularity conjugacies is circular and does not legitimately bootstrap the regularity."
      },
      {
        "Problem": "Unjustified extension of exponential mixing estimates to distributions",
        "Location": "Lemma 3.1 and Lemma 3.3",
        "Explanation": "Exponential mixing (Proposition 2.2) controls correlations of Hölder functions. The paper repeatedly applies these bounds to series of distributions paired with test functions, without establishing continuity of the mixing estimate in the distributional topology. In particular, convergence of the formal sums defining ω_λ and the Sobolev‐space bounds for D^αω_λ lack a rigorous underpinning."
      },
      {
        "Problem": "Ill–posed choice of smoothing scales J_n in the KAM iteration",
        "Location": "Section 4 (definition of J_n and the inductive step)",
        "Explanation": "The iteration requires choosing integer smoothing parameters J_n≫1, but J_n is defined by J_n=ε_n^{−5/[2(k_n−σ)]}, which need not be an integer (nor uniformly bounded below). Without a mechanism to round or adjust J_n while preserving the required smoothing estimates, the key inductive inequalities fail, and the convergence scheme collapses."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 17236,
      "thinking": 1856,
      "output": 677
    }
  },
  {
    "entry_id": 177,
    "retraction_id": "1509.00106v3",
    "paper_id": "1509.00106v2",
    "retraction_comment": "This paper has been withdrawn by the author due to a crucial error in equation (21)",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Initialization of the smoothness parameter γ₀ requires knowing ‖x₀ − x*‖ (R₀) and the prox-diameter D_{UA}, which depend on the unknown solution and geometry of U.",
        "Location": "Theorem 1, equation (26) and preceding discussion",
        "Explanation": "The choice γ₀ = c R₀ √(2 DUA/(k₀−c̄)) and the complexity bound both involve R₀=‖x₀−x*‖ and DUA. In practice these constants are unavailable, so γ₀ cannot be set as prescribed, undermining implementability and the claimed optimal rate."
      },
      {
        "Problem": "The core subproblem u*_{γ}(x)=argmax_{u∈U}⟨Aᵀx,u⟩−ϕ(u)−γ b(Au) may be as hard as the original problem unless U,ϕ and A have special structure.",
        "Location": "Algorithm 1, Step 3",
        "Explanation": "Algorithm 1 relies on solving a composite smoothing maximization over U at each iteration. Without further assumptions (e.g., A orthogonal or separable structure), this subproblem is intractable in general, so the algorithm cannot be applied broadly as claimed."
      },
      {
        "Problem": "The update rule τₖ = c̄/(k+ k₀) often gives τ₀>1, violating the requirement τₖ∈(0,1] used in the accelerated scheme.",
        "Location": "Lemma 5 and Algorithm 1 initialization",
        "Explanation": "They set c̄ = max{L_b,1+1/k₀}≥1 and τ₀=c̄/k₀≥1. If c̄>k₀, then τ₀>1, so the extrapolation step x̂ₖ=(1−τₖ)xₖ+τₖ x̃ₖ is not a convex combination, breaking the assumptions of Lemmas 2–4 and invalidating the descent analysis."
      },
      {
        "Problem": "The primal recovery and feasibility guarantees for constrained problems assume strong duality (existence of a saddle point) without any constraint qualification.",
        "Location": "Theorem 2",
        "Explanation": "The proof of convergence for the averaged primal sequence uses saddle-point arguments and sets F*=-ϕ*, but strong duality (e.g., via Slater’s condition) is never stated. Without it, the dual may have a nonzero gap and the recovered ūₖ need not approach feasibility or optimality."
      },
      {
        "Problem": "The analysis hinges on U being bounded so that the prox-diameter D_{UA} is finite, yet in the general setting U=dom(f*) can be unbounded.",
        "Location": "Section 1 (definition of U) and Lemma 1, inequality (9)",
        "Explanation": "Lemma 1 and the error bound fγ(x)≤f(x)≤fγ(x)+γ D_{UA} require D_{UA}<∞. However, when f is arbitrary convex, dom(f*) need not be bounded. The paper does not restrict to bounded domains, so the smoothing error and complexity bounds may not hold."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 28698,
      "thinking": 3328,
      "output": 771
    }
  },
  {
    "entry_id": 178,
    "retraction_id": "1905.01749v2",
    "paper_id": "1905.01749v1",
    "retraction_comment": "There is a serious flaw with Theorems 2-4 which makes their results incorrect. We are working on fixing the issue and uploading a new version of this paper. This flaw, however, does not in any way affect the correctness of evaluations and the gains obtained using the proposed approach",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "The partition‐merging heuristic explores only one merge sequence (always merging the two fastest partitions), severely restricting the search space and missing potentially better groupings that could alleviate shared network bottlenecks.",
        "Location": "Section V-D (Hierarchical Partitioning), Algorithm 4, merging loop",
        "Explanation": "By only ever merging the two partitions with the smallest estimated completion times, Iris builds a single linear hierarchy of |Pbase| configurations. In real topologies, optimal partitionings often require merging different subsets—e.g., partitions whose trees overlap on congested links—to minimize completion times. The heuristic’s narrow merge strategy can therefore omit much better partitions, invalidating the claim of near-optimal performance."
      },
      {
        "Problem": "The selection of the best hierarchy layer minimizes unweighted average completion time instead of the weighted completion time dictated by the objective vector ωR, so mixed‐objective requirements are not actually optimized.",
        "Location": "Section V-D, Algorithm 4, computation of κl and layer selection",
        "Explanation": "Although the system accepts a binary objective vector ωR to indicate which receiver ranks matter, the merging evaluation aggregates completion times uniformly (κl = Σ|P|·κP). This ignores ωR’s weights—i.e., it treats all receivers equally—so the final partitioning may fail to prioritize the completion times of receivers marked with a '1', breaking correctness for mixed objectives."
      },
      {
        "Problem": "Forwarding‐tree selection uses the full original request volume VR in edge weights rather than each partition’s actual remaining volume, misestimating load and biasing tree choice.",
        "Location": "Section V-A (Choosing Forwarding Trees), Algorithm 1, weight We = Le + VR/Be",
        "Explanation": "All partitions, regardless of their size or residual data V[res]P, use the total VR in the additive term VR/Be. After partial progress or for small partitions, this overstates the expected load on each edge and can steer the Steiner‐tree heuristic toward suboptimal trees that do not reflect the true remaining transfer volume."
      },
      {
        "Problem": "The theoretical optimality analysis in Section IV assumes an idealized ‘infinite‐core’ topology (single uplink/downlink per node, no core bottlenecks), which does not hold in real inter‐datacenter networks; hence the optimal partitioning structure does not generalize.",
        "Location": "Section IV (Optimal Partitioning of Receivers on a Relaxed Topology), Theorems 1–4",
        "Explanation": "All proofs rely on a model where the network core has infinite capacity and each datacenter has only one ingress/egress link. In practice, traffic shares finite-capacity core links and nodes have multiple interfaces. Thus the derived receiver‐grouping rules (e.g., always grouping by downlink‐rate order) may be suboptimal or even counterproductive in real WANs, undermining the paper’s theoretical foundation."
      },
      {
        "Problem": "Minimum‐completion‐time estimation simulates per‐timeslot max‐min fair rates using current bandwidth estimates but ignores future arrivals/departures and variability, leading to inaccurate predictions and poor partition decisions.",
        "Location": "Section V-B (Estimating Minimum Completion Times), Algorithm 2",
        "Explanation": "Algorithm 2 assumes that available bandwidth Be(t) remains fixed and that no new transfers arrive, yet the system processes requests online. In reality, concurrent transfer dynamics can significantly change link loads, so the static per‐timeslot simulation can yield misleading κP estimates. Decisions based on these inaccurate forecasts may produce partitions and trees that perform poorly under true network conditions."
      }
    ],
    "think_summary": [
      "**Analyzing performance estimates**\n\nI’m thinking about how minimum completion time estimates assume static bandwidth and no new transfers. In a dynamic system, new requests can show up and change resource availability, which makes these estimates less accurate. Decisions based on faulty estimates could lead to poor performance outcomes and affect the validity of the claims being made. So, understanding this dynamic is crucial for developing better heuristics and making informed decisions. It's important to keep this context in mind!"
    ],
    "token_usage": {
      "input": 29183,
      "thinking": 3648,
      "output": 828
    }
  },
  {
    "entry_id": 179,
    "retraction_id": "1312.6644v4",
    "paper_id": "1312.6644v3",
    "retraction_comment": "This paper has been withdrawn by the author because Eqs. (7) and (8) are not correct. An update with corrected expressions and plots will follow soon",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Purely harmonic model ignores anharmonicities essential for normal diffusive heat transport and fails near structural phase transitions where the potential is strongly non-linear.",
        "Location": "Page 2, Section II (Model and Methods), around Eq.(1)",
        "Explanation": "The analysis is entirely based on a quadratic (harmonic) expansion of the ion interactions. However, Fourier’s law and normal heat conduction arise from anharmonic phonon-phonon scattering, which is absent in a purely harmonic model. Moreover, near the zig-zag and helicoidal transition points the effective potential becomes quartic and the harmonic approximation breaks down, invalidating the predicted transport behavior."
      },
      {
        "Problem": "Infinite high-frequency cutoff limit and renormalization lead to divergent potential shifts that are neither regularized nor justified.",
        "Location": "Page 3, just before Eq.(5) – “In that limit Ĝ(s)^{-1} = … and the renormalized potential is V_R = V − 2γ(0)”",
        "Explanation": "Taking the cutoff Λ→∞ makes the dissipation kernel γ(0) divergent. Subtracting 2γ(0) from the physical coupling matrix V therefore introduces an infinite, undefined renormalization. Without a clear regularization or renormalization prescription, the computed normal-mode frequencies and all subsequent results become ill-posed."
      },
      {
        "Problem": "Laser cooling is modeled as equilibrium Ohmic bosonic reservoirs, which does not capture the actual non-equilibrium, frequency-dependent noise of Doppler cooling.",
        "Location": "Page 2, Section II, definitions of H_E and spectral density I^(l)(ω) (Eq.(2))",
        "Explanation": "Laser cooling of trapped ions arises from driven atomic transitions and photon recoil, yielding non-thermal, colored noise and non-Markovian dissipation. Treating it as coupling to an equilibrium bath of harmonic oscillators with an Ohmic spectrum oversimplifies the physics and may misrepresent both the dissipation rates and noise correlations essential to heat transport."
      },
      {
        "Problem": "Definition of local temperature via momentum variance may not correspond to a thermodynamic temperature in a non-equilibrium steady state.",
        "Location": "Page 3, paragraph preceding Eq.(3), where T_i is set by ⟨P_i^2⟩",
        "Explanation": "Assigning a temperature to each ion coordinate by equating its momentum variance to that of a thermal oscillator assumes local thermal equilibrium and mode decoupling. In the non-equilibrium Gaussian steady state of a coupled harmonic network, momentum marginals are Gaussian but not generally thermal, so the inferred “temperature profiles” may be physically misleading."
      },
      {
        "Problem": "Stability of the generalized eigenvalue spectrum is assumed but not demonstrated, jeopardizing the validity of the residue-theorem integrals.",
        "Location": "Page 3–4, around Eqs.(5)–(8)",
        "Explanation": "The analytic method relies on the poles {s_α} of Ĝ(s)^{-1} lying strictly in the left half-plane (Re(s_α)<0) so that the contour integrals converge. No proof or numerical check is provided that the combined ion-plus-reservoir system yields a stable spectrum, especially near the phase transitions or in the weak-coupling limit where modes can approach imaginary axis, invalidating the residue-based evaluation."
      }
    ],
    "think_summary": [
      "**Identifying Major Issues**\n\nI’m thinking about the initial steps for addressing the major issues. I want to focus on the first four or five significant problems. I’ll need to create JSON array entries that include three key fields for each entry: Problem, Location, and Explanation. This structured approach will help clarify the issues we’re dealing with, making it easier to communicate and resolve them efficiently. Let's get started on identifying those specific problems!"
    ],
    "token_usage": {
      "input": 12657,
      "thinking": 3584,
      "output": 749
    }
  },
  {
    "entry_id": 180,
    "retraction_id": "2103.11473v2",
    "paper_id": "2103.11473v1",
    "retraction_comment": "The proof of the cluster property (E4) for the superposition is wrong. [REDACTED-NAME] and [REDACTED-NAME] gave a counterexample s.t. the statement of cluster property can't hold in its full generality",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "The cluster axiom (E4) fails for non‐trivial superpositions of masses",
        "Location": "Theorem 2.7 (cluster property), p. 6–7",
        "Explanation": "If Sₙ,ρ=∫Sₙ,m ρ(dm) with ρ not a single Dirac measure, then for test‐data f, g one finds\nlim_{t→∞}∑_{n,k}S_{n+k,ρ}(Θf_n^*⊗T_tg_k)=∫A(m)B(m)ρ(dm)\nwhile\n(∑_nS_{n,ρ}(Θf_n^*))(∑_kS_{k,ρ}(g_k))=(∫A(m)ρ(dm))(∫B(m)ρ(dm)),\nand in general ∫A B≠(∫A)(∫B). Thus the required factorization fails unless ρ is supported at a single mass."
      },
      {
        "Problem": "Misidentification of the vacuum subspace in the direct–integral Hilbert space",
        "Location": "Proof of Theorem 2.7, direct integral argument, p. 7",
        "Explanation": "The fibrewise generators H_m each have one‐dimensional zero‐eigenspace span{Ω_m}, but the direct integral\nH_ρ=∫⊕H_m ρ(dm)\nhas a zero‐eigenspace isomorphic to L²(ρ), not a one‐dimensional span{[Ω]_ρ}. In particular any measurable scalar function c(m) gives an invariant vector c(m)Ω_m, so uniqueness of the vacuum (and hence cluster) fails."
      },
      {
        "Problem": "Omitted verification of measurability and direct‐integral technical hypotheses",
        "Location": "Construction of H_ρ as a direct integral, p. 7",
        "Explanation": "The authors assert that (H_m)_{m∈[m₀,∞)} is a measurable field of Hilbert spaces and form the direct integral without checking the standard hypotheses (measurability of the field, separability, choice of fundamental sequence, etc.) required by the theory in [1]."
      },
      {
        "Problem": "Lack of a positivity and continuity check for the characteristic functional μ̂^ρ",
        "Location": "End of proof of Theorem 2.7, p. 8",
        "Explanation": "The paper defines μ̂^ρ(ϕ)=∑(iⁿ/n!)S_{n,ρ}(ϕ⊗⋯⊗ϕ) and appeals to Bochner–Minlos for existence and uniqueness, but does not verify that μ̂^ρ is positive‐definite or continuous on the nuclear Schwartz topology—both are nontrivial and essential for the theorem."
      },
      {
        "Problem": "Insufficient justification of the distribution‐estimate (E0) via L²‐bounds",
        "Location": "Verification of (E0) in Theorem 2.7, p. 6",
        "Explanation": "The authors bound |S_{2n,m}(f₁,…,f_{2n})| by ∏‖f_i‖_{L²} and then assert a Schwartz‐seminorm estimate without demonstrating how the L²‐norms are controlled by the chosen seminorms (k·k)_p on S(ℝᵈ). The requisite embedding S→L² and continuity estimates are not provided."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 8955,
      "thinking": 5184,
      "output": 784
    }
  },
  {
    "entry_id": 181,
    "retraction_id": "1303.6851v2",
    "paper_id": "1303.6851v1",
    "retraction_comment": "This paper has been withdrawn by the author due to a loophole in the argument of the classical bound",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Invalid classical correspondence by defining Pα = Bα + Bβ for incompatible settings",
        "Location": "Section III A, Equations (17)–(19)",
        "Explanation": "In the standard CHSH scenario, Bα and Bβ are outcomes of mutually exclusive measurement settings and cannot be summed in a single run. The paper’s classical model treats Pα = Bα + Bβ as if both Bα and Bβ are simultaneously well-defined random variables, which violates the measurement-context exclusivity inherent even in classical analogues of hidden-variable models. This flawed identification renders the derived “classical” bound of 2√2 meaningless as a comparison to the bona fide local bound of 2."
      },
      {
        "Problem": "Conflation of locality with the uncertainty principle derivation",
        "Location": "Section II, Equations (14)–(15)",
        "Explanation": "Recovering the Tsirelson bound via Cauchy–Schwarz (uncertainty) inequalities on operator sums does not address or supplant the physical locality assumptions used in Bell’s theorem. The derivation merely rephrases known operator-norm arguments and does not show that quantum correlations admit a local-realist explanation. Thus concluding that nonlocality is “improper” from this derivation is logically unsound."
      },
      {
        "Problem": "Misrepresentation of the PR box as inherently inconsistent",
        "Location": "Section III B, page 5",
        "Explanation": "The critique demands the existence of a global joint probability distribution over all four binary outcomes (A, A′, B, B′), yet the PR box model is defined only by well-specified no-signalling conditional probabilities for compatible input pairs. Requiring joint distributions for incompatible measurement contexts is a counterfactual imposition not required by the operational definition of the PR box, so the alleged inconsistency arises from a misunderstanding of contextuality rather than a flaw in the PR-box correlation itself."
      },
      {
        "Problem": "Neglect of locality assumptions in the ‘classical’ Tsirelson-type bound",
        "Location": "Section III A, entire subsection",
        "Explanation": "By allowing arbitrary correlations between Bα and Bβ, the new classical model abandons the locality (and measurement independence) constraints that underpin the standard CHSH bound of 2. The resulting 2√2 bound therefore describes a non-local or conspiratorial classical hidden-variable theory, and cannot be legitimately compared to the quantum bound under local realism."
      },
      {
        "Problem": "Overgeneralized conclusion that nonlocality is 'improper'",
        "Location": "Introduction and Conclusion",
        "Explanation": "The paper repeatedly asserts that nonlocality is physically invalid based on the presented uncertainty-principle arguments, yet it overlooks the empirical fact that Bell-inequality violations under spacelike separation imply the failure of local hidden-variable models, independent of the Tsirelson bound derivation. The conclusion conflates mathematical bounds on operator sums with the foundational notion of locality tested in Bell experiments, leading to an unsound dismissal of nonlocality."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 8952,
      "thinking": 2752,
      "output": 698
    }
  },
  {
    "entry_id": 182,
    "retraction_id": "1208.2556v2",
    "paper_id": "1208.2556v1",
    "retraction_comment": "This paper has been withdrawn by the author due to some nodes in the graph have not been taken into account",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Unsupported generality and incorrect formulation of Lemma 2.1",
        "Location": "Lemma 2.1 (Page 2)",
        "Explanation": "The author asserts without justification that every nontrivial Collatz cycle must have the specific 7-term parametric form {36k+16, 18k+8, 9k+4, …, 12k+5}. There is no argument that all cycles must exhibit exactly that fixed parity pattern or length—longer or differently patterned cycles are a priori possible. This overrestriction invalidates all subsequent deductions based on this form."
      },
      {
        "Problem": "False exclusion of values divisible by 3",
        "Location": "Section 3, first paragraph (Page 3)",
        "Explanation": "The paper claims that any cycle entry divisible by 3 cannot occur because it cannot be reached by the 3n+1 map. However, even numbers divisible by 3 can arise via the n/2 step from n≡6 (mod 18). Excluding residues 0,3,6 (mod 9) is therefore unsound and corrupts the modular case analysis."
      },
      {
        "Problem": "Misuse of inverse maps κ⁻¹ and τ⁻¹ without integrality checks",
        "Location": "Backwards iteration steps in Section 3 (Pages 4–7)",
        "Explanation": "Throughout the backward tracing of a hypothetical cycle, the author applies κ⁻¹(n)=(n−1)/3 and τ⁻¹(n)=2n without verifying that (n−1) is divisible by 3 when κ⁻¹ is used. Many computed predecessors are not integers, so the stepwise contradictions are invalid."
      },
      {
        "Problem": "Incomplete directed graph of residue transitions",
        "Location": "Figure 1 and surrounding text (Page 4)",
        "Explanation": "The directed graph purports to capture all possible transitions of residues mod 9 under C and its inverses, but it omits paths that pass through residues divisible by 3 or involve multiple successive κ or τ steps. As a result, the case-by-case elimination of k mod 9 is based on an incomplete state graph."
      },
      {
        "Problem": "Faulty final inference ‘no k ⇒ no cycle’",
        "Location": "Conclusion of Theorem 3.1 (Page 7)",
        "Explanation": "After showing that no natural k fits the overrestricted parametric form, the author concludes that no nontrivial cycle exists. This is circular: it only shows there is no cycle of the assumed template, not that no cycle of any form can exist."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 7898,
      "thinking": 1344,
      "output": 605
    }
  },
  {
    "entry_id": 183,
    "retraction_id": "1804.05635v2",
    "paper_id": "1804.05635v1",
    "retraction_comment": "Theorem 1 is questionable and needs revision. Others parts should also be modified accorrdingly. Before the new version is ready, this version should not be referred",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Reliance on unverifiable win declarations (x_i) with no enforcement mechanism",
        "Location": "Section III.A (Definition 1 and best-response discussion)",
        "Explanation": "The scheme assumes that bidders truthfully declare xi (win/loss) and that any false declaration will be detected and penalized. However, no concrete mechanism is provided for detecting or enforcing penalties for incorrect xi. Without a reliable audit or enforcement protocol, malicious bidders could misreport xi to manipulate the outcome, invalidating both correctness and incentive properties."
      },
      {
        "Problem": "Incomplete convergence proof under asynchronous execution",
        "Location": "Section III.C (Theorem 1 and subsequent convergence argument)",
        "Explanation": "The authors argue that each agent’s bid sequence is finite and that xi must eventually stabilize, but they do not specify a formal model of asynchrony or fairness (e.g., bounded delays, guaranteed activations). In a real distributed system without such guarantees, there is no proof that the global auction will converge to a stable state."
      },
      {
        "Problem": "Unsubstantiated incentive properties in dynamic setting",
        "Location": "Section III.A and III.B (dynamic game model and best-response rules)",
        "Explanation": "While the paper claims the critical-value pricing remains strategy-proof, there is no formal game-theoretic analysis of equilibria in the proposed dynamic, repeated-interaction model. Bidders may strategically time their updates or misreport b_i within [c_i, v_i] to influence rivals’ critical values, undermining any claims of strategy-proofness."
      },
      {
        "Problem": "Potential inaccuracy of decentralized payment computation",
        "Location": "Section III.D (Algorithm 4 for key successor and payment)",
        "Explanation": "Algorithm 4 requires each winner to identify its key successor and compute its critical payment based on the entire set of non-winning bids. Yet bidders only maintain local views of neighbors’ bids. The paper does not prove that local neighbor information suffices to correctly identify the global key successor in all multi-unit, multi-type settings."
      },
      {
        "Problem": "Flawed consistency proof between centralized and decentralized outcomes",
        "Location": "Section III.C (Theorem 4)",
        "Explanation": "Theorem 4’s induction assumes that lowering bids (b_i ≤ v_i) cannot alter the relative ranking of requests in a way that affects winning status. However, in BRF-based rankings, reducing b_i may demote a bid below a conflicting neighbor, causing a different feasible allocation. The proof does not account for such reordering, so the claim that both algorithms always produce the same winner set is not generally valid."
      }
    ],
    "think_summary": [
      "**Identifying critical errors**\n\nI'm focusing on critical errors or unsoundness that could invalidate conclusions, while steering clear of minor issues. I'm aiming to pinpoint up to five major problems in a structured format. I plan to parse the paper for flaws in assumptions, proofs, algorithms, and claimed contributions. \n\nThere’s a key concern regarding the decentralization process where agents need to communicate. Another issue arises with the main contributions where the assumption of agents having complete knowledge of all neighbors and updates might not hold up without full network broadcasts.",
      "**Evaluating decentralization and assumptions**\n\nI see that the system isn't fully decentralized due to complex messaging. In the game theory model, they assume that boosting strategies can only focus on increasing bi and xi, while agents can't set si incorrectly.\n\nThey claim a pricing scheme is strategy-proof, yet they never prove it, raising questions about enforcing penalties for false reporting. In Algorithm 2, the chosen variable range creates potential uncertainties in outcomes. \n\nThey fail to address message delays and asynchrony, which could impact information propagation, and their definition of the winner determination rule needs clarity on ranking losers.",
      "I'm analyzing a scenario with three bidders who might conflict, looking at their valuations and bids. Initially, they update their responses based on others' bids, which could lead to a suboptimal convergence. A major concern is the assumption that agents can calculate critical values with complete knowledge of each other's bids, while some details about transmission delays remain unclear.\n\nI also see algorithmic complexity issues, particularly regarding communication overhead and the lack of a defined penalty for misreporting. This could break their payment scheme, as honest reporting is crucial. Plus, the convergence proof has flaws, as it neglects interdependencies between agents."
    ],
    "token_usage": {
      "input": 17766,
      "thinking": 3072,
      "output": 597
    }
  },
  {
    "entry_id": 184,
    "retraction_id": "1503.01380v2",
    "paper_id": "1503.01380v1",
    "retraction_comment": "incomplete and inaccurate, requesting withdrawal immediately. the ranking method is not correct",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Lack of out-of-sample validation",
        "Location": "Section 3 (Results)",
        "Explanation": "The paper reports R² and quartile-match percentages all on the same data used to fit the regression (no held-out test set or cross-validation). This in-sample evaluation likely overestimates predictive performance and does not demonstrate that the model will generalize to new journals or years."
      },
      {
        "Problem": "Inconsistent variable selection despite stated criteria",
        "Location": "Page 6, Summary Output table",
        "Explanation": "The algorithm (Step 7–9) mandates removing any predictor with P-value > 0.05, yet the final regression still includes ‘Total Docs. (2012)’ (P=0.2796) and ‘Total Refs.’ (P=0.2857). This violation of the model-selection procedure indicates a flawed or ad hoc approach to variable inclusion."
      },
      {
        "Problem": "Misuse of PCA for feature elimination",
        "Location": "Pages 5–6, PCA Model and Table: Principal Component Analysis",
        "Explanation": "The authors remove variables whose principal components explain little variance, but PCA is unsupervised and identifies directions of maximal total variance, not maximal predictive power. Discarding features solely on PCA variance risks dropping variables that are important predictors of the response."
      },
      {
        "Problem": "Inappropriate treatment of ‘Quarter’ as a continuous numeric predictor",
        "Location": "Page 2, Definition of Quarter variable",
        "Explanation": "‘Quarter’ is a nominal category with no true numerical distance between Q1–Q4. Encoding it as a single linear variable assumes equal spacing and a linear trend, which misrepresents its relationship to journal influence and invalidates the interpretation of its coefficient."
      },
      {
        "Problem": "Arbitrary unsupervised classification with K-means on 1D scores",
        "Location": "Pages 9–10, Classification Process (K-Means Clustering)",
        "Explanation": "The paper uses K-means on the one-dimensional JIS to separate ‘National’ vs. ‘International’ journals without any labeled training data or external validation. This arbitrary clustering lacks grounding in actual class labels, and no classification accuracy or robustness analysis is provided."
      }
    ],
    "think_summary": [
      "**Evaluating journal influence methodology**\n\nI’m examining a paper titled \"Journal rank in the Science and Technology domain: A lightweight quantitative approach for evaluation.\" They introduce a regression-based technique for calculating a Journal Influence Score (JIS), skipping citations from other journals. They apply multiple linear regression, PCA, and k-means for classification. However, I’m spotting potential critical issues, such as omitted variable bias since they rely solely on journal-specific features and overlook crucial citation network effects. This approach seems flawed as it disregards predictive elements.",
      "**Identifying model flaws**\n\nIt's tough to justify a model that lacks external citation data. They don't test out-of-sample performance, focusing only on in-sample correlation, which raises concerns about overfitting and optimism bias. Their accuracy comparison between models only looks at quartile match percentages, not actual rank correlation or error metrics. Additionally, the use of PCA after regression is confusing; they discard variables based on PCA rankings but then use original features. Finally, their inclusion of non-significant coefficients violates their own selection criteria, making the model selection seem flawed.",
      "**Highlighting critical model issues**\n\nI'm noticing some significant flaws in their methodology. They assert that first-quarter journals have more influence, but the coefficient for the quarter variable is negative, suggesting that JIS decreases as the quarter increases. This raises questions, especially as they treat quarter as a continuous variable when it's nominal. Their use of k-means for classification is also problematic; it oversimplifies the clusters and lacks accuracy evaluation. Additionally, they fail to validate their model properly, relying on in-sample data without sufficient statistical measures or diagnostics.",
      "**Identifying modeling inconsistencies**\n\nI'm pointing out several modeling inconsistencies. The algorithm specifies removing predictors with p-values over 0.05, yet the final regression includes Total Docs (p=0.28) and Total Refs (p=0.29), revealing flawed selection. Additionally, using PCA to drop features based on low explained variance risks losing predictive variables, which could bias the model. The quarter variable's treatment as a numeric predictor is inappropriate since it’s categorical and not inherently ordered. Lastly, using k-means clustering on one-dimensional JIS lacks supervision and fails to validate classification accuracy."
    ],
    "token_usage": {
      "input": 18405,
      "thinking": 1664,
      "output": 530
    }
  },
  {
    "entry_id": 185,
    "retraction_id": "1711.11197v4",
    "paper_id": "1711.11197v3",
    "retraction_comment": "Theorem 4.3 is false as states and it requires a completely different approach. Section 5 is completely correct but it will developed separately",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Unjustified integrability of the Lie algebroid bF",
        "Location": "Section 3 (Definition 3.1 and surrounding text)",
        "Explanation": "The paper defines bF as the subbundle of the b-tangent bundle whose sections are vector fields tangent to the leaves and to the boundary foliation, and then assumes without proof that it integrates to the Blup holonomy groupoid bH(M,F). However, not every Lie algebroid is integrable: obstructions (monodromy, non-vanishing of certain periods) may prevent existence of a smooth, Hausdorff integrating groupoid. The authors do not verify the Crainic–Fernandes integrability conditions or cite a result guaranteeing integrability in their setting, so the existence and smooth structure of bH(M,F) is not justified."
      },
      {
        "Problem": "Overstrong amenability assumption for Blup and holonomy groupoids",
        "Location": "Section 2.4 (paragraph beginning “Since in this paper all the groupoids…”) and throughout index constructions",
        "Explanation": "The authors assume that all groupoids they consider (in particular holonomy, Blup and deformation groupoids) are amenable, so that reduced and maximal C*-algebras coincide. However, holonomy groupoids of general foliations can be non-amenable, and amenability is not preserved under blowup constructions without additional hypotheses. Failing amenability invalidates the identification C* = C*_r and hence the K-theory computations of the analytic and topological indices."
      },
      {
        "Problem": "Lack of global properness for the R*-action in the Blup construction",
        "Location": "Section 2.5.2 (Definition 2.13 and the ‘functoriality’ claims)",
        "Explanation": "The Debord-Skandalis Blup is formed by quotienting D(M,X)\\(X×ℝ) by the ℝ*-action, and the paper asserts this action is free and proper. In fact, Debord and Skandalis only establish local properness conditions in general. Without verifying global properness (closedness of orbits, Hausdorff quotient), the smooth manifold and Lie groupoid structures on Blup(M,X) and on the associated Blup groupoids are not guaranteed."
      },
      {
        "Problem": "Overlooked distinction between Morita equivalence and strict C*-algebra isomorphism",
        "Location": "Section 4 (Lemma 4.1 and Definition 4.2)",
        "Explanation": "The authors use Morita equivalences of Lie groupoids to assert isomorphisms of C*-algebras (and hence of their K-theory) by identifying C*(G) with C*(H) when G∼_M H. But Morita equivalence of groupoids yields only stable equivalence (strong Morita) of their C*-algebras, not canonical ∗-isomorphisms. One must check that the bimodules implement K-theory isomorphisms correctly; glossing over this can invalidate the comparison of analytic and topological indices."
      },
      {
        "Problem": "Insufficient verification of properness for the semidirect groupoid in the APS–families setting",
        "Location": "Section 5.1 (Proposition 5.1)",
        "Explanation": "To construct the topological APS index for families, the paper considers the semidirect product Γ^b_φ(M_f) ⋉ₕ ℝ^N and asserts it is free and proper, citing injectivity of h and a reference. However, the subgroupoid Γ^b_φ(M) is not saturated in the full pair groupoid, so properness of the larger semidirect product does not directly imply properness of the restricted one. The authors do not supply a separate argument that the restricted action remains properly discontinuous, casting doubt on the smoothness and Hausdorffness of the resulting orbit space B^φ_APS."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 25182,
      "thinking": 1600,
      "output": 905
    }
  },
  {
    "entry_id": 186,
    "retraction_id": "2301.09693v2",
    "paper_id": "2301.09693v1",
    "retraction_comment": "An important technical mistake in the set-up of this variant of generalization to Seiberg-Witten equations was pointed out to the author. In particular, the equations are not elliptic as claimed. As a result, any statement about (or uses) regularity and transversality of the moduli space has to be disregarded. However, the moduli space is still compact",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Unjustified existence of the scalar function λ solving the Kazdan–Warner–type equation in Lemma 5.7",
        "Location": "Lemma 5.7 (page 27)",
        "Explanation": "The author asserts that one can always solve\n    Δλ + (¼n|α|² – |φ|²) e^λ + C(x) = 0\nfor a smooth λ by choosing C(x) so that ∫_X C ω∧ω = 0.  But existence of solutions to such equations requires nontrivial solvability conditions (e.g. sign‐and‐mean constraints on the coefficient f(x)=¼n|α|²–|φ|²) which are neither checked nor guaranteed here.  Without verifying these integral/sign conditions, the asserted unique gauge fixing and thus the uniqueness of the solution to (5.6) fails."
      },
      {
        "Problem": "Insufficient perturbation directions in the linearization—surjectivity in Lemma 3.4 is flawed",
        "Location": "Lemma 3.4 (page 15)",
        "Explanation": "To achieve transversality the author perturbs the Dirac equation by τ∈Ω¹⊗su(E), claiming fiberwise surjectivity of the map τ↦τ·ψ.  However, for a fixed nonzero spinor ψ(x), the map su(n)→E_x given by a↦a·ψ(x) spans only the ψ(x)⊥ directions, not the full C^n fiber.  Thus the combined linearized operator fails to be onto, and 0 need not be a regular value of the perturbed section."
      },
      {
        "Problem": "No proof of invariance under deformations of the SU(n)–connection B",
        "Location": "Theorem 4.5 and its proof (pages 21–23)",
        "Explanation": "Although the invariant SWE(s) is claimed to be independent of the background SU(n)–connection B on E, the paper only varies (g, τ, ω) in a cobordism argument.  There is no homotopy or cobordism shown that relates the moduli spaces for different choices of B, so independence of B is unsubstantiated."
      },
      {
        "Problem": "Misuse of a nonconstant function C(x) in the elliptic equation (5.8)–(5.9)",
        "Location": "Derivation of (5.9) in Lemma 5.7 (page 27)",
        "Explanation": "The author introduces C(x) satisfying ∫_X C ω∧ω = 0 into the equation\n    Δλ + F(x)e^λ + C(x) = 0,\nwithout showing C is constant or verifying the usual obstructions to solvability (e.g. Fredholm alternative for the Laplacian).  In general a nonconstant C(x) destroys the global solvability condition, so the claimed existence of λ is flawed."
      },
      {
        "Problem": "Key L¹→C⁰ estimate relies on Proposition 6.15 without adequate justification",
        "Location": "Section 6.2, Proposition 6.14 (pages 32–34)",
        "Explanation": "The crucial bound\n    sup |s| ≤ C M_φ(K,e^s) + D\nfollows by contradiction from a compactness lemma (Proposition 6.15) whose proof is only sketched.  In particular, the measurability and a.e. constancy of the weak limit u_∞’s eigenvalues, and the construction of holomorphic subsheaves from its spectral projections, are merely cited and not established here.  Without a complete argument, the lower‐bound/convexity machinery for the Donaldson–type functional collapses."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 48527,
      "thinking": 4352,
      "output": 858
    }
  },
  {
    "entry_id": 187,
    "retraction_id": "1609.01275v2",
    "paper_id": "1609.01275v1",
    "retraction_comment": "There is a serious error 3 lines below \"Step (II)\". It is not true that \"It follows that the set C' of all oriented colourings for ... is equal to the set of extension colourings.. .\" (In fact they only account for half of the extension colourings.)",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Existence of a generic periodic origami realization for every 3-connected triangulated torus graph is assumed but not proved",
        "Location": "Section 3, Lemma 3.1 (pp. 6–8)",
        "Explanation": "The proof of Lemma 3.1 reduces an arbitrary generic origami’s quotient graph by contracting edges to a 3-connected triangulated torus graph and then assumes there is a corresponding periodic origami realization. No construction or existence argument is given to embed that contracted graph in R³ with the same periods and algebraically independent coordinates. Without showing that every such graph admits a geometric (and generic) origami realisation, the inductive reduction fails."
      },
      {
        "Problem": "Preservation of geometric genericity under joint-splitting moves is not established",
        "Location": "Section 3, Lemma 3.2 (p. 8) and the construction in Section 2",
        "Explanation": "The inductive proof uses joint-splitting moves to build up from smaller motifs—but nowhere is it shown that performing a splitting move on a generic motif yields another motif whose joint coordinates remain algebraically independent. If splitting introduces algebraic relations (e.g. by forcing new vertex positions to satisfy polynomial constraints), the ‘generic’ hypothesis fails at later stages, invalidating the combinatorial palindromy argument which relies on genericity."
      },
      {
        "Problem": "Base case (the single-joint motif) is not shown to satisfy the strong palindromic property locally",
        "Location": "Section 4, Example 4.2 (p. 11) and Definition 5.2",
        "Explanation": "The inductive step in Lemma 5.3 hinges on a strong palindromic property holding at every joint in the base origami. While the global vanishing of the base polynomial is observed, no check is given that for each local oriented colouring at the sole joint the required palindromic symmetry of the partial sums X_λ holds. Without this, the induction has no valid starting point."
      },
      {
        "Problem": "Ambiguous/mis-stated definition of palindromicity",
        "Location": "Definition 5.1 (p. 13)",
        "Explanation": "The paper defines a Laurent polynomial p(z₁,z₂) to be palindromic if “p(z)=z^α p(z)”, but never clarifies that one side should be evaluated at (z₁^(-1),z₂^(-1)). As written, p(z)=z^α p(z) is nonsense except in trivial cases. This misuse obscures the symmetry argument and casts doubt on all subsequent deductions invoking palindromy."
      },
      {
        "Problem": "Failure to rigorously deduce 1-dimensional RUM spectrum from palindromicity",
        "Location": "Introduction and end of Section 1 (pp. 1–2)",
        "Explanation": "The core application is that palindromicity forces the zero set on T² to be a 1-dimensional curve. The paper merely asserts this is 'elementary'—but does not rule out degenerate possibilities (e.g. p≡0 or p having only isolated zeros). No argument is given that the palindromic polynomial is nonzero and has no isolated roots, so the claimed conclusion of a generically 1-dimensional RUM spectrum remains unsubstantiated."
      }
    ],
    "think_summary": [
      "**Evaluating quotient graphs and origami**\n\nI'm thinking about how they reduce quotient graphs through edge contractions and whether the resulting graph truly represents a periodic origami. They've shown it’s possible to create a 3-connected triangulated torus graph through edge contraction, but ensuring it corresponds to a periodic origami really hinges on maintaining geometry. They suggest a categorical correspondence, but I wonder if it’s clear that every 3-connected triangulated torus graph maps to a generic periodic origami with algebraically independent coordinates while respecting periodicity.",
      "**Exploring graph embeddings and properties**\n\nThis seems nontrivial, but I think it’s known that any 3-connected graph on a torus can be represented in R^3 as a periodic triangulated origami. However, they haven't proven that such a framework exists generically. There's a gap where they need to show that each contracted graph has a geometric realization as a generic periodic origami.\n\nThen there's the combinatorial coloring argument. They define oriented colorings linked to determinant entries, claiming that expansion creates a correspondence to oriented coloring, which might indeed apply to matrix-tree expansions. They suggest that the palindromic property is preserved under splitting, but I wonder if they've adequately established the base origami's strong palindromic property.",
      "**Analyzing matrix properties and origami**\n\nI see that the matrix they're working with is a 3x3, which ends up having a determinant of zero since two rows share columns. This leads to a trivial base origami with pO = 0, but I wonder if the palindromic property applies in this case.\n\nEven though zero is trivially palindromic, they need a non-zero pO to derive the one-dimensional spectrum. It’s interesting that they're claiming any generic origami has a palindromic polynomial; still, does that hold for the base origami, considering its properties? It looks like they need to clarify how the dimension theorem applies, especially since the base origami isn't truly generic."
    ],
    "token_usage": {
      "input": 17887,
      "thinking": 5696,
      "output": 767
    }
  },
  {
    "entry_id": 188,
    "retraction_id": "2106.14795v2",
    "paper_id": "2106.14795v1",
    "retraction_comment": "Proof of Threorem 17, part 2 not correct as displayed",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Lack of verification of the discrete inf–sup (Brezz–Fortin) stability condition for the mixed finite element pair (P₀,P₁)",
        "Location": "Section 3, equations (23a)–(23b)",
        "Explanation": "The authors assert that in 1D the lowest–order Raviart–Thomas element coincides with the pair P₀×P₁ (piecewise constants × continuous piecewise linears) without demonstrating that this choice satisfies the necessary inf–sup (LBB) condition. Without such a stability proof, well‐posedness of the discrete mixed formulation and all subsequent error estimates are not justified."
      },
      {
        "Problem": "Unjustified stability/error bound for the discrete flux in H¹ used in the state error analysis",
        "Location": "Proof of Theorem 12 (page 15)",
        "Explanation": "In bounding (ψ−Πhψ, z−zh), the authors invoke ||z−zh||_{H¹}≤C||u||_{L²} but never establish stability or an a priori bound for the discrete flux error in H¹. This step is critical to conclude the L² error estimate for the state, yet no proof or reference is provided for this bound."
      },
      {
        "Problem": "Missing justification for the L∞‐error estimate of the adjoint state and multiplier",
        "Location": "Theorem 13 and Lemma 14 (pages 17–18)",
        "Explanation": "The paper cites Brezzi–Fortin for the estimate ||p−ph||_{L∞}+||q−qh||_{L∞}≤Ch, but no discussion is given as to the regularity requirements (e.g. mesh quasi‐uniformity, higher regularity of p) under which an L∞–error bound holds. Standard mixed‐FE theory typically yields L²–errors of order h and requires additional arguments for L∞–bounds."
      },
      {
        "Problem": "Application of subdifferential chain and sum rules in the BV setting without qualification",
        "Location": "Proof of Theorem 4 (pages 5–7)",
        "Explanation": "The derivation of the optimality conditions invokes convex‐analysis chain and sum rules on the functional J(u)=½||Su−yd||²+α‖u′‖_{M} in the nonreflexive Banach space BV(Ω), but no constraint qualification (e.g. continuity of one term at the point) is verified. In particular, the subdifferential ∂(‖u′‖_{M}) is set‐valued and delicate in BV, and the sum rule requires conditions that are not checked."
      },
      {
        "Problem": "Overly restrictive and unverified structural assumption on the adjoint zeros",
        "Location": "Assumption 15 (page 18)",
        "Explanation": "Error estimates for the control rely on the assumption that the set {x:p(x)=0} is finite and all roots are simple. However, no justification is given that, for general desired states yd, the adjoint p will satisfy this property or a uniform lower bound on root separation. Without this, the convergence rate for the control may fail."
      }
    ],
    "think_summary": [
      "**Analyzing paper for issues**\n\nThe user is asking me to check the attached paper for critical errors that could invalidate its conclusions. They specifically want to ignore minor problems and focus on significant mathematical or logical mistakes, like issues with existence, uniqueness, or error estimates. I'll need to use a specific format for my findings, outlining up to five critical problems in a structured JSON object. So, I’m gearing up to carefully analyze the provided content from the paper.",
      "**Reviewing equation roles**\n\nThe discrete state equation has y and z assigned incorrectly; they should be reversed. In RT0, z should be in P0 and y should be scalar in P1. They define z as equal to the derivative of y, and z plays the role of flux or gradient here. The usual representation flips; z belongs in H1, approximated by P1 continuous, while y is in L2 approximated by P0. It seems they've chosen the roles incorrectly. I'm exploring these definitions further to clarify their mathematical structure.",
      "**Evaluating error estimates**\n\nI'm examining the a priori error estimates for the state, looking for issues with flux assumptions that might arise from the analysis. They derived bounds for certain terms but appear to struggle with establishing proper error orders, particularly concerning the estimates for ||z - z_h||. Their use of subdifferential in measure spaces looks standard, but I'm finding they need clearer definitions for continuity involving S. \n\nAlso, they haven't explicitly provided error estimates for y in L2, which raises potential inconsistencies. Overall, I’m identifying areas where the argument feels incomplete and needs clarification."
    ],
    "token_usage": {
      "input": 29429,
      "thinking": 6656,
      "output": 731
    }
  },
  {
    "entry_id": 189,
    "retraction_id": "2108.05981v2",
    "paper_id": "2108.05981v1",
    "retraction_comment": "The article has been withdrawn due to incorrect model formulation. Particularly, introducing the so-called \"field with two elements\" was a mistake that made the main equation (observation) of the paper incorrect",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Undefined discrete Laplacian and unjustified dispersion relation",
        "Location": "Page 3, Section 2.A, Equations (1)–(2)",
        "Explanation": "The paper replaces the continuous Laplacian with a discrete operator Δ but never defines its action or the underlying lattice topology, nor justifies why the Ansatz F(n,t)=F₀ exp(2πi n/q−ωt) yields ω²−(2πi/q)²=m_q². Without a precise definition of Δ (boundary conditions, dimension, normalization), the derived mass relation is unfounded."
      },
      {
        "Problem": "Arbitrary static limit to define critical mass",
        "Location": "Page 4, Section 2.A, Equations (3)–(4)",
        "Explanation": "The authors set ω→0 in the dispersion relation to obtain −(2πi/q_c)²=m_{q_c}² and thus m_{q_c}=2π/q_c. This step treats the discrete momentum mode as a physical particle mass at the 'critical point' without dynamical justification, conflating lattice finite-size effects with genuine field-theoretic masses."
      },
      {
        "Problem": "Circular identification of q_c via Higgs mass",
        "Location": "Page 4–5, Section 2.B, Equations (5)–(6)",
        "Explanation": "The model assumes m_{q_c}=m_Higgs in reduced Planck units to solve q_c=2π/m_H, effectively fitting the cutoff q_c to the known Higgs mass rather than predicting it. This reverses cause and effect and undermines any predictive power."
      },
      {
        "Problem": "Erroneous numerical determination of q* for Monster group matching",
        "Location": "Page 5, Section 3, around Equation (9)",
        "Explanation": "Solving q*³−q*≈8×10⁵³ gives q*≈9.314×10¹⁷, but the paper then selects q*=9650949592≈9.314×10¹⁷, a ten-order-of-magnitude mismatch. This blatant numerical error invalidates the subsequent mass estimate m_{q*}."
      },
      {
        "Problem": "Unjustified attribution of factor 24 to spatial dimensions",
        "Location": "Page 6, Section 3, Equations (11)–(13)",
        "Explanation": "The authors import the central charge 24 from the Monster CFT and reinterpret it as the number of spatial dimensions D in the discrete Laplacian, implying a 24-dimensional space. There is no basis for a physical Higgs field living in 24 spatial dimensions, making the final mass relation purely numerological."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 10443,
      "thinking": 1536,
      "output": 636
    }
  },
  {
    "entry_id": 190,
    "retraction_id": "2209.07447v2",
    "paper_id": "2209.07447v1",
    "retraction_comment": "Theorem 11.1 is false : the kernel is not trivial as stated",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Incorrect injectivity claim for the connecting map ψ₀ in the Galois cohomology sequence",
        "Location": "Section 12, Lemma 12.1 (page 31)",
        "Explanation": "From the exact sequence 1→μ₃→S×SL₃→M₀→1 one gets a long exact cohomology sequence\n H¹(K,S×SL₃)→H¹(K,M₀)→H²(K,μ₃).  Since H¹(K,SL₃)=1 but H¹(K,S)=Kˣ/N_{E/K}(Eˣ) is generally nontrivial, the kernel of ψ₀:H¹(K,M₀)→H²(K,μ₃) is exactly Im(H¹(K,S)), which need not vanish.  The paper’s claim that ψ₀ has trivial kernel is therefore false, undermining the final descent argument."
      },
      {
        "Problem": "Unsound argument for anisotropy of Str(A)′ in the division‐algebra case",
        "Location": "Section 5.1, Proposition 5.1 (page 8)",
        "Explanation": "The proof argues that any Gₘ subgroup of Str(A)' must act by scalars on A and hence coincide with the central Gₘ, giving a contradiction.  However, this relies on a dubious density argument over K and ignores the possibility of non‐split (anisotropic) tori.  No rigorous justification is given for why an embedding Gₘ→Str(A)' must act via scalar multiplication, so the necessity direction is unsound."
      },
      {
        "Problem": "Overlooked hypotheses in the use of the norm principle",
        "Location": "Section 9, proof of Theorem 9.1 (page 21–22)",
        "Explanation": "The proof applies the norm principle to conclude that NL/K(Nrd(D⊗K L))⊆Nrd(D), but the classical norm principle requires the algebra D to satisfy additional conditions (e.g. central simple over K) and a proper analysis of how reduced norms behave under finite extensions of degree prime to the degree of D.  These technical prerequisites are neither stated nor verified."
      },
      {
        "Problem": "Unjustified construction of the twisted subgroup F inside Str(A)",
        "Location": "Section 10, Proposition 10.6 (page 24)",
        "Explanation": "The proposition asserts that the K-subgroup F≅int(g)(R·H) exists as a subgroup of Str(A) by checking Galois stability of g(R·H)g⁻¹.  But invoking the torsion bijection and twisted conjugation does not guarantee that a priori abstractly isomorphic K-form actually embeds as a subgroup of Str(A).  The step from Galois-stable points to a closed subgroup scheme is not justified."
      },
      {
        "Problem": "Missing justification for the uniqueness of the stable A₂-subsystem in D₄",
        "Location": "Section 11, proof of Theorem 11.1 (page 27–29)",
        "Explanation": "The argument relies on the claim that any element of order 3 in Aut(Σ(D₄)) fixes a unique subroot system of type A₂, and hence S·SL₁(D) cannot embed in H₀.  No reference or proof is given for this uniqueness (up to conjugacy) of the Gal(E/K)-stable A₂-subsystem, yet it is crucial to ruling out the second case.  Without it, the classification step fails."
      }
    ],
    "think_summary": [
      "**Evaluating critical errors**\n\nThe user wants me to check the attached paper for major errors that could undermine its conclusions. I need to analyze pages 1-34 and pinpoint up to five significant issues, including their location and explanations. The focus is on a paper discussing F4 groups and cohomological arguments. \n\nI suspect there might be a misunderstanding in Proposition 5.1 regarding division algebras and their properties. I'll keep searching for logical gaps or coherence issues."
    ],
    "token_usage": {
      "input": 27699,
      "thinking": 2624,
      "output": 787
    }
  },
  {
    "entry_id": 191,
    "retraction_id": "1305.2623v2",
    "paper_id": "1305.2623v1",
    "retraction_comment": "This paper has been withdrawn by the author due to a crucial error in the calculation of Equation (28)",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Contradictory \"strong parameters\" condition for random‐walk mobility",
        "Location": "Section 2.2.1 (definition of strong parameters condition)",
        "Explanation": "The paper requires all member velocities to satisfy both v(n)=ω(√(log n/(nᵈ))) and v(n)=o(n⁻¹). But for any constant d>0, √(log n/nᵈ)≫n⁻¹ as n→∞, so no sequence v(n) can satisfy v(n)≫√(log n/(nᵈ)) and v(n)≪n⁻¹ simultaneously. This makes all results claimed under the \"strong\" condition for random‐walk mobility vacuous."
      },
      {
        "Problem": "Unjustified asymptotic independence in Lemma 2 under random‐walk mobility",
        "Location": "Lemma 2 (properties (1-a) and (1-b)), Section 4",
        "Explanation": "The derivation of P(1ᵢⱼ₁=β, …,1ᵢⱼₕ=β)≈∏P(1ᵢⱼₖ=β) relies on asserting that the union of the covered areas over h slots has negligible overlap, citing [15] without proof. For random‐walk trajectories, the unions Siⱼ can have significant self‐overlap (especially if vT is O(r)), and the paper omits any rigorous bounds on the overlap areas. Without these, the key step reducing correlated events to products is unsupported, undermining the probability estimates."
      },
      {
        "Problem": "Unrestricted cluster‐head exponent d leads to unrealistic and degenerate regimes",
        "Location": "Section 2.1 (network topology), throughout critical-range results",
        "Explanation": "The model lets the number of static cluster heads be nᵈ for any constant d>0, including d>1, which implies nᵈ≫n cluster heads—far more heads than members. In such regimes connectivity trivially occurs at vanishing radii, yet the paper’s scaling laws r∼Θ((log n)/(nᵈ)) or r∼Θ(√(log n/(nᵈ))) are quoted without distinguishing cases d<1 versus d>1. The applicability and interpretation of the results break down when d>1, but the paper never addresses or excludes this regime."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 20738,
      "thinking": 3328,
      "output": 557
    }
  },
  {
    "entry_id": 192,
    "retraction_id": "0909.5521v3",
    "paper_id": "0909.5521v2",
    "retraction_comment": "Manuscript withdrawn, because results are incorrect. If phi = phi_1 AND phi_2, and phi is a Horn formula, it does NOT mean that both phi_1 and phi_2 are Horn formulae. Furthermore, the cardinality constraint CANNOT be expressed as a universal Horn sentence in ESO (NOT even when the structure is ordered)",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Unsupported expressibility of cardinality constraints in ESO universal Horn",
        "Location": "Section 2.1 (page 3), Remarks 5–11 and Corollary 7",
        "Explanation": "The paper assumes that the objective‐function constraint (OFC) ‘|S| ≥ K’ can be encoded as an ESO‐ord‐Π1‐Horn formula independent of the BFC. Grädel’s theorem (Theorem 6) guarantees that entire decision problems in P have ESO universal‐Horn definitions, but does not furnish a standalone Horn encoding of arbitrary cardinality constraints. First‐order Horn logic over ordered structures with successor lacks the ability to compare the size of an SO predicate to a variable K in the input, so the paper’s atomic treatment of ‘|S| ≥ K’ as a Horn formula is unfounded."
      },
      {
        "Problem": "Unjustified separability and reuse of the OFC across problems",
        "Location": "Section 2.1.1 (page 4)",
        "Explanation": "The argument that the OFC part φ2 of a P‐problem’s Horn formula can be cleanly separated from its BFC φ1 and then reused verbatim for NP‐complete problems relies on an unproven ‘separability’ claim. Grädel’s universal‐Horn encoding may intertwine feasibility and objective constraints in a way that resists such factorization, and no formal mechanism is provided to extract a problem‐independent φ2."
      },
      {
        "Problem": "Faulty Assumption 4 on Horn‐clause decomposition",
        "Location": "Assumption 4 (page 2)",
        "Explanation": "The author assumes that if a conjunction of clauses is semantically equivalent to a Horn formula, then each clause must itself be Horn. This is false: non‐Horn clauses can combine to an equivalent Horn formula, and there is no general principle that enforces clause‐wise Hornness from semantic equivalence alone. This invalidates the key step that the reused OFC is guaranteed to be Horn."
      },
      {
        "Problem": "Incomplete Horn encodings for Vertex Cover and Clique cardinality constraints",
        "Location": "Sections 2.2 (page 5) and 2.3 (page 6)",
        "Explanation": "In both Vertex Cover and Clique, the paper rewrites |S|≤K or |S|≥K into an equivalent bound on the complement set T, but never shows how the resulting cardinality constraint ‘|T|≥K1’ can be expressed by a quantifier‐free universal‐Horn first‐order formula. Without a concrete Horn encoding using only successor and order, the claim that these NP‐complete problems admit ESO‐ord‐Π1‐Horn definitions is unsubstantiated."
      },
      {
        "Problem": "Misapplication of Grädel’s theorem to parameterized constraints",
        "Location": "Theorem 6 and its use throughout Section 2",
        "Explanation": "Grädel’s result characterizes polynomial‐time decision languages (over fixed vocabularies) as those definable in ESO universal‐Horn with order and successor. The paper treats K as part of the input vocabulary and attempts to encode the inequality |S|≥K in that logic. However, ESO universal‐Horn captures uniform language families in P, not parameterized cardinality comparisons involving an unbounded numeric input. This misinterpretation undermines the entire reduction from NP‐complete problems to P."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 5556,
      "thinking": 1216,
      "output": 776
    }
  },
  {
    "entry_id": 193,
    "retraction_id": "1810.09527v2",
    "paper_id": "1810.09527v1",
    "retraction_comment": "Appears that the three transitions interpreted as blueshifted molecular emission from methanol and methanimine are recombination line transitions with similar rest frequencies Paper being withdrawn while this is confirmed",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Insufficient evidence for non-thermal maser emission",
        "Location": "Section 4.1, rotation diagram (Figure 3)",
        "Explanation": "The authors infer maser action primarily from deviations from a single-temperature LTE rotation diagram, but provide no direct constraints on brightness temperature or optical depth (τ > –0.004 is only a lower limit assuming unity filling factor). Without interferometric size measurements or variability studies establishing brightness temperatures well above thermal levels, the claim that the 6.7 GHz emission is maser rather than quasi-thermal remains unsubstantiated."
      },
      {
        "Problem": "Angular resolution too coarse to rule out compact star-formation maser clusters",
        "Location": "End of Section 4.1 (discussion of antenna 6 baselines and Figure 2)",
        "Explanation": "With a synthesised beam of ~25″ (~450 pc) and sensitivity lost on baselines >2″ (~40 pc), the data cannot resolve structures below tens of parsecs. A dense cluster of conventional Galactic-type 6.7 GHz masers within a ≲40 pc region could produce the observed integrated luminosity. The argument against an ensemble of star-formation masers is therefore premature without higher-resolution imaging."
      },
      {
        "Problem": "Overinterpretation of line profile as spherical outflow",
        "Location": "Section 4.2, Equation (1) and corresponding fits in Figure 1",
        "Explanation": "The blueshifted, asymmetric line profile is fitted with a symmetric constant-velocity outflow model, invoking obscuration of the redshifted component by central material. However, without spatially resolved velocity mapping or detection of the red wing, the spherical outflow interpretation is speculative and alternative kinematic origins (e.g. rotating disk, inflow, or beam-smeared turbulent gas) are not excluded."
      },
      {
        "Problem": "Invalid isotropic luminosity comparison for beamed maser emission",
        "Location": "Section 4.1, comparison to G 9.62+0.20 luminosity",
        "Explanation": "The authors scale the observed flux in NGC 4945 to an equivalent Galactic distance assuming isotropic emission to compare with a known Galactic maser. Masers are intrinsically beamed, so isotropic luminosity estimates can misrepresent true power and beam geometry. This undermines quantitative comparisons of 'factors of 6000–10000' and the conclusion that the emission is orders of magnitude more luminous than Galactic masers."
      },
      {
        "Problem": "Potential baseline ripples or RFI misidentified as broad spectral features",
        "Location": "Section 2 (Observations) and the broad ~350 km s⁻¹ line profiles in Figure 1",
        "Explanation": "The detected methanol and methanimine lines span several hundred km s⁻¹, but the paper does not discuss checks against baseline ripple, spectral standing waves, or radio-frequency interference across the wide 96–128 MHz zoom bands. Without independent confirmation (e.g. multiple epochs, off-source checks), the broad features could be instrumental artifacts rather than genuine astrophysical lines."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 16153,
      "thinking": 960,
      "output": 711
    }
  },
  {
    "entry_id": 194,
    "retraction_id": "1603.01872v4",
    "paper_id": "1603.01872v3",
    "retraction_comment": "This paper contains an error: the diagram (13) in the proof of Proposition 6.5 does not strictly commute as claimed. See this https URL for more details. The authors would like to thank [REDACTED-NAME] and [REDACTED-NAME] for finding this error and bringing it to their attention",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Ad hoc, potentially inconsistent definition of the map “/H_f” in the proof of Proposition 6.5",
        "Location": "§6, proof of Proposition 6.5 (diagrams (10) and (12))",
        "Explanation": "In order to force commutativity of the little triangle (10) and the trapezoid (12) in the big diagram (9), the authors define a map “/H_f” on (∇F)^hH → (F^+)^hH twice, in two different ways, without checking that these two ad hoc definitions agree.  Since there is only one arrow labelled “/H_f” in (9), it must be a single well-defined P-equivariant map; but the piecewise definitions may conflict, so the claimed global commutativity of (9) is unsupported."
      },
      {
        "Problem": "Omission of necessary model-categorical fibrancy/cofibrancy hypotheses in the construction of fixed-point and mapping spectra",
        "Location": "§4–§6 (definitions of D⁺X, (–)^hH, F_H(EG₊,–))",
        "Explanation": "The authors insist they need not fibrantly replace targets in mapping spectra because their domains are suspension spectra of finitely dominated spaces.  However, without verifying that their chosen model of orthogonal G-spectra makes FG(P₊, X) and the homotopy fixed-point functor (–)^hH homotopically well-behaved on the point-set level, the strict commutativity of the many point-set diagrams (e.g. (8), (11), (13)) is suspect.  Missing fibrancy/cofibrancy can invalidate the claimed strict diagram chases."
      },
      {
        "Problem": "Insufficient justification for reduction to only the 1-connected and finite-cover cases",
        "Location": "§5 (reduction to special cases)",
        "Explanation": "The reduction argument factors an arbitrary fibration Y → Z into a finite covering followed by a 1-connected fibration by choosing models BG → BP of loop groups G → P.  But it tacitly assumes the corresponding subgroups of π₁Z are always of finite index and normal, and that BH remains finitely dominated.  In general fibrations with disconnected fibers or non-normal subgroups may not fit this pattern, so the reduction may fail to handle all cases."
      },
      {
        "Problem": "Use of the Wirthmüller isomorphism in §7 without checking point-set level hypotheses",
        "Location": "§7, Proposition 7.1 (diagram (14))",
        "Explanation": "In the finite-cover case the authors invoke P₊ ∧_G X ≃ F_G(P₊, X) and its refinements as strict equivalences of naive G-spectra.  Yet they do not verify that X (or F_G(P₊, X)) is cofibrant or fibrant so that these assembly/Wirthmüller maps are well-defined weak equivalences on the point-set level.  This gap jeopardizes the asserted commutativity of the expanded diagram (16)–(17)."
      },
      {
        "Problem": "Lack of rigorous control over higher homotopies in extremely large point-set diagrams",
        "Location": "§6–§7 (expansions of (9), (13), (16), (17))",
        "Explanation": "The proof rests on the homotopy commutativity of very elaborate point-set level diagrams in orthogonal spectra, many of whose regions the authors dismiss as “straightforward” or “by diagram chase.”  Without a systematic bookkeeping of higher homotopies (e.g. via an A_∞ or bicategorical framework), there is no guarantee that the various homotopies assemble coherently to give a single homotopy showing functoriality in the stable category."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 15478,
      "thinking": 3968,
      "output": 873
    }
  },
  {
    "entry_id": 195,
    "retraction_id": "2404.01980v6",
    "paper_id": "2404.01980v5",
    "retraction_comment": "There is a flaw in the proof of Theorem 3.3 that the A at t=0 and A at t=T cannot be guaranteed to be identical, thus the key inequality fails",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Unjustified extension of Hamilton’s evolution equation to the normalized flow and to IA as an infimum over curves",
        "Location": "Theorem 2.3 (page 2)",
        "Explanation": "The paper invokes Hamilton’s formula for the evolution of the isoperimetric ratio under the unnormalized Ricci flow and asserts without proof that it remains valid for the normalized flow and for IA defined as an infimum over all area‐A curves. There is no justification that for each A the infimum is attained by a smooth curve, nor that IA is differentiable in the normal (r) direction so that (2.5) holds classically."
      },
      {
        "Problem": "Illegitimate use of the maximum principle on the PDE for IA",
        "Location": "Proposition 3.1 (page 3)",
        "Explanation": "The argument applies the parabolic maximum principle to (2.5) without specifying the spatial domain in r, boundary or initial conditions in r, or the regularity of IA needed. As stated, IA is only known as an infimum and may fail to be a smooth solution on which the maximum principle can be applied."
      },
      {
        "Problem": "Omitted justification for dropping the second‐derivative term",
        "Location": "Proof of Proposition 3.1 (page 3)",
        "Explanation": "To derive the ODE ∂t ln IA² ≥ B(1−IA²), the paper silently assumes that ∂²r ln IA² ≥ 0 at the spatial minimum along each minimizing curve. No argument is given that the minimizing curve is a stationary point in r so that its second normal derivative has the required sign."
      },
      {
        "Problem": "Asymptotic argument in Theorem 3.2 lacks error control",
        "Location": "Proof of Theorem 3.2 (page 3)",
        "Explanation": "The derivation of (3.3) and the ensuing limit as A→0 rely on the claim that (2.4) is 'tight' up to o(A²). However, the paper does not quantify the o(A²) terms or verify that they vanish fast enough to justify the inequality chain and the limiting contradiction."
      }
    ],
    "think_summary": [
      "**Clarifying definitions in mathematics**\n\nI'm pondering whether the defined I_A as the quotient of infimums is actually the same as the infimum of quotients. They gave I_A = inf_Λ L / inf_{Λ̄} L̄, but maybe the ratio of infimums isn’t equivalent to the infimum of the ratios. I see that for the minimal length curve in a general metric versus on a round sphere, the calculations can differ. So, it looks like I_A is a quotient of two infima rather than an infimum over pairs.",
      "**Examining inequalities and assumptions**\n\nI'm considering how the inequality shows that I_A is at least L/L̄ for any curve Λ, but that's not completely accurate. For any curve, L/L̄ is greater than or equal to a certain fraction. They defined I_A differently as an infimum ratio, which gets me thinking about possible issues in their PDE form and the mysterious second derivative term. It seems they assume the curve is stationary, leading to an overlooked justification that's really important for understanding their differential inequality. They miss a critical detail in showing that the second normal derivative must be non-negative."
    ],
    "token_usage": {
      "input": 3058,
      "thinking": 5440,
      "output": 511
    }
  },
  {
    "entry_id": 196,
    "retraction_id": "1105.1572v2",
    "paper_id": "1105.1572v1",
    "retraction_comment": "This was withdrawn because the key distribution figures Figure 1 and 3 in the paper are technically incorrect",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Mischaracterization of eavesdropping detection via loss",
        "Location": "Section IV.D, “Tentative Attack”",
        "Explanation": "The paper asserts that any interception that blocks qubits will be detected simply because Bob will not receive the expected qubits. In reality, QKD channels are lossy and QKD security proofs tolerate and monitor losses; an eavesdropper can exploit channel losses to hide interception. Treating missing qubits as definitive evidence of eavesdropping misunderstands the security model and undermines any claims about detecting denial-of-service or interception attacks."
      },
      {
        "Problem": "Incorrect complexity claim for quantum fingerprinting",
        "Location": "Section VI.B, Fingerprinting",
        "Explanation": "The paper states that quantum fingerprinting uses O(log² n) qubits and is nearly optimal, whereas the celebrated Buhrman–Cleve–Watrous–Wolf protocol achieves O(log n) qubits and has matching Ω(log n) lower bounds. The authors’ assertion and argument conflating state-description length with communication complexity are incorrect and would mislead readers about the true exponential advantage of quantum fingerprinting."
      },
      {
        "Problem": "Overstatement of ‘absolute security’ without authentication or side-channel considerations",
        "Location": "Introduction and Section III.A",
        "Explanation": "The paper repeatedly claims that quantum cryptography provides complete or absolute security guaranteed by physical laws, without acknowledging the necessity of an authenticated classical channel to prevent man-in-the-middle attacks or the vulnerabilities introduced by side-channels and device imperfections. Omitting these essential requirements renders the security claims unsound."
      },
      {
        "Problem": "Misapplication of the uncertainty principle to success probability in BB84",
        "Location": "Section IV.B, Quantum Transmission",
        "Explanation": "The authors attribute the 75% average success rate for Bob (and bound Oscar) to the Heisenberg uncertainty principle, whereas this figure simply arises from probabilistic basis matching in BB84. The uncertainty principle does not impose the 75% limit described. This conflation of classical sampling statistics with quantum uncertainty reflects a misunderstanding of the protocol’s security basis."
      },
      {
        "Problem": "Undefined parameters and sketchy description in privacy amplification",
        "Location": "Section IV.D, Privacy Amplification",
        "Explanation": "The paper refers to privacy amplification mapping an N–K bit reconciled key down to N–K–L–R–S bits via a hash function, yet none of N, K, L, R, S are defined nor is the security bound derived. Without concrete definitions and proof of entropy reduction against Eve’s information, the reader cannot assess whether the final key is secure, calling into question the protocol’s claimed security guarantees."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 16151,
      "thinking": 1664,
      "output": 618
    }
  },
  {
    "entry_id": 197,
    "retraction_id": "2003.05595v3",
    "paper_id": "2003.05595v2",
    "retraction_comment": "Equation (24) was wrong: algebraic cancellations of this type are invalid in general",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Application of bounded-domain gauge lemma to general simply-connected domains",
        "Location": "Section 3 (Lemma 3.1) and Section 4 (start of proof of Theorem 1.1)",
        "Explanation": "Lemma 3.1—which guarantees the existence of a Coulomb gauge P in W¹,²—requires U to be a smooth bounded domain with prescribed boundary conditions. Theorem 1.1, however, is stated for arbitrary simply-connected domains (possibly unbounded or with non-smooth boundary), and no justification or approximation argument is provided to extend the gauge construction to that setting. Without establishing the gauge transform on general U, the existence of P solving the Pfaff system globally is unjustified."
      },
      {
        "Problem": "Unjustified Hodge decomposition for divergence-free form Ξ",
        "Location": "Section 4, equation (10)",
        "Explanation": "The proof asserts that since Ξ≔P⁻¹dP+P⁻¹ΩP is divergence-free and U has trivial first Betti number, there exists ξ∈W¹,²₀(U;so(m)⊗Λ²ℝⁿ) with Ξ=d*ξ. This requires a Hodge decomposition on U with zero boundary data—valid only for bounded domains with smooth boundary. No argument is given for solving d*ξ=Ξ on general (or unbounded) U, so the existence of ξ in the stated space is not established."
      },
      {
        "Problem": "Incorrect vanishing of trace of triple product of skew-symmetric matrices",
        "Location": "Section 4, paragraph following equation (14)",
        "Explanation": "To conclude that ⟨ξ,d*ξ∧d*ξ⟩=0, the author claims that trace(M₁M₂M₃)=0 for all Mᵢ∈so(m). In general dimensions m>2, the trace of a product of three skew-symmetric matrices need not vanish without additional symmetry. The paper provides no proof or citation for this identity, yet it is crucial to derive the energy estimate kΞk_{L²}=0."
      },
      {
        "Problem": "Lack of rigor in distributional algebraic manipulations",
        "Location": "Section 4, derivation from equation (5) to equation (9)",
        "Explanation": "The derivation of the compatibility equation dΞ+Ξ∧Ξ=0 involves multiple multiplications and wedge products of distribution-valued forms, combined with L² and L¹ fields, and differentiation of such products. The paper does not rigorously justify that these operations—especially multiplying distributions by non-smooth functions and interchanging d with multiplication—are valid in the chosen function spaces, leaving the critical algebraic steps unsupported."
      },
      {
        "Problem": "Omitted global patching argument for isometric immersion",
        "Location": "Section 5, proof of Theorem 5.1 (reduction to local Pfaff system)",
        "Explanation": "The proof reduces the global existence of a W²,² isometric immersion to solving local Pfaff and Poincaré systems on coordinate charts and then invokes a 'standard monodromy argument' to patch them. However, there is no detailed treatment of transition functions or compatibility on overlaps, nor verification that the local frames agree globally, leaving a gap in establishing a well-defined global immersion."
      }
    ],
    "think_summary": [
      "**Exploring skew-symmetric matrices**\n\nFor a 3x3 case, I’m considering how general skew-symmetric matrices form so(3) by examining basis rotations around axes. It raises a question: does the trace of three skew matrices always vanish? \n\nI find that trace(M1 M2 M3) equals trace(-M3 M2 M1) due to properties of transposes and cyclic behavior. However, I wonder if the trace being equal to zero is guaranteed—is it possible that it doesn't vanish after all? I need to dig deeper into cyclic properties!",
      "**Examining traces of skew matrices**\n\nI’m analyzing whether the trace of three skew-symmetric matrices M1, M2, and M3 must equal zero. While there's a relation showing that trace(M1 M2 M3) equals -trace(M1 M3 M2), this doesn’t imply the trace is zero in general. \n\nIf I choose identical matrices, the trace won’t vanish. I also consider the skew matrices in dimension 3. The claimed trace relationship under permutation is tricky, and I need to verify examples to explore if that claim holds. Let's dig deeper!"
    ],
    "token_usage": {
      "input": 9122,
      "thinking": 4480,
      "output": 771
    }
  },
  {
    "entry_id": 198,
    "retraction_id": "1609.00445v2",
    "paper_id": "1609.00445v1",
    "retraction_comment": "Withdrawn due to an error in the numerical code, used to obtain the numerical results",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Use of mean-field approximation to claim spin-liquid stabilization",
        "Location": "Mean-field approximation section (page 3)",
        "Explanation": "The paper relies on a classical mean-field treatment to identify macroscopic degeneracy and infers spin-liquid behavior. Mean-field methods cannot capture the strong quantum fluctuations essential for genuine S=1/2 spin liquids, so the existence of a spin liquid cannot be established within this approximation."
      },
      {
        "Problem": "Neglect of symmetric anisotropic exchange (Γ) terms",
        "Location": "Effective Hamiltonian in Eq. (1) (page 2)",
        "Explanation": "In t2g systems with spin–orbit coupling, superexchange generically produces not only Heisenberg and Dzyaloshinsky–Moriya interactions but also symmetric anisotropic (Γ) exchanges. Omitting these terms may qualitatively alter the phase diagram and chiral order energetics."
      },
      {
        "Problem": "Assumption of pure a1g occupancy and neglect of eg levels",
        "Location": "Model section, paragraph beginning “As a simple and realistic case…” (page 2)",
        "Explanation": "The mapping to an S=1/2 a1g-only model assumes Δ_tri ≫ λ_LS, J_H and neglects mixing with eg and higher orbitals. In many pyrochlores the trigonal splitting is comparable to Hund’s and spin–orbit energies, so orbital admixture invalidates the derived J0, J1, D0, and D1 expressions."
      },
      {
        "Problem": "Omission of higher-order exchange processes (e.g. four-spin ring exchange)",
        "Location": "Superexchange derivation discussion (page 2–3)",
        "Explanation": "Fourth‐order virtual hopping processes generate multi‐spin or ring‐exchange terms in pyrochlore lattices, which can compete with or even dominate DM and Heisenberg couplings in S=1/2 systems. Neglecting them may render the proposed phase competition inaccurate."
      },
      {
        "Problem": "Reliance on idealized bond‐dependent hopping sign changes",
        "Location": "Multiorbital effects section, paragraph 1 (page 3)",
        "Explanation": "The J1 and D1 terms stem from assumed sign changes of interorbital hopping integrals between specific bonds. Small deviations in bond angles, lattice distortions, or orbital–lattice coupling can spoil these sign patterns, undermining the proposed control of dimensionality and chiral order."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 14153,
      "thinking": 1280,
      "output": 573
    }
  },
  {
    "entry_id": 199,
    "retraction_id": "1810.09697v2",
    "paper_id": "1810.09697v1",
    "retraction_comment": "We get more general results in Theorem 1.1. Corollary 5.27 and Theorem 1.3 are false",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Use of Lemma 2.8 without full proof or precise references",
        "Location": "Lemma 2.8 (page 4)",
        "Explanation": "Lemma 2.8 collects seven nontrivial facts about factorizations of σ(x^n), σ(P^n), etc., citing a mix of older papers but giving no full statements or proofs. Many later bounds and case splits (in Sections 3 and 5) depend on these facts. If any of these seven items is misstated or mis‐applied, the entire classification collapses. A paper that hinges on dozens of delicate subcases cannot defer these to opaque references without risking invalid deductions."
      },
      {
        "Problem": "Unproved “direct computations” in Lemma 3.2",
        "Location": "Lemma 3.2 (page 3)",
        "Explanation": "The authors assert that for each of the five small Mersenne polynomials M_j and for all exponents h_j in an infinite set Es, neither M_2 nor M_3 ever divides σ**(M_j^{h_j}), and that for j≠2,3 the bi‐unitary sums acquire a non‐Mersenne divisor. No uniform argument is given—only a vague “direct computations” remark. This claim underpins the bound r≤5 on the number of odd factors; if it fails for some h_j, the subsequent enumeration of candidates is invalid."
      },
      {
        "Problem": "No verification that each listed C₁,…,C₁₅ is actually bi‐unitary perfect",
        "Location": "Theorem 1.1 (page 2–3)",
        "Explanation": "The main result lists fifteen explicit polynomials said to exhaust all even bi‐unitary perfects with the given structure. However, the paper gives no standalone proofs (even sketches) that σ**(C_i)=C_i for i=1…15—only an un‐detailed appeal to Maple checks. Without showing these computations or giving a theoretical verification, one cannot rule out errors in the computer code or overlooked edge‐cases."
      },
      {
        "Problem": "Incomplete coverage of mixed‐parity cases in Section 5.2.4",
        "Location": "Section 5.2.4, especially Lemma 5.25 (page 13–14)",
        "Explanation": "When handling the case Q=σ(x^{2m}), the authors break into “a,b both even” and “a even, b odd” (and omit “a odd, b even” by symmetry) but then summarily dismiss the mixed‐parity subcase in Lemma 5.25 by a one‐paragraph parity argument. Given the delicacy of the exponent bounds, this cursory treatment could easily miss a viable solution, so the elimination of that entire region of the search lacks sufficient rigor."
      },
      {
        "Problem": "Heavy reliance on opaque Maple computations",
        "Location": "Various (e.g. end of Section 3, proofs of Lemmas 5.19, 5.24, Corollaries 5.16,5.21,5.26)",
        "Explanation": "Throughout the classification, key irreducibility checks and verification that no further cases survive are delegated to unspecified Maple routines. The paper never states the algorithms used, the range of parameters tested, or any certification that the search was exhaustive. Without this information, one cannot be confident that no overlooked examples invalidate the ‘if and only if’ characterizations."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 12838,
      "thinking": 3456,
      "output": 789
    }
  },
  {
    "entry_id": 200,
    "retraction_id": "1309.2621v7",
    "paper_id": "1309.2621v6",
    "retraction_comment": "This paper was withdrawn because the author did not prove that the function lambda_0(t) = gamma(t) on page 14 is strictly increasing. This is why we cannot make the crucial time change that proves the main theorem about infinite dimensional SRBM",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Unproven existence and uniqueness of infinite-dimensional SRBM",
        "Location": "Section 2, Theorems 2.1 & 2.2",
        "Explanation": "The main constructions assume the existence (and sometimes uniqueness) of an SRBM in the infinite orthant for general banded M-matrix reflections. Except for a narrow Harrison–Reiman argument in the Appendix under strong contraction hypotheses, no general existence/uniqueness of the infinite-dimensional reflected SDE is established. Without this, the entire programme of constructing competing Brownian particles via SRBM is unsupported."
      },
      {
        "Problem": "Gap-to-particle inversion does not guarantee a valid CBP solution",
        "Location": "Lemma 3.2",
        "Explanation": "Although the gap process Z is assumed to be an SRBM, the authors define named particle positions Y by formal summation of Z. They do not verify that the resulting Y satisfy the original SDEs with independent Brownian drivers or that the requisite local‐time terms split correctly. Thus the existence of a true competing-Brownian system realizing the given gap SRBM is not demonstrated."
      },
      {
        "Problem": "Infinite-dimensional Basic Adjoint Relationship (BAR) construction is inadequately justified",
        "Location": "Proof of Theorem 2.1 (Lemma 2.3)",
        "Explanation": "The extension of Dai–Kurtz patchwork-martingale techniques and the time-change arguments from finite to infinite dimensions are only sketched. Key steps—tightness, measurability of the controlled process, and justification of the limit exchanges—are omitted. Without a complete infinite-dimensional martingale-problem framework, the BAR result may fail."
      },
      {
        "Problem": "Unjustified infinite summation and integration in BAR verification",
        "Location": "Proof of Theorem 2.2",
        "Explanation": "When verifying that the product-of-exponentials measure satisfies the BAR, the authors perform multiple integrations by parts over infinitely many boundary faces and sum the resulting boundary integrals. There is no check of absolute convergence or uniform integrability of these infinite sums, so the claimed term-by-term cancellations could be invalid."
      },
      {
        "Problem": "No existence criteria for positive solution of the infinite difference equation",
        "Location": "Theorem 1.1, condition (3)",
        "Explanation": "The central hypothesis requires a strictly positive sequence b solving −q⁺ₙ bₙ₋₁ + bₙ − q⁻ₙ₊₁ bₙ₊₁ = gₙ − gₙ₊₁ for all n≥1. The paper provides no general existence or uniqueness results for this infinite linear system. In many choices of (g, q) the only solutions may change sign or diverge, making the stationary distributions vacuous."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 28872,
      "thinking": 1856,
      "output": 663
    }
  },
  {
    "entry_id": 201,
    "retraction_id": "0811.0505v2",
    "paper_id": "0811.0505v1",
    "retraction_comment": "This paper has been withdrawn by the author since there were errors in the calculus of the defect coefficient in Page 11. The corrected calculus gives actually zero which do not lead to a contradiction on the continuity of the flow-map of the Benjamin-Ono equation. The author warmly thank [REDACTED-NAME] G_rard for having pointing out this error to him",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Unjustified compact embedding of Bourgain spaces leading to false strong convergence and a.e. convergence of F_n",
        "Location": "Section 5.1 (proof of first assertion)",
        "Explanation": "The authors assert that a sequence bounded in X_{1/2,1/2} converges (up to subsequence) strongly in L^p(]-1,1[×T) for all 2≤p<∞ and hence almost everywhere. However, the claimed compact embedding X_{1/2,1/2}→L^p fails in two dimensions: one only has continuous embedding into L^4 and compact embeddings into L^p for p<4, not for arbitrary p. Without genuine strong convergence in a Lebesgue space one cannot deduce almost everywhere convergence of F_n, undermining the subsequent limit arguments for the gauge transform."
      },
      {
        "Problem": "Invalid passage to the limit in the nonlinear exponential map in Lemma 5.4",
        "Location": "Lemma 5.4, page 16",
        "Explanation": "Lemma 5.4 claims that from F_n⇀F in (C^∞)′ and boundedness in R^1_1 one can deduce e^{±iF_n/2}⇀e^{±iF/2} in (C^∞)′. The proof expands the exponential in a power series and appeals to strong convergence of each monomial F_n^k in L^p. But only distributional convergence of F_n is known, not strong convergence in any L^p. Thus the termwise dominated‐convergence argument for the series is unjustified, and the convergence of G_n to G fails without additional compactness."
      },
      {
        "Problem": "Equicontinuity of ||w_n(t)||_{L^2} in Lemma 5.10 not rigorously justified",
        "Location": "Lemma 5.10, page 26–27",
        "Explanation": "The uniform equicontinuity of the functions t↦||w_n(t)||_{L^2} is obtained by estimating \\||w_n(·)−V(·−t_0)w_n(t_0)||_{L^∞_tL^2_x} using Bourgain‐space Duhamel bounds with a small factor δ^(1/2−b). However, the embedding X_{1/2,0}→C_tL^2_x and the interplay of the δ‐gain do not guarantee a genuine modulus of continuity uniform in n. Without a clear control on the linear evolution term and the remainder, the claimed equicontinuity may fail, invalidating the compactness of the defect function a(t)."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 24155,
      "thinking": 4480,
      "output": 608
    }
  },
  {
    "entry_id": 202,
    "retraction_id": "1404.7350v2",
    "paper_id": "1404.7350v1",
    "retraction_comment": "The paper has been withdrawn by the author since Lemma 3.27 is wrong. The author thanks [REDACTED-NAME]",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Reliance on CH for constructing Milliken–Taylor ultrafilters at all successor stages",
        "Location": "Theorem 2.12, Section 2",
        "Explanation": "Theorem 2.12 crucially assumes CH to build the special Milliken–Taylor ultrafilters Uα in Vα. However, after any nontrivial forcing adding reals, CH fails in Vα (for α>0), so the construction of Uα cannot be carried out as stated at later stages of the iteration, undermining the whole inductive step."
      },
      {
        "Problem": "Assuming the diamond principle ♦(S²₁) is preserved through the iteration",
        "Location": "Section 1 (Introduction), p. 5, paragraph on ♦(S²₁)",
        "Explanation": "The argument uses a ♦(S²₁)-sequence in V0 to guess ultrafilters at successor stages. But proper forcings adding reals typically destroy ♦(S²₁), so in Vα for α>0 the diamond sequence may no longer predict ultrafilters, invalidating the at-most-n argument."
      },
      {
        "Problem": "Claim that ℵ₂ remains ℵ₂ under the countable support iteration of σ-centered forcings",
        "Location": "Section 5, Lemma 5.3",
        "Explanation": "The paper asserts that because each iterand is σ-centered (hence ccc) and proper, the entire countable support iteration has the ℵ₂-c.c. and thus preserves ℵ₂. In fact, countable support iterations of ccc forcings need not remain ccc or ℵ₂-c.c., so ℵ₂ may collapse, breaking key cardinal arithmetic assumptions."
      },
      {
        "Problem": "Insufficient justification for modified iteration theorem with stage-dependent relations",
        "Location": "Definition 4.13, Section 4",
        "Explanation": "The paper adapts Shelah’s [35, Ch. XVIII] preservation theorem to relations Rₙ,α whose definitions change at each stage α, but does not verify the necessary uniform closure and absoluteness properties for this non-standard setup, leaving the preservation of the R-properties unproven."
      },
      {
        "Problem": "Unproved generalization of Eisworth’s P-point preservation to k-coloured ultrafilters",
        "Location": "Theorem 3.5, Section 3",
        "Explanation": "Theorem 3.5 extends Eisworth’s result that Mathias forcing with a Milliken–Taylor ultrafilter preserves P-points from the single-colour case to k-colours, but offers no proof or reference for this nontrivial leap, so it is unclear whether M(U) indeed preserves arbitrary P-points in the k-coloured setting."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 43530,
      "thinking": 1792,
      "output": 625
    }
  },
  {
    "entry_id": 203,
    "retraction_id": "1612.01576v2",
    "paper_id": "1612.01576v1",
    "retraction_comment": "This paper has been withdrawn by the authors due to a crucial error in the inductive proof of Theorem 3.1",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [],
    "think_summary": [],
    "token_usage": {
      "input": 23176,
      "thinking": 9152,
      "output": 7
    }
  },
  {
    "entry_id": 204,
    "retraction_id": "1501.05036v2",
    "paper_id": "1501.05036v1",
    "retraction_comment": "Eq. (9) only implies correlation",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Unjustified uniform exponential length shrinkage assumption",
        "Location": "Section 2.3, assumption (H)",
        "Explanation": "Theorem 1 relies on an envelope estimate `(t) ≤ `(t₀)e⁻ᶩ⁽ᵗ – ᵗ₀⁾` for the length of a hyperbolic LCS, with a single global λ>0. However, the Cauchy–Green eigenvalues λ₂(x₀) generally vary along the LCS, and there is no proof or criterion guaranteeing a uniform lower bound on ln λ₂ that would produce a uniform exponential decay. Without verifying (H), the key decay integral in Theorem 1 may fail, invalidating the conclusion of vanishing path-average variation along the LCS."
      },
      {
        "Problem": "Neglect of finite-time non-monotonic transients in LCS length evolution",
        "Location": "Section 3, proof of Theorem 1",
        "Explanation": "The proof bounds the distance between two advected points by the instantaneous LCS length `(t)` and then applies the uniform decay (H) from t₀ to t₀+T. In practice, strainline length can exhibit non-monotonic oscillations or even temporary growth over finite times before settling into exponential decay. The analysis omits any initial transient period, so for moderate T the uniformity estimate on |¯f(x₁)–¯f(x₂)| may not hold, calling into question the practical relevance of the asymptotic bound."
      },
      {
        "Problem": "Lipschitz continuity requirement for time-dependent scalars not ensured",
        "Location": "Section 3, Proposition 2",
        "Explanation": "The bound |¯f(x₁)–¯f(x₂)| ≤ (K_f/T)∫ |φ_t(x₁)–φ_t(x₂)| dt uses a uniform spatial Lipschitz constant K_f for f(x,t). In many mixing diagnostics, f is derived from the flow (e.g. Okubo–Weiss) and may develop steep gradients or singularities over time, making K_f unbounded or growing with t. If K_f cannot be taken independent of t (or T), the decay estimate (9) need not hold, undermining the theoretical result."
      },
      {
        "Problem": "Numerical evidence compares strainlines to contours of instantaneous f rather than contours of path-averaged ¯f",
        "Location": "Section 4.2, Figure 6(a)",
        "Explanation": "Figure 6(a) reports the PDF of the angle between ∇f(x,0) and ξ₂, i.e. alignment between strainlines and contours of the initial scalar field f, not the path-averaged scalar ¯f. This misalignment in the numerical test fails to directly validate the main claim that hyperbolic LCSs align with contours of the path-averaged scalar, leaving the central conclusion unsupported by the presented data."
      },
      {
        "Problem": "Heuristic ergodicity argument for zero-level alignment lacks rigor",
        "Location": "Section 4.2, final paragraphs",
        "Explanation": "To explain why LCSs empirically align with the zero contour of ¯f, the paper invokes an ergodic-style argument that long trajectories sample the domain uniformly so ¯f tends toward its spatial mean. However, the turbulent flow is neither globally ergodic nor stationary, and f is time-dependent, so classical ergodic theorems do not apply. This undermines the post hoc rationale for zero-level alignment."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 24944,
      "thinking": 3328,
      "output": 822
    }
  },
  {
    "entry_id": 205,
    "retraction_id": "1208.6493v2",
    "paper_id": "1208.6493v1",
    "retraction_comment": "This paper has been withdrawn by the author due to an error in a claim about singular supports in the proof",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Incorrect application of the Paley–Wiener–Schwartz theorem leading to a wrong decay estimate for f(n)",
        "Location": "Page 4, paragraph following display (3)",
        "Explanation": "The author asserts that if F∈E′(ℝ) has support in (−π,π), then its inverse Fourier transform f satisfies |f(z)|≤C(1+|z|)^(−N)e^{π|Im z|}, and in particular |f(n)|≤C(1+|n|)^(−N).  In fact the Paley–Wiener–Schwartz theorem gives a growth bound |f(z)|≤C′(1+|z|)^N e^{π|Im z|}, so on the real line f(n) is at most polynomially growing, not decaying.  This invalidates the claimed decay of the coefficients and undermines the convergence arguments in S′(ℝ). "
      },
      {
        "Problem": "Unjustified multiplication of a periodic distribution by the indicator function",
        "Location": "Page 4, paragraph beginning “Also the singular support of Fe …”",
        "Explanation": "The paper claims that since sing supp 1_[−π,π]∩sing supp Fe=∅, the product 1_[−π,π]Fe is well-defined in D′(ℝ).  However, Fe is a periodic distribution and 1_[−π,π] is not a smooth function, so the standard criterion for multiplication in D′ does not directly apply without further microlocal or continuity estimates.  No rigorous justification is given for this key step."
      },
      {
        "Problem": "Use of a non–Schwartz test function to determine the constant in Example 3.2",
        "Location": "Page 5, second paragraph of Example 3.2",
        "Explanation": "To compute the constant c in the identity ∑e^{−inω}=c∑δ_{2πn}, the author plugs in the test function ϕ(ω)=e^{ω^2/(4π)}, which grows exponentially and is not in the Schwartz space S(ℝ).  Since the pairing in S′(ℝ) is only defined against ϕ∈S(ℝ), this computation is invalid."
      },
      {
        "Problem": "Circularity in the uniqueness argument (Theorem 1.2(1))",
        "Location": "Page 2, immediately after Theorem 1.2",
        "Explanation": "The author claims that part (1) (uniqueness) “clearly follows from (2)” by showing that any G with the same samples must reconstruct to the same f.  But (2) was only proved for the original F, not for an arbitrary G; the paper never establishes that G also satisfies the reconstruction formula.  Thus the uniqueness statement lacks an independent proof."
      }
    ],
    "think_summary": [
      "**Clarifying singular support**\n\nI'm evaluating the singular support of a function \\( F \\) and how it interacts with convolution. \\( F \\) is supported within the interval \\( (-\\pi, \\pi) \\), and after convolution with \\( \\delta_{2\\pi n} \\), it results in singular support in open intervals but not at endpoints. It’s interesting that while \\( F \\) may have singularities within those bounds, \\( Fe \\) can be smooth elsewhere. I'm noting that the support shifts with convolution as well. Hmm, it seems like a nuanced relationship!",
      "**Examining singular support and Fourier transforms**\n\nI'm analyzing the singular support of \\( Fe \\) and its relation to \\( F \\). When \\( F \\) is smooth outside the endpoints \\( \\pm \\pi \\), its singular support is confined within \\( (-\\pi, \\pi) \\), so \\( Fe \\) will also lack support at those endpoints. There’s a concern about the Fourier transform of distributions, focusing on assumptions about decay and convolution. If the series isn't bounded, multiplication could become an issue, meaning the product may not be well-defined. It’s a complex interplay!"
    ],
    "token_usage": {
      "input": 3621,
      "thinking": 4544,
      "output": 667
    }
  },
  {
    "entry_id": 206,
    "retraction_id": "1909.06350v2",
    "paper_id": "1909.06350v1",
    "retraction_comment": "The proof contained an error in the definition of the coupling in (4.8) that the authors currently cannot fix. The authors thank [REDACTED-NAME] for pointing this error out to them",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Unnatural growth condition on entry density and potential misuse in tail estimates",
        "Location": "Section 2, Assumption 2.2 (equation (2.2))",
        "Explanation": "Assumption 2.2 allows the density g of the matrix entries to satisfy \\|g\\|_{1+α} ≤ n^β, which means the distribution can degenerate as n→∞. However, the very small–η tail estimate for the least singular value in Lemma 3.3 relies on uniform control of the density (as in [4, Prop. 5.7] or [26, Thm. 3.2]). Permitting \\|g\\|_{1+α} to grow with n invalidates the hypotheses of those results and calls into question the bound on I₂."
      },
      {
        "Problem": "Incomplete verification of conditions for universality of small singular values",
        "Location": "Proposition 2.9",
        "Explanation": "The authors invoke [11, Theorem 3.2] to couple the smallest singular values of X−z and its Ginibre analogue, but do not fully check the required hypotheses—most notably the matching of the self-consistent density of states near zero and the uniform local law at microscopic scales for both ensembles. Without a detailed verification, the coupling estimate (2.15) may not hold."
      },
      {
        "Problem": "Uncontrolled remainder in cumulant expansion of long-time GFT",
        "Location": "Section 5, equation (5.42)",
        "Explanation": "The cumulant expansion for E[w_α g(w)] in (5.42) is truncated at finite order K without an explicit bound on the remainder term Ω(K,g). Although each derivative supposedly yields a factor n^(-1/2), the combinatorial proliferation of terms can overwhelm this gain unless Ω(K,g) is rigorously shown to be o((nη₂)^(-1/2)). The lack of such a bound undermines the error control in the Green function comparison."
      },
      {
        "Problem": "Questionable extension of isotropic local law to η ≪ n^{-1}",
        "Location": "Proposition 2.6",
        "Explanation": "While the averaged and entrywise local laws for H_z are established for η ≫ n^{-1} in [4], the paper extends them to arbitrarily small η by citing an argument from [12, Appendix A] tailored to the Gaussian case. In the current general i.i.d. setting with nonflat covariance operator S, that extension is not justified, yet the small-η regime is used in key estimates for I₄ and I₅."
      },
      {
        "Problem": "Insufficient decay in double GFT step for I₃–I₅ comparison",
        "Location": "Section 4, Lemma 4.3, equation (4.5)",
        "Explanation": "The bound in (4.5) estimates the difference of products of I₃,I₄,I₅ under the OU flow by O(t₁/√(nη₀)). With the chosen scales t₁≈n^(−1+ω₁) and η₀≈n^(−1−δ₀), this error is O(n^(ω₁+δ₀/2)), which grows unless ω₁ and δ₀ are made prohibitively small. No consistent choice is given to ensure an overall O(n^(−c)) bound, thus the claim of negligible error is unfounded."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 29466,
      "thinking": 3648,
      "output": 803
    }
  },
  {
    "entry_id": 207,
    "retraction_id": "0904.3281v2",
    "paper_id": "0904.3281v1",
    "retraction_comment": "The integrality statement is false. See the publication \"A norm compatible system of Galois cohomology classes for GSp(4)\" of the author for a correct statement and proof",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Undue reliance on weight theory for p-adic perverse sheaves",
        "Location": "Page 11, Section 3 (paragraph beginning “Then, by [19] Lem. 2.8…”) ",
        "Explanation": "The vanishing of weight 0 in R^4s_*W and of lower E2^{p,q} in the Hochschild–Serre sequence is deduced from Hodge-module results and Pink’s theorem for ℓ-adic sheaves. However, the paper uses Ekedahl’s formalism of weights for Λ=ℚ_p without verifying that the full theory of weights and purity holds in this p-adic setting. Without a solid p-adic weight theory, the key vanishing statements may fail, undermining the spectral-sequence argument and the identification of cohomology groups."
      },
      {
        "Problem": "Sketchy construction of the trace map f_! f^! → id for pro-sheaves",
        "Location": "Pages 6–7, Proposition 2.6",
        "Explanation": "The norm compatibility relies on an adjunction morphism f_!f^!→1 in the derived category of pro-étale sheaves. The paper gives only an ad hoc construction for constant sheaves and asserts without proof that it extends to the pro-sheaf Log_U in Db_c. The necessary functorial and compatiblity properties of this trace (especially for infinite pro-systems) are not established, casting doubt on the correctness of the norm-compatibility proof."
      },
      {
        "Problem": "Unjustified transfer of Saper’s analytic vanishing to p-adic étale cohomology",
        "Location": "Page 12, just before Corollary 3.3",
        "Explanation": "The claim that E2^{p,q}=0 for q<3 in the Hochschild–Serre E2 page is based on Saper’s L^2-cohomology vanishing theorem (analytic, Hodge-theoretic) and torsion-freeness. But the paper does not explain how these real or complex analytic results carry over to p-adic étale cohomology, nor does it reference a p-adic analogue, leaving a critical gap in the argument that identifies H^1(G, H^3) with H^4_abs."
      },
      {
        "Problem": "Lack of verification that geometric traces agree with Galois corestrictions in the cyclotomic tower",
        "Location": "Page 12, Corollary 3.3",
        "Explanation": "The construction produces compatible trace maps H^4(S(Np^{t+1}))→H^4(S(Np^t)), but the paper assumes without proof that these coincide with the usual corestriction maps in Galois cohomology H^1(Q(ζ_{Np^{t+1}}), H^3)→H^1(Q(ζ_{Np^t}), H^3). This identification is nontrivial and requires checking descent and base-change compatibility, which is not done, jeopardizing the claimed construction of a norm-compatible system."
      },
      {
        "Problem": "Unverified existence and uniqueness of the infinite-length logarithm pro-sheaf",
        "Location": "Page 3, Theorem 2.2 and Definition 2.3",
        "Explanation": "Theorem 2.2 asserts the existence of a universal n-unipotent sheaf Log_E^(n) and passes to the pro-limit Log_E without checking necessary finiteness or coherence conditions on the index category or on Ext in pro-sheaves. The abelian and derived-category arguments for pro-objects are delicate; without verifying these technical hypotheses, the foundational object Log_E may not exist or enjoy the stated universal property, invalidating the entire subsequent construction."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 12310,
      "thinking": 2624,
      "output": 859
    }
  },
  {
    "entry_id": 208,
    "retraction_id": "1705.03737v2",
    "paper_id": "1705.03737v1",
    "retraction_comment": "This paper contains a flaw that the proposed methods were overfitted thus the experimental results were not suitable. At this point, we do not want to update this article bu are developing a quite new approache where the authors are different from this paper,, and the title of our paper will be changed. 8 pages, 6 figures",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Uncalibrated and noisy ground-truth from stereoscopic movie frames",
        "Location": "Section 4 (page 4)",
        "Explanation": "The authors construct their training and test set by extracting left/right frames from commercial 3D movies without any camera calibration, rectification, or verification of alignment. Movie post-production (color grading, cropping, overlays) and compression artifacts introduce unknown misalignments and noise, so the ‘ground-truth’ right images do not reliably correspond to simple horizontal disparities. This label noise undermines both training supervision and the validity of the reported quantitative evaluations."
      },
      {
        "Problem": "No explicit mechanism for occlusion or disocclusion handling",
        "Location": "Section 3 (Equation 3 and rendering network description)",
        "Explanation": "The rendering stage synthesizes the novel view by weighted copying of translated source pixels over a fixed disparity range. There is no provision for filling pixels that become newly visible (disoccluded regions), nor modeling occlusions, leading to holes or ghosting in many real scenes. Without an explicit hole-filling or inpainting strategy, the method cannot reliably produce artifact-free novel views."
      },
      {
        "Problem": "Unfair comparison to Deep3D due to unclear training and implementation parity",
        "Location": "Section 6 (Experiments, pages 6–7)",
        "Explanation": "The paper re-implements Deep3D in MatConvNet but does not specify whether it was retrained on the same movie dataset, used identical preprocessing (e.g., cropping, scaling), or tuned hyperparameters comparably. Differences in training data, training schedule, or framework overhead may account for performance and speed disparities, invalidating claims of superiority over the state of the art."
      },
      {
        "Problem": "Inadequate evaluation metrics that do not reflect stereo or perceptual quality",
        "Location": "Section 6.1 (page 6)",
        "Explanation": "The sole reliance on pixel-wise MSE and MAE between synthesized and ground-truth RGB images overlooks critical aspects of novel-view synthesis such as accurate disparity estimation, occlusion consistency, and perceptual depth cues. Without metrics like disparity error, structural similarity (SSIM), or dedicated stereo quality measures, the reported quantitative gains may not translate into meaningful improvements in 3D view fidelity."
      },
      {
        "Problem": "No statistical analysis of subjective MOS differences",
        "Location": "Section 6.2 (page 7)",
        "Explanation": "The reported mean opinion scores (–0.37 vs. –0.48 on a –3 to +3 scale) are based on only 15 subjects and a difference of 0.11. No significance testing (e.g., t-test or confidence intervals) is provided to establish whether this small gap is statistically reliable. Consequently, the claim of perceptual superiority is not substantiated."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 14768,
      "thinking": 2496,
      "output": 649
    }
  },
  {
    "entry_id": 209,
    "retraction_id": "2307.01627v2",
    "paper_id": "2307.01627v1",
    "retraction_comment": "The proof of Theorem 2.6 is incorrect. Without this theorem the main claim of the paper becomes unproven",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Undefined well-posedness of c(G; A) and its use in the final bound",
        "Location": "Definition 3.1",
        "Explanation": "The authors define c(G; A) as “the number of trivial A-modules appearing as factors in any A-composition series of G,” but they never prove that this number is independent of the choice of A-composition series. In the nonabelian, noncoprime setting minimal normal A-invariant quotients need not be modules in the usual sense, and different series could yield different counts of A-central factors. The final inequality h(G) ≤ 2ℓ(A) + c(G; A)—and in particular the inductive decrement c(…; A)→c(…; A)-1—critically depends on c(G; A) being a well-defined invariant, yet no Jordan–Hölder argument or analogous justification is given."
      },
      {
        "Problem": "Misapplication and unexplained use of the Fong–Swan theorem",
        "Location": "Section 3, page 10, step (1)",
        "Explanation": "The authors invoke the Fong–Swan theorem to pass from a modular character χ (afforded by the section V = Ph as an F_pA-module) to a complex character of X⋊A without verifying that χ is a Brauer character in the required setting or that the solvability and coprimeness hypotheses of Fong–Swan are satisfied. The statement is neither standard for noncoprime actions nor necessary (one can obtain a complex permutation character from V directly), and its misuse casts doubt on the validity of the subsequent character‐theoretic arguments."
      },
      {
        "Problem": "Gaps in the proof of Theorem 2.5 on existence of a regular A-orbit",
        "Location": "Theorem 2.5, proof (pages 5–7)",
        "Explanation": "The derivation of the regular A-character constituent in χ^A compresses several nontrivial steps—extension and restriction of characters from Q, P, A0, and A; control of multiplicities; and the combinatorics of summing characters—into a brief paragraph without justification. In particular, the claim “at most one irreducible QA0-character is missing” and the construction of χ1 lack any detailed argument, making the conclusion that χ^A contains the full regular character of A unsubstantiated."
      },
      {
        "Problem": "Unverified hypotheses in applying Dade’s and Yang’s regular-orbit results",
        "Location": "Proof of Theorem 2.5, invoking Propositions 2.3 and 2.4 (page 5)",
        "Explanation": "To obtain a linear character λ1 of Q with C_A(λ1)=A0, the authors appeal to Proposition 2.3 (Dade) and Proposition 2.4 (Yang) without checking that A/A0 satisfies O_s(A/A0)=1 or the wreath-product freeness conditions required by these results. Since those technical hypotheses are essential to guarantee a regular orbit on each irreducible section, their omission may invalidate the claimed existence of λ1."
      },
      {
        "Problem": "Overlooked necessity of hall-subgroup normalization in Theorem 2.2",
        "Location": "Theorem 2.2 (page 4)",
        "Explanation": "Theorem 2.2 asserts that under the sole assumption ‘A normalizes a Sylow system of G,’ one can inductively apply Theorem 2.1 to find A-stable homogeneous components along any A-series. But Theorem 2.1 requires in each step that A normalizes a Hall r′-subgroup of G⋊A for the relevant prime r. Normalizing just the Sylow subgroups of G does not a priori guarantee normalization of the corresponding Hall subgroups in the semidirect product, so the induction may fail in intermediate steps."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 7996,
      "thinking": 6592,
      "output": 890
    }
  },
  {
    "entry_id": 210,
    "retraction_id": "1907.08721v2",
    "paper_id": "1907.08721v1",
    "retraction_comment": "A wrong fact on Hochschild homology was used in the proof of the main result (section 2, Theorem 2.0.4)",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Misapplication of Vishik’s theorem to ch(E) instead of v",
        "Location": "Theorem 2.0.4 proof, first paragraph",
        "Explanation": "Theorem 2.0.5 (Vishik) guarantees after a blow-up π that π^*(v) lies in the Z-subalgebra generated by divisor classes, where v∈CH^*(X) is the class defining the rank function. In the proof of Theorem 2.0.4 the author instead asserts that π^*(ch(E)) is so generated. This conflates the cycle v (in CH•) with ch(E); Vishik’s result does not apply to ch(E), so the subsequent argument that one can express π^*(ch(E)) via line-bundle Chern characters is unfounded."
      },
      {
        "Problem": "Ignoring integrality/denominator issues in passing from divisor classes to Chern characters of line bundles",
        "Location": "Theorem 2.0.4 proof, argument equating Z-algebra of divisors with that generated by ch(L)",
        "Explanation": "While the Q-algebra generated by divisor classes equals the Q-algebra generated by the ch(L_i), integrally one must invert factorials (since ch(L)=exp(c_1(L)) involves denominators). The paper treats this as an equality of Z-algebras, allowing an integral relation m·π^*ch(E)=P(ch(L_i)), but without accounting for the denominators. This invalidates the key contradiction used to produce a line bundle with nonzero rank."
      },
      {
        "Problem": "Assumed invariance of the dual homological unit under equivalence without proof",
        "Location": "Proof of Theorem 3.0.2, paragraph invoking that the unit for rk₂ is H•(X₂,ω_{X₂})",
        "Explanation": "The argument relies on the claim that under the derived equivalence Φ_E, the dual homological unit for the pulled-back rank function rk₂ on X₁ is exactly H•(X₂,ω_{X₂}). However, no general invariance theorem for dual homological units under arbitrary Fourier–Mukai equivalences is proved. One cannot simply identify the unit on X₁ with H•(X₂,ω) without establishing a natural isomorphism of the two units and verifying all functoriality and maximality conditions."
      },
      {
        "Problem": "Unjustified use of the Mukai-pairing isometry for arbitrary kernels",
        "Location": "Proposition 3.0.1, in the derivation of the Chow-theoretic rank function",
        "Explanation": "The proof applies the Mukai-pairing invariance result of Caldararu to the arbitrary kernel E∈D^b(X₁×X₂), asserting that Φ_{v(E)} is an isometry. But Caldararu’s theorem requires certain hypotheses (e.g. properness, compatibility with Hochschild–Kostant–Rosenberg) that are not verified for the given E. Without checking these, the identification of the new rank as ∫_{X₁}ch(–)∩β₁ is on shaky ground."
      },
      {
        "Problem": "Overlooked conditions for full faithfulness of π^* in Theorem 2.0.4",
        "Location": "Theorem 2.0.4 proof, use of π^* fully faithful because Rπ_*O_{X̃}=O_X",
        "Explanation": "The author claims that for the blow-up π:X̃→X one has Rπ_*O_{X̃}=O_X and hence that π^*:D^b(X)→D^b(X̃) is fully faithful. But full faithfulness also requires vanishing of R^iπ_*O_{X̃} for i>0. Although this holds for blow-ups along smooth centers by Grauert–Riemenschneider, it is not stated or proved. Omitting this check risks that π^* might fail to be fully faithful, breaking the semiorthogonal decomposition and the ensuing Hochschild decomposition."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 8331,
      "thinking": 2048,
      "output": 927
    }
  },
  {
    "entry_id": 211,
    "retraction_id": "1608.07104v2",
    "paper_id": "1608.07104v1",
    "retraction_comment": "This paper has been withdrawn due to errors in the crucial estimates in Lemma 1 and Theorem 5",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [],
    "think_summary": [
      "**Evaluating the user's paper**\n\nThe user wants me to check their attached paper for critical errors that could invalidate conclusions, while ignoring minor ones. They’ve asked for up to five problems presented as JSON objects. I’ll need to specifically look for logical or mathematical mistakes, especially related to definitions, like the X^b_s spaces and any typos in the expression for pk. It appears there might be a crucial error in defining pk as not squared, so I must pay close attention.",
      "**Analyzing the construction of solutions**\n\nThe user mentions working with functions \\( u \\) and \\( v \\) in the Sobolev space, noting that regularity is minor since by applying Meyers, \\( v \\) moves to a higher regularity class. It seems they are constructing \\( v_j \\) as certain CGO solutions for elliptic problems, using boundary maps effectively. They also utilize Fourier arguments, but there’s a concern about ensuring the imaginary components yield bounded growth on periodic domains while keeping real parts manageable. I need to ensure interpretations are correct about the domain and conditions for boundedness.",
      "**Evaluating the mathematical structure**\n\nThe user discusses operator applications on periodic distributions, emphasizing that \\( r \\) is generated from Fourier series on a periodic domain. They analyze whether certain conditions are satisfied, such as smallness on specific regions. There are concerns about uniform integrability and a supremum condition relating to dimensional measures. The user also mentions ensuring the bounded nature of extensions and continuity regarding functions on boundaries. It looks like most arguments hold, with minor typos noted. I'm looking forward to diving deeper into any potential errors and their implications for the main conclusions.",
      "**Assessing mathematical validity**\n\nI see the boundary values lead to uniqueness, and the Dirichlet uniqueness assumption holds under the right conditions. The coefficients seem adequately smooth, and the relevant equations check out. While I note a minor assumption may be missing regarding boundary conditions, it doesn't invalidate the paper's core claims. I could list minor issues, but overall, I don’t find critical errors. So, I think it’s safe to conclude with an empty list of significant problems: \"[]\". That sums up my assessment!"
    ],
    "token_usage": {
      "input": 10724,
      "thinking": 6400,
      "output": 7
    }
  },
  {
    "entry_id": 212,
    "retraction_id": "2401.17112v2",
    "paper_id": "2401.17112v1",
    "retraction_comment": "Bug. Lemma 1 is incorrect. The lemma needs the sets to be closed under subtraction which they are not",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [],
    "think_summary": [],
    "token_usage": {
      "input": 1850,
      "thinking": 6720,
      "output": 7
    }
  },
  {
    "entry_id": 213,
    "retraction_id": "1710.01525v2",
    "paper_id": "1710.01525v1",
    "retraction_comment": "We can not prove Lemma 1 in Sect 2.4, and Terras did not prove it either, we misunderstood Terras's result here. Thus our proof about Theorem 2 is wrong",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Only one inequality of the claimed double bound is proved",
        "Location": "Proof of Theorem 2, Section 2.4 (page 7, final paragraph)",
        "Explanation": "The statement to be proved is 2^{E(n)-1} < 3^{O(n)} < 2^{E(n)}, but the argument by contradiction only rules out the possibility 3^{O(n)} ≤ 2^{E(n)-1}, thereby establishing the lower bound 2^{E(n)-1} < 3^{O(n)}. The upper bound 3^{O(n)} < 2^{E(n)} is never addressed or derived, so the full claim remains unproved."
      },
      {
        "Problem": "Misstatement of periodicity modulus in Lemma 1",
        "Location": "Lemma 1, Section 2.4 (page 6)",
        "Explanation": "The paper asserts that if L = 2^{E(n)}·r + n then for all sufficiently large r one has G(L)=G(n). Standard results (e.g. Terras 1976) require matching the entire parity pattern of length G(n), which in general imposes a congruence condition mod 2^{G(n)}, not merely mod 2^{E(n)}. Without further justification, the claim that 2^{E(n)} suffices to force G(L)=G(n) is unfounded."
      },
      {
        "Problem": "Invalid bound on the remainder term in Lemma 3",
        "Location": "Lemma 3, Section 2.4 (page 6)",
        "Explanation": "The lemma writes Res(n)=∑_{i=0}^{O(n)-1} 3^{O(n)-1-i}/2^{λ(i)} and claims each summand is <1/3, hence Res(n)<O(n)/3. The justification given—using su[i]>n and K<n—yields only 3^{O(n)-1-i}/2^{λ(i)}<1, not <1/3, so the asserted Res(n)<O(n)/3 is incorrect. At best one can conclude Res(n)<O(n). Although the proof’s contradiction step only needs a bounded remainder, the specific 1/3 factor is unjustified."
      },
      {
        "Problem": "Confusing and inconsistent notation for counting odd and even steps",
        "Location": "Notation paragraph (page 2)",
        "Explanation": "The paper defines v_i = s_i mod 2 and ̅v_i = v_i⊕1, then dv(a,b)=∑v_i. It states O(n)=dv(0,G(n)) and E(n)=dv(0,G(n)), using the same dv symbol for both counts. This omits specifying whether E(n) counts zeros of v or of ̅v, rendering the definitions of O(n) and E(n) ambiguous and undermining clarity in subsequent arguments."
      }
    ],
    "think_summary": [
      "**Identifying a critical flaw**\n\nIt looks like the argument being reviewed only establishes one side of a critical double inequality. They show that \\(3^O(n) > 2^{E(n) - 1}\\), but fail to prove the upper bound of \\(3^O < 2^E\\). This results in an incomplete proof, as they don’t rule out \\(3^O > 2^E\\). The original equation was proposed to show both inequalities, but they only addressed one, resulting in a key error in their conclusion.",
      "**Analyzing critical problems**\n\nThe argument presented contains several key flaws. First, they incorrectly conclude that both Eq.(3) and Eq.(4) can't coexist. They fail to establish the upper limit of \\(3^{O(n)} < 2^{E(n)}\\) and have issues with the lemma regarding periodicity. Their lemma misapplies bounds and lacks proper justification. Additionally, they confuse notation and rely on proofs without adequate reference. I’m considering a structured JSON list of identified issues, focusing on clarity and comprehension for each problem."
    ],
    "token_usage": {
      "input": 6852,
      "thinking": 5184,
      "output": 662
    }
  },
  {
    "entry_id": 214,
    "retraction_id": "2011.05544v2",
    "paper_id": "2011.05544v1",
    "retraction_comment": "Comments are welcome. There is a problem with the Theorem 4.7. Which could be fixed by taking double duals (category of reflexive sheaves) but it ruins the double deformation construction",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Invalid extension of vector bundles over codimension-≥2 complements",
        "Location": "Page 7, Theorem 4.7",
        "Explanation": "Theorem 4.7 claims that any vector bundle on U⊂X with codim(X∖U)≥2 extends uniquely to a vector bundle on X, citing a MathOverflow post. In fact one only obtains a reflexive coherent sheaf in general, which need not be locally free when dim X≥3. This invalidates all subsequent constructions relying on such extensions as genuine vector bundles."
      },
      {
        "Problem": "Misapplication of Horrocks' theorem to higher dimensions",
        "Location": "Page 8, Corollary 4.10",
        "Explanation": "Corollary 4.10 reduces extension over the punctured plane to Horrocks' theorem, which applies only to regular local rings of dimension 2. Here the local ring at (0,0) in A×A² has dimension ≥3, so the triviality of bundles on the punctured spectrum fails and the claimed extension to X×A² breaks down."
      },
      {
        "Problem": "Unjustified pushouts in the exact category of vector bundles",
        "Location": "Page 7–8, Definition 4.5",
        "Explanation": "The “deformation to split extension” is constructed via pushouts of admissible monomorphisms in (Bq)^n Vect(X×A¹). Exact categories need not admit such pushouts, and even in Coh(X×A¹) the pushout of locally free sheaves need not remain locally free. Thus the key deformations may leave the category of binary complexes of vector bundles."
      },
      {
        "Problem": "Circular and incomplete A¹-invariance argument for binary complexes",
        "Location": "Page 3, Proposition 2.5",
        "Explanation": "The proof that K((Bq)^n Vect(X))→K((Bq)^n Vect(X×A¹)) is a homotopy equivalence invokes an exact sequence (2.1) without checking that its summands remain A¹-invariant or split functorially in the binary‐complex setting. The inductive step “take n to be n+i” is ill-defined, and no base for the induction in the binary case is established."
      },
      {
        "Problem": "Unverified applicability of Grayson’s fibration to Vect(X×A^d)",
        "Location": "Page 4, Equation (3.2)",
        "Explanation": "The fibration (3.2) is derived from Grayson’s Theorems 3.1–3.2, which require an R‐linear simplicial exact category M. The paper never checks that the simplicial category d↦(Bq)^n Vect(X×A^d) carries the necessary kΔ•-linear structure or satisfies the contractibility hypotheses, so the fibration sequence may not exist as claimed."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 10893,
      "thinking": 3136,
      "output": 670
    }
  },
  {
    "entry_id": 215,
    "retraction_id": "1301.3486v2",
    "paper_id": "1301.3486v1",
    "retraction_comment": "Withdrawn because certain correction terms that arise in the Lace expansion of Section 3 were not identified and taken into account in the subsequent derivation. A new version with these correction terms included is in preparation",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Gap in the validity of Hypothesis H for long-range models",
        "Location": "Proposition 1.2 vs Theorem 1.3",
        "Explanation": "Theorem 1.3 (and Proposition 1.4) require Hypothesis H, which is proved in Proposition 1.2 only for long-range percolation when d > 4(2∧α). However, Theorem 1.3 is stated under the weaker condition d > 3(2∧α). Thus there is a mismatch: Hypothesis H is not verified in the full regime where the main Hausdorff convergence result is claimed."
      },
      {
        "Problem": "Incorrect reference for uniform boundedness of Ψ̂ₙ(k)",
        "Location": "Section 2, paragraph after eq. (2.15)",
        "Explanation": "The argument for analyzing P̂_z(k) requires that the numerator Ψ̂_z(k)=∑ₙψₙzⁿ be uniformly bounded for z≤1 and all k. The text refers to Proposition 4.4 for this boundedness, but Proposition 4.4 only controls temporal derivatives of ψₙ. The required uniform bound on ∑ψₙzⁿ actually follows from Proposition 4.3(iii) (which shows ∑ₙ∑_x|x|^{δ}ψₙ(x)<∞), not from Proposition 4.4."
      },
      {
        "Problem": "Undefined constant v_α for finite-range (α=∞) case",
        "Location": "Assumption E and definition of f_α(n) in eq. (1.11)",
        "Explanation": "Finite-range models are treated by setting α=∞, so (2∧α)=2 and f_α(n)∝(v_αn)^(-1/2). However, Assumption E only defines v_α for finite α (including α=2) and does not specify v_∞ for the α=∞ case. This leaves the scaling factor f_α(n) undefined in the finite-range setting."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 60525,
      "thinking": 3008,
      "output": 493
    }
  },
  {
    "entry_id": 216,
    "retraction_id": "2101.07819v3",
    "paper_id": "2101.07819v2",
    "retraction_comment": "We are withdrawing because of a significant error. The category Weil used to define tangent structures is too strict for the construction of our main example. In particular the map alpha in 7.19 is not well-defined. To resolve this we will replace Weil with a suitable $\\infty$-category and post a new version when possible. We are grateful to [REDACTED-NAME] and [REDACTED-NAME] for pointing out this error",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [],
    "think_summary": [],
    "token_usage": {
      "input": 87864,
      "thinking": 256,
      "output": 7
    }
  },
  {
    "entry_id": 217,
    "retraction_id": "1711.07883v3",
    "paper_id": "1711.07883v2",
    "retraction_comment": "The paper has some problems on the Poisson homomorphism from the motivic Hall algebra to the motivic quantum torus",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Unproven existence of orientation data on DT/PT moduli schemes",
        "Location": "Section 1.2.10",
        "Explanation": "The construction of the global motives S^φ_DT(α) and S^φ_PT(α) requires choosing square roots of the canonical line bundles (orientations) on the moduli d-critical schemes. The paper simply assumes these orientations exist without citing a general existence theorem or verifying the necessary obstruction vanishing. Without establishing orientation data, the motivic vanishing cycle cannot be globally defined and the motivic correspondence statements become ill-posed."
      },
      {
        "Problem": "Unjustified extension of the integration map to completed/infinite-type Hall algebras",
        "Location": "Sections 2.2 and 2.3 (Theorem 2.9 and §2.3)",
        "Explanation": "The Poisson algebra homomorphism I is proved for the semi-classical Hall algebra of finite-type d-critical elements. Later the paper applies I to Λ-completed and infinite-type Hall algebras (e.g. H_ssc(A≤1)^Λ in (2.3.6)), without verifying that I extends continuously or compatibly to these completions. The convergence and well-definedness of I on infinite sums of motives is not addressed, undermining the subsequent application of integration to Hall algebra identities."
      },
      {
        "Problem": "Use of non-d-critical elements in Hall algebra identities requiring d-critical input",
        "Location": "Section 2.3 (Proposition 2.14)",
        "Explanation": "Proposition 2.14 employs Hall algebra elements such as 1_P and 1_Q (characteristic functions of torsion subcategories) which are not represented by d-critical schemes or stacks. Since the integration map I and its Poisson properties rely on d-critical structures, convolution identities involving non-d-critical elements fall outside its domain and cannot be integrated. This invalidates the derivation of the motivic DT/PT correspondence via I."
      },
      {
        "Problem": "Lack of verification of compatibility of d-critical structures and orientations under Fourier–Mukai for the flop",
        "Location": "Section 3.2–3.4",
        "Explanation": "The motivic flop formula hinges on transporting motives via the derived equivalence Φ. However, the paper does not verify that Φ carries the chosen d-critical charts and orientation data on Y to those on Y′ in a way that preserves the global motives S^φ. Without such compatibility, applying Φ to motivic partition functions is not justified, and the flop formula may fail."
      },
      {
        "Problem": "Unjustified generalization of the degree-zero motivic formula to higher rank",
        "Location": "Section 4.4 (Proposition 4.7)",
        "Explanation": "In Proposition 4.7 the paper asserts that the rank-r degree-zero motivic partition function equals (S_{1,0})^r times [Y]^virt by analogy with rank 1, treating higher-rank degree-zero objects as r independent copies of points. This leap assumes a trivial product formula in the motivic ring, ignoring potential extensions between summands and non-commutativity in the motivic quantum torus. No proof is given that the Hilbert scheme of rank-r degree-zero framed objects factorizes motivically, so the proposed formula lacks justification."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 23401,
      "thinking": 1024,
      "output": 773
    }
  },
  {
    "entry_id": 218,
    "retraction_id": "1603.02912v3",
    "paper_id": "1603.02912v2",
    "retraction_comment": "The calculation of the apparent charge density in this paper was wrong",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Misapplication of the Lorentz transformation of the four-current",
        "Location": "Eqs. (1)–(5) and surrounding discussion",
        "Explanation": "The author treats the transformation of ρ and j as if one can separately Lorentz-transform the components and then ‘‘correct’’ by a second coordinate transform.  In fact, (ρ,j) is a four-vector field Fμ(x), and the correct transformation is  F′μ(x′)=Λμν Fν(x)  evaluated at the corresponding spacetime point x↔x′.  There is no extra averaging or ‘‘miscounting’’ step—the standard result ρ′=γ(ρ−V jx) (and hence a nonzero ρ′ when ρ=0,jx≠0) already gives the charge density in the moving frame."
      },
      {
        "Problem": "Faulty ‘‘miscounting’’ argument in the sampling cell",
        "Location": "Discussion around Eqs. (3)–(4) and Fig. 1",
        "Explanation": "The paper claims that sampling at different t in the rest frame ‘‘adds’’ electrons that entered the cell later.  But in a steady‐state current the rest‐frame electron density is time‐independent, so counting at varying t does not change the total.  There is no ‘‘extra’’ electrons to explain away the standard ρ′."
      },
      {
        "Problem": "Neglect of Lorentz contraction of the sampling volume",
        "Location": "Entire averaging/counting argument",
        "Explanation": "The author ignores that a physical sampling cell moving at velocity V is length-contracted by 1/γ in the x-direction, which directly increases the measured density by γ.  This contraction, together with the four-current transformation, exactly produces the standard induced ρ′."
      },
      {
        "Problem": "Omission of the positive ion lattice contribution",
        "Location": "Model setup in Introduction and macroscopic averaging",
        "Explanation": "A neutral conductor at rest has stationary positive ions and moving electrons.  Both species’ charge-current four-vectors must be transformed.  The paper considers only the electrons’ miscounting and ignores that the ion density in the moving frame also transforms (and does not ‘‘miscount’’), yielding a net ρ′."
      },
      {
        "Problem": "Contradiction with the well-established electromagnetic field-tensor transformation",
        "Location": "Overall conclusion (no induced electric dipole or E-field)",
        "Explanation": "Maxwell’s equations in covariant form imply that a pure magnetic dipole at rest (Fμν with only spatial components) acquires a nonzero electric dipole moment under a boost.  Denying any induced ρ′ leads to a violation of the transformation properties of the field tensor Fμν, in conflict with experiment and countless textbook results."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 3434,
      "thinking": 1664,
      "output": 651
    }
  },
  {
    "entry_id": 219,
    "retraction_id": "2205.10165v2",
    "paper_id": "2205.10165v1",
    "retraction_comment": "It is not proved that the function $S$ is in fact an inner function",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Unjustified conclusion that the limit S is a singular inner function",
        "Location": "Section 1, after the recursion definition of {S_n}",
        "Explanation": "The author invokes Montel’s theorem to extract a subsequence S_{n_k}→S locally uniformly and then cites Hurwitz’s theorem to conclude S∈SInn. However, neither Montel nor Hurwitz guarantees that the boundary values of S satisfy |S(e^{iθ})|=1 almost everywhere. It is well known that a locally uniform limit of inner functions need not be inner (e.g. z^n→0). The paper fails to prove that S has unimodular radial boundary limits a.e. and no zeros in U, so S may not be singular inner and the counterexample collapses."
      },
      {
        "Problem": "Incorrect form of the unit–disk automorphism",
        "Location": "Theorem 1.2, surjectivity argument (page 3)",
        "Explanation": "The author asserts that any bijective inner self‐map of U must be w(z)=e^{iθ}(z−a)/(1−a z).  The correct form is w(z)=e^{iθ}(z−a)/(1−\\bar a z).  Omitting the conjugation on a is a nontrivial error in the classification of disk automorphisms."
      },
      {
        "Problem": "Unproven contraction mapping property",
        "Location": "Remark 1.1 and Remark 1.4",
        "Explanation": "The paper claims h(t)=exp[−(1+z t)/(1−z t)] is a contraction on |t|<1 since |h′(t)|≤c<1.  No uniform bound c<1 independent of z and t is established; in fact |h′(t)| blows up as z t→1.  Hence the Banach fixed–point theorem does not apply as stated, and uniqueness/convergence of the iteration is not secured by this argument."
      },
      {
        "Problem": "Lack of exclusion of constant fixed points",
        "Location": "Section 1, derivation of the fixed–point equation S=h(S)",
        "Explanation": "While the author finds a subsequential limit S satisfying S=exp[−(1+zS)/(1−zS)], there is no a priori argument excluding the possibility that S is a constant of modulus <1.  One must show that no constant solves the functional equation on U before concluding S is nonconstant and inner."
      },
      {
        "Problem": "Interchange of limits for boundary behavior without justification",
        "Location": "Section 1, assertion that S∈SInn",
        "Explanation": "The argument tacitly uses lim_{n→∞} lim_{r→1−}|S_{n_k}(re^{iθ})|=1 a.e. ⇒ lim_{r→1−}|S(r e^{iθ})|=1 a.e., but no result is cited to permit swapping the n→∞ and r→1− limits.  This interchange is nontrivial and must be justified (e.g. via normal families on Stolz regions), otherwise the boundary modulus condition defining innerness may fail for S."
      }
    ],
    "think_summary": [
      "**Exploring fixed point concepts**\n\nI'm noticing that they're assuming \\( h(t) \\) is a contraction, referring to Banach's fixed point in one remark. Still, they switch to Montel and Hurwitz in the proof without establishing a unique fixed point through contraction—just asserting its existence instead. They claim \\( h(t) \\) has a unique fixed point at \\( t=S \\), but they don't show the iterative scheme's convergence. They also select different subsequences converging to the same fixed point, which is interesting!",
      "**Identifying potential flaws**\n\nI'm noticing a potential flaw in their statement that \\( zS \\) is inner but not singular inner due to a zero at the origin. Any inner function with a zero should have a Blaschke factor, making \\( zS \\) a product of a Blaschke and a singular inner function—so it's not truly singular. They reference the Carathéodory theorem and Jordan curve but don’t utilize it in their conclusion. Also, they make a claim about contractions that lacks uniform bounding across \\( z \\), which is critical for the Banach fixed point.",
      "**Examining assumptions and errors**\n\nI'm exploring their claim that they chose one \\( S \\) for a counterexample, realizing unique fixed points aren't necessary since multiple accumulation points can still satisfy the fixed equation. Their remark about Banach appears unused, which isn't critical. Key is their assumption regarding \\( f(w) = w \\exp \\left( \\frac{1+w}{1-w} \\right) \\). While \\( f \\) is analytic on \\( w(U) \\), they mistakenly omitted conjugation, but since \\( a \\) ends up equating to zero, the overall logic appears sound, even if there are minor conceptual errors.",
      "**Identifying critical flaws**\n\nI'm analyzing their approach and realizing they only used functional equations, which is problematic since the original goal was to determine if \\( S \\) is singular inner. They haven't proven that yet, so the counterexample is flawed. Key issues include showing that \\( S \\) is singular inner, verifying boundary conditions, and misapplying the Banach contraction without uniform contraction proof. Their assumption about inner functions and convergence also lacks justification. Overall, I can distill these into five key flaws: proving \\( S \\), the automorphism representation, the contraction claim, assuming non-constant solutions, and boundary property limits."
    ],
    "token_usage": {
      "input": 3409,
      "thinking": 8576,
      "output": 734
    }
  },
  {
    "entry_id": 220,
    "retraction_id": "1511.00570v2",
    "paper_id": "1511.00570v1",
    "retraction_comment": "This paper has been withdrawn because the analysis therein completely oversimplified the physics during primordial nucleosynthesis",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Incorrect evolution equation for α in the pre–deuterium‐bottleneck era: missing proton absorption term",
        "Location": "Eq. (13), first line",
        "Explanation": "The paper gives α̇ = Γₙ + (Γₙ + Γ_X) α, but a correct derivation from Ṅₙ = −(Γₙ+ΓₙX)Nₙ and Ṅₚ = ΓₙNₙ − ΓₚX Nₚ yields α̇ = Γₙ + [Γₙ + ΓₙX − ΓₚX] α. Omitting the −ΓₚX α term biases the integrals a and b and therefore the predicted change in Yₚ."
      },
      {
        "Problem": "Incorrect evolution equation for α in the post–deuterium‐bottleneck era: wrong prefactor",
        "Location": "Eq. (13), second line",
        "Explanation": "They use α̇ = (α−1)(Γ₄X − ΓₚX), but defining α = Nₚ,f/N₄ and Ṅ₄ = −Γ₄X N₄, Ṅₚ,f = −ΓₚX Nₚ,f gives α̇ = α(Γ₄X − ΓₚX). The extra ‘−1’ shifts the c integral and thus the final ∆X₄, invalidating the constraint especially near V(Rₓ)=0."
      },
      {
        "Problem": "Oversimplified BBN network—neglect of deuterium and other light nuclei",
        "Location": "Sec. II, Eqs. (5)–(8) and surrounding discussion",
        "Explanation": "The analysis tracks only neutrons, protons, and ⁴He, ignoring the formation and destruction of D, ³He, and other intermediates. Macro absorption or catalysis of these species could shift the deuterium bottleneck temperature TB and alter the ⁴He yield, so the pure absorption model is inadequate to capture full BBN dynamics."
      },
      {
        "Problem": "Arbitrary assumption that TB is unaffected if <10% of baryons are absorbed",
        "Location": "Paragraph below Eq. (20)",
        "Explanation": "They assert the analysis holds as long as fewer than 10% of neutrons are lost between T_F and T_B, but provide no calculation showing how baryon absorption alters TB. Without quantifying the sensitivity of TB to macro interactions, this threshold is unjustified and could permit significant shifts in Yₚ even in the ‘allowed’ regime."
      },
      {
        "Problem": "Crude interpolation of the time–temperature relation",
        "Location": "Eqs. (19)–(20) and accompanying text",
        "Explanation": "The use of a simple tanh-based interpolation for θ(T₉) between two anchor points neglects known features like neutrino decoupling and e⁺e⁻ annihilation. Errors in θ(T) feed directly into the integrals a, b, and c and can substantially alter the computed macro‐induced shifts in Yₚ."
      }
    ],
    "think_summary": [
      "**Evaluating BBN network assumptions**\n\nI'm looking at the BBN network and noticing some oversimplifications, like focusing only on certain elements while ignoring the effects on deuterium and uncertainties in parameters. There's this assumption about baryon absorption being less than 10% affecting temperatures, but it feels arbitrary and unverified. I’m also concerned about possible errors in time-temperature interpolations that could lead to significant inaccuracies. Lastly, there seems to be a misprint in a constraint equation that implies incorrect scaling, which I need to examine further."
    ],
    "token_usage": {
      "input": 7338,
      "thinking": 4992,
      "output": 731
    }
  },
  {
    "entry_id": 221,
    "retraction_id": "1911.02706v2",
    "paper_id": "1911.02706v1",
    "retraction_comment": "A missing sign in the argument to prove Lemma 2 renders the proof incorrect. The note is withdrawn since we are unable to provide a corrected proof that works with the stated generality",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Sign error and flawed computation in Lemma 2’s Bochner–Bianchi argument",
        "Location": "Lemma 2 (page 5), formula for –δδ(hφ)",
        "Explanation": "The paper asserts –δδ(hφ)=Δ²ϕ+Ric·∇²ϕ+½(∇s,∇ϕ), but a correct application of δ(hφ)_i=−Δ∇_iϕ+Ric_i^j∇_jϕ and ∇^iRic_{ij}=−½∇_j s gives –δδ(hφ)=Δ²ϕ−Ric·∇²ϕ+½(∇s,∇ϕ). This sign error propagates into the derived fourth‐order operator on s and invalidates the conclusion ∇²s=0."
      },
      {
        "Problem": "Insufficient justification that Hessian variations detect nonconstancy of s",
        "Location": "Lemma 2 (page 5), use of hϕ=∇²ϕ variations",
        "Explanation": "The proof uses only metric deformations of the form hϕ=∇²ϕ to force a PDE on s, but does not show that these variations span a large enough subspace of TgM₁ to conclude ∇²s=0 everywhere. Without confirming density or spanning properties, vanishing of the first variation on this subspace need not imply s is constant."
      },
      {
        "Problem": "Potential factor and integration‐by‐parts inconsistencies in the Euler–Lagrange derivation",
        "Location": "Lemma 1 (page 3), transition from (7) to (8) and expression for λ in (9)",
        "Explanation": "The integration by parts leading from the first variation formula (7) to the tensor equation (8) and the multiplier λ in (9) involves delicate handling of the ¼sg trace h term and divergence terms. Any misplacement of factors or overlooked boundary vanishing could alter the form of the critical equation, compromising all downstream arguments."
      },
      {
        "Problem": "Misapplication of Aronszajn’s unique continuation to a nonlinear scalar‐curvature equation",
        "Location": "Theorem 3 (page 6), use of (10) to extend local Einstein condition globally",
        "Explanation": "The paper appeals to Aronszajn’s theorem—valid for linear elliptic PDEs—to conclude that a scalar curvature nonzero on an open set must be constant everywhere from the nonlinear equation (10). Without linearization or additional hypotheses ensuring applicability, this step is not justified."
      },
      {
        "Problem": "Unsubstantiated curvature formulas in Example 6",
        "Location": "Example 6 (page 7), formulas for s_{gε} and s^J_{gε}",
        "Explanation": "The claimed expressions for the scalar and J‐scalar curvatures under vertical dilation of the Calabi–Eckmann metric are given without derivation. Given the complexity of the almost complex structure and its curvature, these assertions require detailed verification before serving as valid motivation."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 6413,
      "thinking": 4800,
      "output": 709
    }
  },
  {
    "entry_id": 222,
    "retraction_id": "1806.06939v2",
    "paper_id": "1806.06939v1",
    "retraction_comment": "The objective in (8) allows for trivial solutions e.g. the prior",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Inconsistent likelihood modeling",
        "Location": "Sec 3.2 Eq (5)–(6) vs. Sec 3.3 negative log-likelihood definition",
        "Explanation": "The paper posits a Gaussian observation model on the pre-softmax logits (Eq 5–6) but then substitutes a robust L1 plus spatial smoothness penalty as the ‘negative log p(sf|sp,ω)’ in Eq 7. This loss does not correspond to the assumed Gaussian likelihood, so the variational objective is not an ELBO of any coherent probabilistic model, invalidating the theoretical Bayesian treatment."
      },
      {
        "Problem": "Flawed importance-sampling bound derivation",
        "Location": "Sec 3.3 Eq (8) and Appendix A",
        "Explanation": "The two successive inequalities used to derive the bound in Eq (8) rely on restrictive assumptions (e.g. all probabilities ≤1) and yield a looser surrogate objective that is not guaranteed to approximate the true variational bound. As a result, minimizing the proposed objective does not correspond to minimizing the KL divergence to the true posterior, undermining the claimed theoretical justification."
      },
      {
        "Problem": "Oracle-based multi-modality evaluation",
        "Location": "Sec 4 Table 3 (oracle top 5% best-sample mIoU)",
        "Explanation": "Measuring diversity by selecting the best 5% of samples according to ground truth (an ‘oracle’) artificially inflates performance and does not reflect the model’s predictive distribution quality in real deployment, where ground truth is unavailable. This biased metric overestimates the method’s ability to capture true multi-modality."
      },
      {
        "Problem": "Uncontrolled comparison to prior work",
        "Location": "Sec 4 Table 1 and accompanying text",
        "Explanation": "The state-of-the-art comparison uses different segmentation networks and fine-tuning schedules for each method, and the input data to each model is generated by a different segmentation algorithm. These inconsistencies prevent an apples-to-apples evaluation, so the reported improvements over [15,11] cannot be conclusively attributed to the proposed Bayesian approach."
      },
      {
        "Problem": "Inadequate CVAE baseline",
        "Location": "Sec 4 Table 4 and ‘Comparison to a CVAE baseline’ paragraph",
        "Explanation": "The implemented CVAE baseline uses a single Gaussian noise injection at the last layer—an overly simplistic design not representative of modern CVAE architectures. This weak baseline yields unfair comparisons and invalidates claims about the superiority of the Bayesian method over CVAEs."
      }
    ],
    "think_summary": [
      "**Evaluating method soundness**\n\nI’m pondering the use of a recognition network to approximate weight posterior. It’s certainly a novel approach, but I can’t help but question its soundness—could it overfit the training set and fail to generalize? They report good empirical performance without substantial error. There’s also the question of whether the method captures multi-modality effectively. While they implement weight sampling that introduces variation, I wonder if this really achieves the pixel-level multi-modality they're aiming for. The diversity shown in their best-of-5 oracle results seems promising, but the true evaluation of that distribution might be lacking.",
      "**Critiquing experimental comparisons**\n\nI see an important issue: they compare their results to a CVAE baseline but do so improperly. They claim the CVAE performs worse, yet they used a single Gaussian noise injection and a potentially underpowered network. This weakens their claim of superiority. They also note differences in segmentation algorithms and fine-tuned their own model without controlling for others, making comparisons unclear. Additionally, their diversity measurement using “mean of oracle top 5%” is biased since it favors models aligning closely with ground truth, neglecting proper metrics for assessing multi-modality.",
      "**Assessing performance comparison issues**\n\nI’m analyzing the lack of standardized inputs and inconsistent evaluation settings, which can really confuse performance comparisons. Specifically, the CVAE baseline is problematic because it only uses a single Gaussian noise injection in the last layer. This design is not in line with state-of-the-art CVAE methodologies and makes the comparison unfair. This undermines the conclusions about the superiority of the Bayesian method. I need to summarize these issues in a JSON list format to clarify the problems."
    ],
    "token_usage": {
      "input": 17755,
      "thinking": 4416,
      "output": 601
    }
  },
  {
    "entry_id": 223,
    "retraction_id": "1310.8031v2",
    "paper_id": "1310.8031v1",
    "retraction_comment": "The solution for the NS equations provided can only be constant or very small magnitude",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Unrealistic uniform Sobolev‐norm assumption",
        "Location": "Page 7, equation (38)",
        "Explanation": "The paper assumes supₖ∥u₀∥ₖ<∞, i.e. a uniform bound on all Sobolev norms of the initial data. This is far stronger than mere smoothness (C^∞) and amounts to an analyticity‐type assumption with uniform radius. No justification is given, and it effectively restricts the result to a very special class of initial data rather than all smooth divergence‐free fields."
      },
      {
        "Problem": "Incorrect resolution of the nonlinear differential inequality",
        "Location": "Page 8, equations (41)–(43)",
        "Explanation": "From (41) one obtains d/dt y ≤ C y^q with q=(4r+3)/(2r+1)>1, which is known to lead to finite‐time blow‐up of y(t), not global boundedness. The subsequent ‘‘solution’’ in (42)–(43) misplaces signs and exponents, falsely concluding y(t) stays bounded for all t<T by picking large r. In reality this differential inequality predicts blow‐up in finite time, invalidating the core bootstrap in Proposition 5."
      },
      {
        "Problem": "Misapplication of Gagliardo–Nirenberg interpolation in Lemma 4",
        "Location": "Page 5–6, equations (33)–(36)",
        "Explanation": "The derivation of estimate (36) purportedly bounds each nonlinear term by c′∥u∥_r^{1+1/2}∥u∥_{r+1/2}. The interpolation exponents and constants depend subtantially on r, yet the paper treats c′ as r‐independent and ignores the precise embedding conditions. This step is neither rigorously justified nor consistent with standard Sobolev embeddings in 3D."
      },
      {
        "Problem": "Eigenvalue miscalculation in the damping estimate",
        "Location": "Page 8, end of Proposition 5",
        "Explanation": "The author uses 2λ₁=4π to control decay, but from (4) one has λ₁=(2π)²=4π², hence 2λ₁=8π². This numerical error weakens the Poincaré‐type damping and undermines the quantitative bounds claimed in (43). Even if corrected, the qualitative blow‐up issue remains."
      },
      {
        "Problem": "Circular bootstrap without low‐order norm control",
        "Location": "Section 3, Proposition 5 overall argument",
        "Explanation": "The proof directly attempts to bound a high Sobolev norm ∥u∥_r without first establishing global control of the basic L² or H¹ norms beyond short time. There is no mechanism to prevent accumulation of nonlinear growth at low modes, making the bootstrap on ∥u∥_r invalid from the outset."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 7469,
      "thinking": 3072,
      "output": 687
    }
  },
  {
    "entry_id": 224,
    "retraction_id": "0904.3516v4",
    "paper_id": "0904.3516v3",
    "retraction_comment": "This paper has been withdrawn by the authors. The present version has several results that are correct, but, there is a problem in the use of sections 7 and 8 to derive generic properties for the set of analytic potentials g. All sections before this are OK",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "No derivation of the key integral representation φ_β(x)=∫h_β(w,x) dν^*_β(w)",
        "Location": "Sections 2–5",
        "Explanation": "The large-β asymptotics of φ_β(x) and the ensuing Varadhan argument both rest on writing φ_β(x) as a Laplace integral of the kernel h_β(w,x) against the dual Ruelle-eigenmeasure ν^*_β on Σ:  φ_β(x)=∫h_β(w,x) dν^*_β(w).  However, the paper never proves this representation for their novel kernel h_β (they only show an analogous formula for a fixed shift-invariant measure µ coming from Section 2, not for ν^*_β).  Without establishing φ_β in terms of h_β and ν^*_β, the Varadhan-type limit leading to V(x)=sup_w[H_∞(w,x)−I^*(w)] is unsupported."
      },
      {
        "Problem": "Genericity results are proved only in C^α but then asserted for analytic potentials",
        "Location": "Section 8 (Theorem 8.1) and conclusions in Section 6",
        "Explanation": "The paper’s genericity theorem (that for an open–dense set of potentials one has a unique periodic maximizing measure and R^*>0 off its support) is carried out in the Banach space C^α(K).  Yet the main statements (e.g. Theorem 6.2, Abstract) claim this genericity for real-analytic g.  Real-analytic functions do not form a Baire space under the C^α or any comparable topology, so residuality in C^α does not imply ‘‘generic analytic’’.  Hence the claimed generic piecewise-analytic subaction for analytic g is not established."
      },
      {
        "Problem": "Non-uniqueness of the limit kernel H_∞(ω,x)",
        "Location": "Corollary 5.2",
        "Explanation": "The existence of H_∞(ω,x)=lim_{β_n→∞}(1/β_n) log h_{β_n}(ω,x) is obtained by a diagonal/Arzelà–Ascoli extraction of a subsequence β_n.  But there is no argument that every sequence β→∞ yields the same H_∞ or that the chosen subsequence is canonical.  In principle different subsequences could give different H_∞, so the subsequent calibrated subaction V(x)=sup_w[H_∞(w,x)−I^*(w)] might depend on this choice, invalidating the uniqueness and well-definedness of V."
      },
      {
        "Problem": "Flawed argument that finite I^*(w) implies finite return time to the maximizing support",
        "Location": "Section 6 (proof that R^* good ⇒ only finitely many w with I^*(w)<∞)",
        "Explanation": "The paper argues that if I^*(w)<∞ then the orbit {σ^n(w)} must enter the maximizing support M in uniformly bounded time, else one contradicts positivity of R^*.  The decomposition into two cases (a) infinitely many long blocks in A_δ and (b) all blocks bounded) is neither exhaustive nor rigorous: it does not preclude orbits that return to A_δ in bounded time but still avoid M indefinitely, nor does the shadowing argument enforce passage into M.  Thus the key finiteness of candidate w’s used in the piecewise-analytic conclusion is unjustified."
      },
      {
        "Problem": "Twist assumption on the original kernel W₁ does not imply twist for the limiting kernel W=W_∞−V^*",
        "Location": "Section 6 (use of twist in monotonicity of w_x)",
        "Explanation": "All monotonicity and ‘‘locally constant’’ arguments for the selection w_x rely on a twist condition for the involution kernel actually used in V(x)=sup[W(w,x)−…].  The twist is only assumed for the initial kernel W₁ (or any kernel cohomologous to it), but the paper replaces W₁ by W_∞ and then by W=W_∞−V^*.  Although twist is invariant under adding functions of w alone, the passage from W₁ to W_∞ is nontrivial and no check is given that W_∞ (or W) still satisfies the required twist inequality.  Without this, the crucial monotonicity of w_x fails."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 35677,
      "thinking": 8384,
      "output": 1011
    }
  },
  {
    "entry_id": 225,
    "retraction_id": "1509.01802v3",
    "paper_id": "1509.01802v2",
    "retraction_comment": "This paper has been withdrawn by the author due to a crucial error in equations 22 and 23",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Misidentification of factorization in term c) as two independent 2→2 scatterings",
        "Location": "Section 2.3 (Eq. 17) and Fig. 2",
        "Explanation": "The decomposition of the color-ordered 7-gluon amplitude (term c) into three sub-amplitudes linked by propagators P₃₄₅ and P₆₇₁ does not correspond to two disconnected hard scatterings. The intermediate lines share external legs (gluons 1 and 3 appear in both sub-amplitudes), so this ‘‘factorization’’ remains a connected 3→4 process and cannot be interpreted as two independent 2→2 DPI subprocesses."
      },
      {
        "Problem": "Overreliance on a single color-ordered amplitude without full permutation sum",
        "Location": "Section 2 (Eq. 1–2)",
        "Explanation": "While individual color-ordered amplitudes are gauge invariant, the physical amplitude squared involves a sum over all non-cyclic permutations with potential cancellations or enhancements among terms. Focusing on one ordering omits interference terms (even if 1/N² suppressed) that can alter or cancel supposed singular behaviors relevant for DPI kinematics."
      },
      {
        "Problem": "Kinematic region for simultaneous P₃₄₅→0 and P₆₇₁→0 is measure zero and yields no enhancement",
        "Location": "Section 2.3.1 (Eqs. 20–25)",
        "Explanation": "The double collinear limit where both invariants s₃₄₅ and s₆₇₁ vanish corresponds to a highly constrained (codimension-4) region in phase space. The cancellation of the p₂ dependence in Eq. (24) shows the amplitude remains finite and even vanishes in the exact alignment (Eq. 25), so there is no genuine power-law enhancement after phase-space integration to support a DPI-like contribution."
      },
      {
        "Problem": "Failure to account for convolution with parton distributions and phase-space integration",
        "Location": "Concluding Discussion (Section 3)",
        "Explanation": "Conclusions are drawn solely from tree-level amplitude singularities without embedding the result into the full cross-section calculation, which requires integration over parton transverse momenta and weighting by double parton distributions. Potential logarithmic enhancements or suppressions from PDF evolution and kₜ-factorization are neglected, invalidating claims about the size of DPI contributions."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 10839,
      "thinking": 2688,
      "output": 578
    }
  },
  {
    "entry_id": 226,
    "retraction_id": "2312.05804v3",
    "paper_id": "2312.05804v2",
    "retraction_comment": "Error in the derivation of equation 11 in section 4.3.1",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Independent application of Score Distillation Sampling (SDS) to decoupled body and clothing layers",
        "Location": "Section 4.2, Equations (8)–(9)",
        "Explanation": "The paper applies SDS separately to the body and clothing NeRFs as if they were independent. In practice, the diffusion model’s gradients depend on the entire rendered scene (including occlusion, shading, and context). Decoupling SDS in isolation ignores these interactions, leading to guidance that is mismatched to the integrated 3D human, and thus undermining the fidelity and consistency of the generated geometry and appearance."
      },
      {
        "Problem": "Semantic-confidence network lacks ground-truth supervision and may converge to trivial or incorrect weighting",
        "Location": "Section 4.3.1, Equations (10)–(12)",
        "Explanation": "The 3D semantic weight network F_sw is trained solely to regress its own predictions (semantic confidence sc) toward features fs produced by a pretrained semantic-aware network F_sa, without any labeled clothing masks or segmentation ground truth. This self-supervision can lead to collapsed or semantically meaningless confidence estimates, invalidating claims of robust removal of non-clothing content."
      },
      {
        "Problem": "Ill-posed shape-matching loss for SMPL-driven deformation",
        "Location": "Section 4.3.2, Equation (13)",
        "Explanation": "The matching loss L_match = Huber(I_cloth_mask + I_cloth_smpl - I_body_smpl) does not directly measure overlap or alignment between body and clothing meshes. Summing and subtracting binary masks can be minimized by trivial distortions (e.g., shrinking the body mesh), and the weak L2 regularization on vertex offsets may not sufficiently prevent degenerate solutions. This undermines reliable geometric matching of body and clothing layers."
      },
      {
        "Problem": "Incorrect density initialization via softplus inverse of sigmoid-transformed SDF values",
        "Location": "Section 4.2, Equation (5)",
        "Explanation": "The density δ_x is initialized as max(0, softplus^{-1}(v_x)), where v_x is a sigmoid of a signed distance. However, softplus^{-1} is not the inverse of softplus, and applying it to bounded sigmoid outputs is mathematically inconsistent. This misapplication can produce invalid or unstable density values, preventing correct convergence to the intended geometric priors."
      },
      {
        "Problem": "Contradiction between claimed template-free generation and reliance on SMPL/SMPL-X priors",
        "Location": "Abstract, Introduction (Sec. 1)",
        "Explanation": "The paper asserts generation \"without being constrained by specific templates,\" yet the entire method is built upon SMPL and SMPL-X parametric body models (including inverse LBS, layered priors, and deformation networks). This undermines the novelty claim and suggests the method remains heavily template-dependent."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 19808,
      "thinking": 960,
      "output": 658
    }
  },
  {
    "entry_id": 227,
    "retraction_id": "1401.1766v5",
    "paper_id": "1401.1766v4",
    "retraction_comment": "This paper has been withdrawn by the author due to errors in figure 1",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Unrigorous evaluation methodology relying solely on subjective judgments",
        "Location": "Pages 16–17, Results section",
        "Explanation": "The study evaluates G-Bean against PubMed using only subjective preferences collected from 20 graduate students, without employing standard IR evaluation metrics (e.g., precision, recall, MAP) or a gold-standard relevance judgement set. There is no reporting of inter-annotator agreement or statistical tests, undermining the validity of the performance claims."
      },
      {
        "Problem": "Unfair and suboptimal PubMed baseline configuration",
        "Location": "Page 17, discussion of query #17 and Table 3",
        "Explanation": "The comparison uses PubMed’s default behavior (assumed AND between all tokens) without exploiting PubMed’s advanced query features (MeSH automatic term mapping, Boolean operators, field tags). This leads to queries returning no results, unfairly disadvantaging PubMed and biasing the comparison in favor of G-Bean."
      },
      {
        "Problem": "Unsupported claim that four selected ontologies ‘cover all concepts in NLM database’",
        "Location": "Page 6, Ontology-graph construction (paragraph 2)",
        "Explanation": "The paper asserts that merging MeSH, SNOMED CT, CSP, and AOD covers all concept senses in the NLM database, citing a generic set-cover heuristic. No empirical analysis or evidence is provided to substantiate that these four ontologies alone suffice to index the full breadth of MEDLINE concepts."
      },
      {
        "Problem": "Potential bias from using OHSUMED subset for IDF estimation in TF-IDF re-ranking",
        "Location": "Pages 9–10, Ontology-graph based query expansion scheme, equations (2)–(3)",
        "Explanation": "The TF-IDF re-ranking of candidate CUIs uses IDF values computed from the OHSUMED subset (348,566 documents), which is not representative of the full MEDLINE corpus. This may skew expansion term weights toward the clinical topics prevalent in OHSUMED, reducing generalizability across broader queries."
      },
      {
        "Problem": "No statistical significance testing of observed performance differences",
        "Location": "Pages 16–17, Results section",
        "Explanation": "While the paper reports counts of queries for which G-Bean outperforms PubMed, it does not perform any statistical significance tests (e.g., chi-square, paired t-test) to determine whether the observed differences are unlikely to be due to chance, weakening confidence in the superiority claims."
      }
    ],
    "think_summary": [
      "**Identifying Critical Errors**\n\nI'm focusing on the need to pinpoint critical errors and methodological flaws in the user's paper that could invalidate its conclusions. I'll look for up to five issues, identifying things like subjective evaluations, where students served as evaluators instead of domain experts, and the absence of standard objectivity metrics. Another concern is the biased evaluation environment because they didn't use the optimized PubMed setup. Each of these points will be detailed in JSON entries for clear communication.",
      "**Analyzing Methodological Flaws**\n\nI need to identify several methodological issues in the user's paper. First, selecting the top 500 PPV CUIs for query expansion may introduce noise without a solid cutoff or ablation study. The TF-IDF re-ranking relies on a small subset of MEDLINE, which could bias results. The speed-up claims for parallel indexing seem misleading, and there’s no measurement of overhead. Also, they're overclaiming coverage of concepts with ontology integration, and there's a lack of statistical significance in subjective evaluations. I'll detail these issues for clarity."
    ],
    "token_usage": {
      "input": 16102,
      "thinking": 1024,
      "output": 589
    }
  },
  {
    "entry_id": 228,
    "retraction_id": "2011.03931v2",
    "paper_id": "2011.03931v1",
    "retraction_comment": "There is a error in the experimental EBSD map of albite due to pseudosymmetry. The two parts A and B are actually linked by a 180_ rotation around b-axis. The theory remains valid to my point of view, but the EBSD map of albite cannot be used as an \"experimental proof\". I would like to apologize to the readers of the first versions deposited on Arxiv",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Arbitrary selection of geometric tolerances undermines predictive reliability",
        "Location": "Section 3.1 (p. 12–13) and Section 4.4 (p. 20)",
        "Explanation": "The identification of heteroplanes hinges on fixed thresholds (e.g. Δ‖v‖/‖v‖≤5%, |θ−φ|≤5°, Dmax chosen ad hoc). No rationale or sensitivity analysis is given for these cut-off values. Small changes in these tolerances would admit or exclude entire sets of candidate heterotwins, calling into question the robustness and objectivity of the predictions."
      },
      {
        "Problem": "Lack of energetic or atomic‐scale justification for quasi‐invariant planes",
        "Location": "Section 1.3.1 (p. 6–8) and Section 3.1 (p. 12–13)",
        "Explanation": "The core hypothesis replaces truly invariant twin planes with 'quasi-invariant' heteroplanes, allowing slight intraplanar distortion. However, no energetics, atomistic modeling, or dislocation analysis is presented to show that such distortions and associated interface structures are physically realizable or energetically favorable. Without at least a minimal energetic framework, the existence of these heterotwins remains speculative."
      },
      {
        "Problem": "Neglect of atomic shuffles and space‐group symmetry in transformation twin predictions",
        "Location": "Section 1.3.2 (p. 8–10) and Section 5.3 (p. 31–34)",
        "Explanation": "Transformation twins in NiTi involve not only lattice distortion but also specific atomic shuffles and habit‐plane invariance conditions governed by the space group. The purely reticular approach omits these essential atomic‐level motions. Consequently, the predicted (133)∥(311̅) heterotwin, though supported by TKD misorientation data, lacks verification that the required shuffle patterns and atomic registries can actually occur at the interface."
      },
      {
        "Problem": "Failure to resolve the classical pericline twin discrepancy",
        "Location": "Section 1.2.2 (p. 5–7) and Section 5.1.2–5.1.3 (p. 24–27)",
        "Explanation": "One of the motivations for introducing heteroplanes was the long‐standing mismatch between rhombic‐section predictions and observed pericline twin planes in feldspars. Although the new theory proposes rational heteroplanes for pericline, no new experimental data (e.g. high‐precision striation measurements on (010) faces) are presented to confirm these assignments. The pericline issue remains unaddressed, yet is cited as a central justification for the paradigm shift."
      },
      {
        "Problem": "Possible misclassification of faceted twin morphology as distinct heterotwin mode",
        "Location": "Section 5.2 (p. 28–30)",
        "Explanation": "The paper interprets the 90°⟨010⟩ boundary in Mg micropillars as a b‐axis heterotwin with (100)∥(001) heteroplane. However, such facets have elsewhere been recognized as basal–prismatic segments of conventional {10̅12} extension twins rather than a separate twin mode. Without transmission TEM confirmation of a true crystallographic twin boundary, this morphology may reflect faceting and not a new heterotwin, calling into question its classification."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 69174,
      "thinking": 1216,
      "output": 769
    }
  },
  {
    "entry_id": 229,
    "retraction_id": "1501.02643v7",
    "paper_id": "1501.02643v6",
    "retraction_comment": "This article has been withdrawn due to error in Eq. 8",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Invalid two‐level projection of the s–d exchange interaction",
        "Location": "Section “Effective Hamiltonians”, Eq. (8)",
        "Explanation": "The authors project the full S·σ exchange term onto a two‐level SMM basis and retain only a τzσz coupling in Eq. (8). In reality the SMM’s transverse components (Sx, Sy) have nonzero matrix elements in the low‐lying doublet and would induce off‐diagonal τxσ and τyσ terms. Dropping these generically changes the band topology and can close or shift energy gaps, invalidating the simple mass‐term argument for quantized spin Hall conductivity."
      },
      {
        "Problem": "Use of a continuum Dirac model to obtain half‐integer Chern numbers without regularization",
        "Location": "Section “Spin Hall Effect”, Eqs. (14)–(17)",
        "Explanation": "The half‐quantized Hall conductivity in Eq. (17) arises from integrating the Berry curvature of an unregularized Dirac Hamiltonian over all k. In any lattice realization or with a proper ultraviolet cutoff, physical Chern numbers are integer‐quantized. The continuum calculation thus suffers from the parity anomaly and does not correspond to a measurable half‐integer invariant on a real TI surface."
      },
      {
        "Problem": "Neglect of spatial structure and momentum dependence of the impurity coupling",
        "Location": "Section “Doped Topological Insulator”, Eqs. (6)–(8)",
        "Explanation": "Modeling the SMM as producing a uniform, momentum‐independent mass term λτzσz assumes a perfectly local and homogeneous exchange field across the entire 2D Brillouin zone. A single‐impurity (quantum dot) in reality induces resonant scattering and a localized perturbation, not a global gap opening. This discrepancy undermines the premise of a uniform spin‐Hall gap."
      },
      {
        "Problem": "Omission of Zeeman and orbital magnetic‐field effects on the surface states",
        "Location": "Discussion following Eq. (3) and in building Eq. (8)",
        "Explanation": "Although the orbital Peierls substitution and the σz Zeeman term are mentioned, they are then dropped entirely from the effective Hamiltonian. A perpendicular magnetic field generically produces Landau quantization and modifies the TI surface dispersion and gap structure. Ignoring these effects can qualitatively alter the predicted topological phases and conductivities."
      },
      {
        "Problem": "Ill‐defined pseudo‐spin Chern number giving non‐integer values",
        "Location": "Section “Effects of Tunneling”, Eqs. (23)–(26)",
        "Explanation": "The pseudo‐spin Chern numbers C± in Eq. (26) are expressed as half the difference ρ(∞)–ρ(0), yielding ±1/2 for ∆x/λ≪1. Topological invariants on 2D lattices must be integers. The emergence of half‐integers signals that the pseudo‐spin construction does not define a true Chern insulator phase, calling into question the claimed nontrivial topology in the tunneling regime."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 7720,
      "thinking": 1472,
      "output": 723
    }
  },
  {
    "entry_id": 230,
    "retraction_id": "1903.02670v2",
    "paper_id": "1903.02670v1",
    "retraction_comment": "We can not use fixed-point theorem in the spaces defined in section 4.",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Invalid global well-posedness argument for Theorem 1.5 (IVP (1.1) in H^s for 0<s<1)",
        "Location": "Section 5, Proof of Theorem 1.5 (page 17)",
        "Explanation": "The authors claim global well-posedness in H^s for any s>0 by deriving a priori H^1 estimates. However, for 0<s<1 the initial data φ∈H^s need not satisfy φ′∈L^2, so the H^1 energy estimates in (5.64)–(5.66) are not available. Thus the extension to global existence for s<1 is not justified."
      },
      {
        "Problem": "Unsupported global well-posedness claim in Theorem 1.4 (IVP (1.2) in H^s for s>−1/2)",
        "Location": "Section 5, Proof of Theorem 1.4 (page 17)",
        "Explanation": "Theorem 1.4 asserts global well-posedness for s>−1/2 by referring to the argument of Theorem 1.5. But the latter uses H^1 energy bounds which cannot be transferred to the equation (1.2) at negative or low regularity. No a priori H^s (s>−1/2) control is provided, so global existence remains unproven."
      },
      {
        "Problem": "Incorrect semigroup estimate in Lemma 2.1, equation (2.8)",
        "Location": "Section 2, Lemma 2.1, inequality (2.8) (page 6)",
        "Explanation": "The bound sup_ξ ξ^{2λ} e^{t(−ξ^2+μ/(1+ξ^2)^{1/2})} is claimed to be ≲e^{μT}(λe)^λ t^{-λ}.  In fact one shows sup_ξ ξ^{2λ}e^{-tξ^2}= (λ/(e t))^λ, so the correct multiplier is (λ/e)^λ, not (λe)^λ. This typo propagates into all subsequent time-decay estimates."
      },
      {
        "Problem": "Lack of functional-analytic justification for the nonlocal operator (1−∂_x^2)^{−1/2}",
        "Location": "Introduction, definition of IVP (1.1) (pages 1–2)",
        "Explanation": "The term μ(1−∂_x^2)^{−1/2}u is introduced without specifying its domain or continuity properties on H^s(R).  Although it is a standard Bessel potential, the authors never prove that it defines a bounded map H^s→H^{s+1}, nor address singular integrals on R required in the well-posedness arguments."
      },
      {
        "Problem": "Unjustified convergence result for Theorem 6.2 (μ→0 limit for IVP (1.1))",
        "Location": "Section 6, statement and proof of Theorem 6.2 (page 19)",
        "Explanation": "The authors assert that u_μ→u in C([0,T];H^s) by the same argument as for IVP (1.2).  However, the Duhamel formula and bilinear estimates for (∂_xu)^2 differ from those in Proposition 2.3, and no uniform in μ bound is established in the relevant X^s_T or Z^s_T spaces.  The convergence proof is therefore incomplete."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 19055,
      "thinking": 6656,
      "output": 830
    }
  },
  {
    "entry_id": 231,
    "retraction_id": "1408.0367v2",
    "paper_id": "1408.0367v1",
    "retraction_comment": "This paper has been withdrawn by the author. The paper only considers the omega>k regime, but the undamped modes have omega<k",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Misapplication of flat‐space thermal vacuum polarization to an expanding FRW background",
        "Location": "Section III.B, Eq. 19",
        "Explanation": "The derivation of the effective mass m_A^2=e^2 T^2 N_f/6 and the conductivity σ∼T/e^2 uses Minkowski‐space thermal equilibrium results. In an FRW universe with T∝1/a, the screening scale and dissipation kernel are non‐local in time, and the simple local replacements Π_R→m_A^2 and Π_R→i σ ω are not valid for super‐horizon or time‐varying modes."
      },
      {
        "Problem": "Dimensional inconsistency and misestimation of damping exponent",
        "Location": "Section III.B, Eqs. 21–22",
        "Explanation": "The roots p_{1,2} of p^2+(aσ)p+(k^2+a^2m_A^2)=0 are treated as constants, and p1≈a m_A^2/σ is used to give an exponential damping e^{−p1(η−η_r)}. However, plugging in m_A^2∼e^2T^2 and σ∼T/e^2 leads to p1∼a e^4 T, so that the exponent p1Δη carries dimensions of mass×time and cannot be exponentiated without a missing conversion factor. The claimed damping rate ∼e^{4T/H} is thus dimensionally meaningless."
      },
      {
        "Problem": "Use of a local conductivity term σA' and mass term Π_{ij}A_j in the effective field equation without justification",
        "Location": "Section II, Eq. 3",
        "Explanation": "Equation (3) replaces the retarded vacuum polarization Π_R by local terms σ ∂_tA and Π_{ij}A^j. But in non‐equilibrium quantum field theory the self‐energy is generally non‐local in time (memory integral over past field configurations). The substitution of a frequency‐independent σ and m_A^2 neglects the full ω,k‐dependence of Π_R and is only valid in the strict ω→0 limit."
      },
      {
        "Problem": "Instantaneous matching of mode functions at reheating ignores continuous evolution of plasma properties",
        "Location": "Section III.C, Eqs. 23–26",
        "Explanation": "The paper matches the de Sitter modes directly to radiation‐era modes by enforcing continuity at η=η_r, assuming step‐function behavior in σ and m_A. In reality, reheating is a continuous process during which both the temperature and the plasma response change smoothly. The instantaneous matching overestimates cancellation in the numerator and leads to the spurious conclusion of no growth or decay at reheating."
      },
      {
        "Problem": "Pulling Π_S out of the time integrals in the induced fluctuations calculation",
        "Location": "Section IV, Eq. 30",
        "Explanation": "Equation (30) replaces the double time integral of GR(η,ρ) GR(η,ρ′) Π^T_S(ρ,ρ′) by Π^T_S(η,η) times an algebraic factor. This ignores the non‐local time structure of the source correlator Π_S, which for thermal or quantum fluctuations can vary on timescales comparable to 1/σ or 1/H. The approximation thus misestimates the magnitude and scale‐dependence of the sourced magnetic spectrum."
      }
    ],
    "think_summary": [
      "**Identifying critical issues**\n\nThe user is looking for up to five key problems in a paper regarding magnetic fields, specifically focusing on serious errors. \n\nFirst, there's a possible incorrect inclusion of vacuum polarization terms in the equations. Second, the use of flat-space vacuum polarization may not apply well to an expanding universe, as assumptions about thermal loops and constants might not hold true under variable conditions. \n\nI need to clarify these critical points while ignoring minor issues.",
      "**Evaluating critical problems**\n\nI need to outline several significant issues in the paper. First, it seemingly ignores non-local memory integrals, focusing only on local mass/damping, which could misinterpret thermal polarization. Next, the section with the negative exponent has dimensional inconsistencies, treating it as dimensionless when it should have units.\n\nAlso, their naive estimate of conductivity overlooks numerical factors, leading to incorrect scaling. They treat certain terms as constants while they are actually functions of time, causing inconsistencies. Lastly, their assumptions about scalar fluctuations in the radiation era miss critical time-dependent behaviors and flaws in matching functions.",
      "**Identifying key issues**\n\nI need to isolate the top five critical problems in the paper. \n\nFirst, there's a misapplication of flat-space vacuum polarization results in an expanding universe context, as it ignores how T changes with expansion. \n\nNext, the dimensional consistency of the damping exponent is flawed, leading to nonsensical units.\n\nThen, treating conductivity and vacuum polarization as local misses significant time-dependent effects. \n\nFourth, the matching at reheating is unrealistic, neglecting gradual transitions. \n\nLastly, pulling the vacuum polarization from time integrals overlooks important nonlocal contributions. These issues will be formatted in a structured way for clarity."
    ],
    "token_usage": {
      "input": 6501,
      "thinking": 1408,
      "output": 809
    }
  },
  {
    "entry_id": 232,
    "retraction_id": "1809.01643v2",
    "paper_id": "1809.01643v1",
    "retraction_comment": "Error in proof for efficiency bound. Variation of time variable is not taken into account",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Contradictory treatment assignment assumption undermines propensity score modeling",
        "Location": "Section 2.1, Assumption 2.3",
        "Explanation": "Assumption 2.3 defines treatment D deterministically as an indicator of a thresholded function of X, implying p(X)∈{0,1}. Yet all subsequent identification and estimation rely on a stochastic propensity score p(X)=P(D=1|X) strictly between 0 and 1 (Assumption 2.6 and Lemma 2.1). This internal contradiction invalidates the derivation of the AIPW estimator and its efficiency properties."
      },
      {
        "Problem": "Unrealistic uniform convergence rates for machine learning first stages",
        "Location": "Section 2.2, Assumption 2.8",
        "Explanation": "Assumption 2.8 requires sup-norm (L∞) errors of the ML estimators to be o(N⁻¹/4) and L2‐errors to satisfy product rates o(N⁻¹/2). Most common high-dimensional methods (random forests, boosting, even Lasso without strong sparsity conditions) cannot guarantee such uniform convergence in p≫N settings. Without verifying these rates for any concrete learner, the root-N inference claim in Theorem 1 is unjustified."
      },
      {
        "Problem": "Improper handling of sample splitting in repeated cross-sections",
        "Location": "Section 2.2, Estimation procedure steps 1–4 (around equation (2.3))",
        "Explanation": "The K-fold sample‐splitting scheme pools observations across treatment and time periods, yet the CLT for the AIPW estimator is applied as if {A(Wi,p,γ)} are iid. In repeated cross‐sections, T and D are drawn from different populations; splitting without preserving the time‐strata proportions invalidates the iid argument and may bias the estimator and its variance estimate."
      },
      {
        "Problem": "Biased inference in panel data extension due to omission of sample splitting",
        "Location": "Section 3.1, equation (3.1)",
        "Explanation": "In the panel data setting the author abandons sample splitting and plugs the same data into first‐stage ML estimates and into the second‐stage AIPW formula. This reuse induces overfitting bias in high dimensions that does not vanish at root‐N, so the claimed √N‐consistency and efficiency (by analogy to Theorem 1) do not hold without further correction."
      },
      {
        "Problem": "Empirical instability from extreme inverse‐probability weights ignored",
        "Location": "Section 4, Table 1 and accompanying discussion",
        "Explanation": "The raw IPW and AIPW estimates in Table 1 differ dramatically in sign and magnitude from OLS (e.g. 1991 IPW = +0.06 vs. OLS = –0.93). Such volatility signals near‐zero overlap (p(X)≈1 for treated or p(X)≈0 for controls), leading to infinite or highly variable weights. The paper neither diagnoses overlap violations nor proposes weight trimming or regularization, so the practical reliability of the semiparametric estimator is doubtful."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 19803,
      "thinking": 1600,
      "output": 730
    }
  },
  {
    "entry_id": 233,
    "retraction_id": "1203.2581v2",
    "paper_id": "1203.2581v1",
    "retraction_comment": "This paper has been withdrawn by the author due to a crucial error in equation 2",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Effective model omits the 3D Fe-3dz² Fermi pocket",
        "Location": "Page 2, in the construction of the effective model below Eq. (1)",
        "Explanation": "The first-principles calculation (Fig. 1) shows six Fermi pockets, including a strongly three-dimensional Fe-3dz²–derived pocket near Z. The phenomenological Hamiltonian in Eq. (1) retains only four bands (pockets 1–4) and neglects this 3D pocket entirely. Since the proposed horizontal line nodes arise from interlayer pairing, omitting the pocket with the strongest c-axis dispersion invalidates any conclusions about k_z–dependent gap structure."
      },
      {
        "Problem": "Pairing interaction ansatz is oversimplified and neglects interband/orbital coupling",
        "Location": "Page 3, Eqs. (2a)–(2c) and the paragraph defining φ₁(k), φ₂(k), φ_⊥(k)",
        "Explanation": "The model assumes three separable intraband interactions V₁, V₂, V_⊥ with pre-selected form factors φᵢ(k), and entirely omits any interband or orbital-off-diagonal pairing terms. In multi-orbital iron pnictides, interband scattering and orbital mixing are essential to pairing and gap anisotropy. The exclusion of these channels makes the pairing solutions non-generic and likely unphysical."
      },
      {
        "Problem": "Unjustified mapping of DFT exchange couplings to pairing strengths",
        "Location": "Page 4, paragraph beginning “Finally we have also performed the first-principles total energy calculations…”",
        "Explanation": "The authors extract J₁, J₂, J_⊥ by fitting magnetic energies in a nonmagnetic, heavily hole-doped metal to a Heisenberg model, then assert that these J’s can be used directly as pairing interactions (Vᵢ=Jᵢ). There is no microscopic justification for this substitution. Moreover, the obtained ratio J_⊥/J₁ ≈0.12 is far below the V_⊥/V₁≳0.8 required to produce nodes, contradicting the paper’s own criterion."
      },
      {
        "Problem": "Model parameters fit only Fermi surface topology, ignoring orbital weights and dispersion details",
        "Location": "Page 2, paragraph defining ε_k and hopping parameters tΓ₁, tΓ₂, t₁,X₁, t₂,X₂, μ’s, and b",
        "Explanation": "The tight-binding parameters are tuned solely to reproduce the FS contours at k_z=0, but orbital character and band curvature (velocities) are critical for determining pairing interaction strength and gap anisotropy. Without fitting the full band dispersions and orbital weights, predictions of nodal positions and gap modulation along k_z are unreliable."
      },
      {
        "Problem": "Predicted nodal s-wave requires unrealistically strong interlayer pairing",
        "Location": "Page 3–4, Fig. 4 and accompanying text (mixed Sx²⁺y²+Sz² phase for V_⊥/V₁>0.7)",
        "Explanation": "The horizontal line nodes emerge only in the regime V₂≪V₁ and V_⊥/V₁>0.7. There is no experimental or theoretical support for such a dominant interlayer pairing channel in KFe₂As₂. All known interactions in this compound are strongly two-dimensional, making the required parameter regime physically implausible."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 10247,
      "thinking": 2048,
      "output": 822
    }
  },
  {
    "entry_id": 234,
    "retraction_id": "1912.11842v3",
    "paper_id": "1912.11842v2",
    "retraction_comment": "There is a serious mistake in the section 4 in this paper. The paper concludes that there is new particle production in the system due to the plasma oscillation. However, the fact that the imaginary part of the self-energy will always be zero in the nonrelativistic limit indicates no new particle created",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Unjustified ‘classical limit’ replacement of quantum expectation values with c-number background fields",
        "Location": "Sec. III A 1, Eqs. (72)–(80)",
        "Explanation": "The step hΨ̄Ψi → ψ̄₀ψ₀ (and analogous replacements for Ψ̄ΓΨ, Ψ̄SΨ, etc.) discards all quantum fluctuations without demonstration that these contributions are negligible. There is no self-consistent derivation showing that the chosen c-number background ψ₀(x) satisfies the full many-body Dirac-Maxwell equations at finite temperature, so the subsequent evaluation of the polarization tensor is based on an ad hoc approximation rather than the underlying quantum field theory."
      },
      {
        "Problem": "Background polarization tensor violates gauge invariance (non-transversality)",
        "Location": "Sec. III A 3, Eq. (144) and discussion immediately following",
        "Explanation": "The computed Π^{μν}_{bac}(k) is proportional to g^{μν} only and does not satisfy k_μΠ^{μν}(k)=0. This contradicts the Ward identity required by local U(1) gauge invariance, indicating that the approximation scheme breaks gauge symmetry and renders the dispersion relations physically unreliable."
      },
      {
        "Problem": "Absence of any explicit solution for the background Dirac field ψ₀ and electromagnetic field A_μ",
        "Location": "Sec. II B, Eqs. (19)–(21)",
        "Explanation": "The entire Furry-picture decomposition relies on classical background fields satisfying (iD/–m)ψ₀=0 and ∂_μF^{μν}=eψ̄₀γ^νψ₀, but the paper never exhibits or solves for these ψ₀,A_μ in a plasma context. Without a concrete background configuration, all further perturbative expansions around ψ₀,A_μ lack a well-defined starting point."
      },
      {
        "Problem": "Misinterpretation of vacuum polarization correction as real particle production",
        "Location": "Sec. IV A 1, Eqs. (165)–(167) and accompanying text",
        "Explanation": "The term ω_p^2/(4m^2) in the dispersion relation is attributed to on-shell electron-positron pair production induced by plasma oscillations. In fact, this term arises from virtual loop corrections (effective mass renormalization) and does not correspond to a change in real particle number. Interpreting it as actual pair production is erroneous."
      },
      {
        "Problem": "Unwarranted assumption of separate Fermi-Dirac equilibrium distributions for electrons and positrons in a pair plasma",
        "Location": "Sec. III A 2, Eqs. (128)–(131), (147) and surrounding assumptions",
        "Explanation": "The paper assumes static, equal Fermi-Dirac distributions for electrons and positrons (N⁺=N⁻, j₀=0) without considering annihilation, creation processes, or collision dynamics that in reality couple their distributions. This idealization is incompatible with a thermally equilibrated relativistic pair plasma and invalidates the derived background polarization."
      }
    ],
    "think_summary": [
      "**Identifying critical problems**\n\nThe user has shared a detailed 19-page document on a study involving QED plasmas and background field methods, and they want up to five critical issues presented in a JSON format. I need to focus on identifying logical mistakes, unsupported assumptions, and issues with the methodologies. For instance, one potential problem relates to their classical limit approach which likely overlooks important quantum fluctuations. The classical limit method they're using seems flawed, as it improperly disregards significant correlators in TFD.",
      "**Finding critical errors**\n\nI see several critical issues in the study. First, they use psi0 as a classical field without justification, undermining the calculation of their polarization tensor. This neglects quantum fluctuations, which is significant. Second, their claim about particle production due to plasma oscillation misinterprets effective mass corrections as real production, which isn’t valid. Third, they fail to consider the conditions necessary for pair creation in strong fields. Lastly, they violate gauge invariance with their polarization tensor, which leads to inconsistencies in their approach. These should all be flagged as critical problems.",
      "**Summarizing critical problems**\n\nI’ve identified several significant issues within the user's study. First, they improperly use a classical limit method that equates quantum expectations to classical fields without justification, neglecting crucial quantum fluctuations. Second, the background polarization tensor is not transverse, which violates gauge invariance. Third, their assumption of a background field lacks a self-consistent solution to the relevant equations. Lastly, they misinterpret certain frequency corrections as real particle production. I’ll compile these into a JSON format, detailing each problem, location, and explanation."
    ],
    "token_usage": {
      "input": 29008,
      "thinking": 1344,
      "output": 745
    }
  },
  {
    "entry_id": 235,
    "retraction_id": "2212.07368v2",
    "paper_id": "2212.07368v1",
    "retraction_comment": "There is an error in the use of Corollary 1 in our Paper, which does not apply in our case",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Potential cancellation in summed signal can hide support",
        "Location": "Section II.B and Theorem 1",
        "Explanation": "The uniqueness proof relies on recovering the union support of all channels from the sum x_Σ = Σ_m x_m. However, if two channels have exponentials at the same location with amplitudes that cancel (e.g., equal magnitude and opposite phase), those components vanish in x_Σ, causing U to miss parts of the true support and invalidating both support recovery and the uniqueness claim."
      },
      {
        "Problem": "Assumed known number of exponentials (model order K_Σ)",
        "Location": "Section IV Step 1 (estimating support)",
        "Explanation": "The algorithm requires prior knowledge of the total number of distinct exponentials K_Σ to form the Vandermonde matrix V̂_Σ and recovery matrix Ũ. No procedure is provided to estimate K_Σ; mis-specification of K_Σ leads to an incorrect sensing matrix Â and failure of subsequent recovery steps."
      },
      {
        "Problem": "Non-convex two-step solver lacks convergence or optimality guarantees",
        "Location": "Section IV Shuffled Regression (Eqns 9–11 and Algorithm 1)",
        "Explanation": "The proposed alternating scheme—robust MM-estimation of regression coefficients followed by projection of the permutation indicator—solves a highly non-convex problem. The paper acknowledges that convergence is not guaranteed, yet relies on this heuristic for all experimental results, leaving open the possibility of convergence to suboptimal or incorrect solutions."
      },
      {
        "Problem": "Robust MM-estimator not designed for errors in the regressors (leverage points)",
        "Location": "Section IV Step 2 (Robust regression)",
        "Explanation": "The MM-estimator is chosen to handle outliers in the response vector y but assumes the design matrix Â is accurate. In this setup, errors in spectral estimation introduce leverage points (outlying rows in Â), for which standard robust regression methods may fail, undermining the claimed robustness to regressor errors."
      },
      {
        "Problem": "Missing minimum separation condition for spectral estimation",
        "Location": "Section II.B and Section IV Step 1",
        "Explanation": "While uniqueness of mixture recovery requires N ≥ 2K, practical line-spectral methods (Prony’s method, low-rank denoising) also need a minimum separation between exponential frequencies to avoid ill-conditioning and spurious roots. No separation condition is stated, so the support estimation step can fail for closely spaced or nearly colliding components."
      }
    ],
    "think_summary": [
      "**Identifying critical problems in research**\n\nThe user wants a list of the top 5 critical problems from the paper, formatted as JSON. I’ll review the paper to identify substantial errors or unsoundness that might invalidate its conclusions, discarding minor issues for clarity. \n\nOne potential critical problem could relate to the assumption of a unique permutation structure for multi-channel generalization. Specifically, I’m questioning whether their definition of the binary matrix, Qmm', holds validity when applied to multiple channels. This could be ambiguous—especially when considering how samples are selected from multiple channels at once.",
      "**Assessing critical issues in definitions and methods**\n\nI’m noticing that the definitions in the paper might not accurately capture the shuffling of samples within channels and across time points. It seems to imply full mixing at each sample index, which doesn’t align with the physical scenario.\n\n1. The line spectral estimation methods mentioned assume perfect recovery under ideal conditions but lack analysis for error propagation.\n\n2. The minimization solution is heuristic and non-convex, which raises questions about recovery guarantees.\n\n3. They claim uniqueness leads to practical recovery without addressing noise conditions.\n\n4. The generalization in multi-channel setups depends on a lemma that may not apply due to differences across channels.\n\nThese could be critical issues undermining their conclusions.",
      "**Crafting critical problem entries**\n\nI’m identifying critical problems based on the paper and distilling them into a JSON format as requested. \n\n1. **Cancellation in signal sum hides support**: In Section II.B, Theorem 1, they assume that the summed signal preserves support, but cancellations can lead to lost support.\n\n2. **Model order KΣ assumed known**: In Section IV, Step 1, they suppose KΣ is known for spectral estimation, but in reality, it must be estimated, which can lead to inaccuracies.\n\n3. **Non-convex iterative algorithm lacks convergence guarantee**: In Section IV, they describe a heuristic algorithm with no proof of convergence, risking local minima.\n\n4. **Robust regression not designed for contaminated regressors**: In Section IV, Step 2, while robust regression handles outliers, it doesn't address errors from design matrix contamination.\n\n5. **Spectral estimation requires minimum separation**: In Section II.B and Section IV, they fail to state necessary conditions for unique recovery, which could lead to method failures in closely spaced signals.\n\nNow I’ll prepare this as a JSON list."
    ],
    "token_usage": {
      "input": 19487,
      "thinking": 2176,
      "output": 599
    }
  },
  {
    "entry_id": 236,
    "retraction_id": "1104.1920v2",
    "paper_id": "1104.1920v1",
    "retraction_comment": "This paper has been withdrawn by the author due to a crucial error in equation 42, 50, 51",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Misinterpretation of purification and loose ‘upper bound’ critique",
        "Location": "Section II, pp. 2–3 (paragraph immediately before Eq. (1))",
        "Explanation": "The authors argue that because the practical Alice–Bob state ρ_AB is mixed, Eve cannot in general hold its exact purification, and so the standard Holevo‐based bound is not tight.  This misunderstands the security model: by definition in a collective attack Eve controls all excess noise and can be assumed to hold a purification of the global Alice–Bob–environment state.  There is no ‘imperfection’ that prevents Eve from purifying ρ_AB.  As a result, the entire motivation for re-deriving a tighter bound in Sec. III is flawed."
      },
      {
        "Problem": "No proof that Gaussian attacks are optimal",
        "Location": "Section III (derivation of new bounds)",
        "Explanation": "All of the new bounds are derived by restricting Eve’s operation to arbitrary symplectic (i.e. Gaussian) transformations.  The paper never proves that non‐Gaussian collective attacks cannot give Eve strictly more information.  Without invoking the known Gaussian optimality theorems (and checking the required conditions), the claim that these bounds are 'tight' against all collective attacks is unjustified."
      },
      {
        "Problem": "Claim that Eve’s Holevo information in reverse reconciliation is independent of Alice–Bob correlation",
        "Location": "Section III, end of derivation (around Eqs. (33)–(34))",
        "Explanation": "The authors find that their new χ_BE depends only on channel parameters T, χ_line and on V, but is independent of the Alice–Bob entanglement parameter α.  Physically, Eve’s maximal accessible information must depend on how strongly Alice and Bob are correlated.  The apparent α-independence indicates a mistake in the symplectic eigenvalue algebra or an overly restrictive attack model that decouples Eve from Alice’s mode—undermining the claimed optimality."
      },
      {
        "Problem": "Algebraic inconsistencies in symplectic eigenvalue formulas",
        "Location": "Eqs. (31)–(34) (and similarly Eqs. (39)–(41))",
        "Explanation": "The covariance‐matrix entries and resulting invariants Δ and D are extremely intricate, but no verification (e.g. limiting‐case checks beyond Δ→1 when α→1) is provided.  Small sign errors or dropped cross terms in Eqs. (31)–(34) would completely change the eigenvalues and thus the Holevo bound.  Without an independent consistency check (e.g. numerical diagonalization of a sample covariance), these critical expressions cannot be trusted."
      },
      {
        "Problem": "Security model incomplete—no finite-size or composability analysis",
        "Location": "Conclusion (Section IV)",
        "Explanation": "The paper claims ‘tight’ unconditional security bounds but addresses only the asymptotic key rate (I(A:B)−χ).  Modern CV-QKD security requires composable proofs and finite-size analyses.  Ignoring finite-statistics effects and composable‐security definitions means the results cannot be considered complete or operational for real implementations."
      }
    ],
    "think_summary": [
      "**Analyzing CV-QKD Paper Flaws**\n\nI’m looking to identify critical errors that could undermine conclusions in this CV-QKD security paper. First, there seems to be a misunderstanding of purification—specifically about Eve purifying Alice and Bob's states. Their assumption that Alice and Bob share a mixed state is flawed, as under collective attacks, purification occurs. Additionally, I need to verify the correctness of their equations, particularly equation (11). It may have sign issues that could change the outcomes of their arguments.",
      "**Identifying Errors in Equations**\n\nIn equation 11, I’m noting potential issues with the expression for χ. The authors derived a new bound using symplectic forms but may have assumed incorrect dimensions for Eve and Bob's modes. Their independence claim regarding χ_BE relies on Gaussian attacks, possibly neglecting non-Gaussian scenarios. They assert that their bounds are tight without proving optimality against non-Gaussian attacks and don't address errors in the appendix. Additionally, their treatment of reconciliation efficiency and finite-size effects raises further concerns. I need to investigate these inaccuracies.",
      "**Identifying Key Issues**\n\nThey claim that Eve's information is independent of the Alice-Bob correlation, which is counter-intuitive—strong correlations should limit Eve’s information. I need to break down five critical problems into JSON format. \n\nThe first problem is a misunderstanding of mixed state purification; they incorrectly assert that it can’t be purified. Second, they lack proof of Gaussian optimality, failing to show non-Gaussian attacks won’t give Eve more information. Third, their conclusion about χ_BE's independence from Alice-Bob correlation contradicts established understanding. \n\nOther potential issues include algebra mistakes in symplectic expressions and incomplete security models. I’ll compile these findings into JSON."
    ],
    "token_usage": {
      "input": 23989,
      "thinking": 1216,
      "output": 762
    }
  },
  {
    "entry_id": 237,
    "retraction_id": "1406.6450v2",
    "paper_id": "1406.6450v1",
    "retraction_comment": "This paper has been withdrawn by the authors due to a gap in the inequality of (2.7)",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Misapplication of the subnormal backward‐extension criterion (Lemma 2.2) in proving part (ii)",
        "Location": "Section 2, proof of Theorem 1.1, paragraph beginning “Write N ≡ …” and equation (2.7)",
        "Explanation": "Lemma 2.2 is a necessary and sufficient test for extending a subnormal shift from the subspace k₂≥1 to the full 2-variable shift along the k₂-direction.  In part (ii) the authors attempt an analogous ‘horizontal’ extension along the k₁-direction by defining N={k₁≥1,k₂≥0}, but instead disintegrate the Berger measure of M∩N={k₁≥1,k₂≥1}.  This does not match the hypotheses of Lemma 2.2, so their computation of the critical ε≤5/12 bound for subnormality of (T₁,T₂)|_N is invalid."
      },
      {
        "Problem": "Invalid application of Agler’s subnormality criterion",
        "Location": "Section 2, proof of Theorem 1.1, paragraph starting “For the subnormality of T₁+T₂, we shall use Agler’s criterion …”",
        "Explanation": "Agler’s criterion characterizes subnormality only for a contraction S (i.e. ∥S∥≤1).  The authors apply it directly to (T₁+T₂)/2 (denoted ‘T₁+T₂₂’ in the text) without verifying that ∥(T₁+T₂)/2∥≤1.  Without establishing the contraction hypothesis, the positivity of the Pₙ moments is not equivalent to subnormality."
      },
      {
        "Problem": "Undefined and ambiguous notation in the contraction scaling",
        "Location": "Section 2, proof of Theorem 1.1, immediately following equation (2.7)",
        "Explanation": "The manuscript introduces the symbol “T₁+T₂₂” (and later ‘(T₁+T₂)²’) without ever defining what the subscript ‘2’ denotes.  It appears they intend (T₁+T₂)/2 to apply Agler’s theorem, but this is never stated, rendering the subsequent moment‐sequence derivation and use of Agler’s test unclear and unsupported."
      },
      {
        "Problem": "Pointwise support claims in the disintegration are unjustified",
        "Location": "Section 2, proof of Theorem 1.1, uses of Lemma 2.5 in parts (ii) and (iii)",
        "Explanation": "After invoking the existence of a disintegration x↦λₓ of the Berger measure, the authors assert supp λₓ={x}×Y (and similarly supp λ_y=X×{y}) pointwise.  However, disintegration measures are only defined up to sets of marginal‐measure zero, and no argument is given that these support assertions hold except on a null set.  This undermines the precise computation of integrals like ∫1/t dμ and the extremal‐measure comparisons."
      },
      {
        "Problem": "Lack of rigorous verification of the L¹‐integrability conditions in Lemma 2.2",
        "Location": "Section 2, application of Lemma 2.2 conditions (i) and (ii) in parts (ii) and (iii)",
        "Explanation": "Lemma 2.2 requires checking (i) that 1/t∈L¹(μ_M) and (ii) the bound β₀₀²≤(∫(1/t)dμ_M)^{-1}.  While the authors compute formal values for these integrals, they do not justify convergence of ∫1/t dμ_M near t=0, nor do they rigorously compare the resulting bound with the computed β₀₀².  Without these verifications, the necessity and sufficiency of their ε‐bounds is unestablished."
      }
    ],
    "think_summary": [
      "**Analyzing paper for errors**\n\nThe user wants me to check the paper for critical errors and unsoundness that could invalidate the conclusions. I'll focus on identifying the five most significant problems as JSON entries, specifically looking for issues like incorrect usage of terminology, missing measures, calculation errors, and logical gaps.\n\nThe main result discusses a theorem related to commuting pairs of subnormals with specific conditions based on a parameter epsilon. I'll review these details closely to find critical flaws that impact the conclusions.",
      "**Examining lemmas and proofs**\n\nI need to closely review the lemmas used in the proof and identify potential issues, especially with Lemma 2.2 for hyponormality and Lemma 2.3 for subnormal backward extension.\n\nOne particular concern is how they apply these lemmas—specifically, checking if W(α,β)|M or |N are indeed subnormal. It seems they might have incorrectly defined M∩N while asserting subnormality. They need to apply Lemma 2.2 accurately and ensure they're using the Berger measure for the right subspace instead of a combination of measures, which could lead to invalid conclusions.",
      "**Identifying critical errors**\n\nIn Section 2, proof of Theorem 1.1, there are ambiguities that undermine the calculations. For instance, the term \"(T1+T2)_2\" is introduced without definition, raising questions about its meaning, which could invalidate later moment calculations.\n\nAdditionally, there's an implicit assumption about pointwise disintegration in parts (ii) and (iii). The authors assert pointwise support properties without rigorous justification, possibly invalidating measure calculations.\n\nFinally, they fail to verify the L^1 integrability conditions needed for Lemma 2.2, which might affect the subnormal extension criteria. I can assemble these findings into a JSON array."
    ],
    "token_usage": {
      "input": 11679,
      "thinking": 3136,
      "output": 913
    }
  },
  {
    "entry_id": 238,
    "retraction_id": "1510.05979v4",
    "paper_id": "1510.05979v3",
    "retraction_comment": "This paper has been withdrawn by the author due to a crucial error in equation (39)",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Ill-defined integral in the continuous equation due to non-integrable singularity",
        "Location": "Section 2, equations (28) and (SC)",
        "Explanation": "The kernel ϕ(w)=−σw/|w|^{2+σ} yields an integrand of order |s−r|^{−1−σ} near r→s, which diverges for σ>0. The paper does not introduce any principal-value regularization or otherwise handle this non-integrable self-interaction, so the integro-differential equation (SC) is not well defined."
      },
      {
        "Problem": "Incorrect coercivity estimate in the variational existence proof",
        "Location": "Section 3.1, equations (44)–(45)",
        "Explanation": "They claim ∫₀¹|y(s)−y(0)|^{−σ}ds≥C‖y‖_{H¹}^{σ/2}, which is dimensionally and analytically wrong. The correct inequality is ∫|y(s)−y(0)|^{−σ}ds≥C‖y‖_{H¹}^{−σ}, so their conclusion that Aσ(y)→∞ as ‖y‖→0 fails. The functional may not be coercive as stated."
      },
      {
        "Problem": "Minimizer may admit self-intersections (collisions) without infinite action",
        "Location": "Section 3.1, existence of minimizer of Aσ on Λ",
        "Explanation": "For σ<1, an isolated collision y(s₁)=y(s₂) still yields a locally integrable singularity ∼|s−s₀|^{−σ}, so the action remains finite. The paper does not forbid such self-intersections nor prove the minimizer is injective or collision-free, hence it may not satisfy the Euler-Lagrange equation or represent a ‘continuous choreography’."
      },
      {
        "Problem": "Unjustified derivation of Euler-Lagrange equation for the continuous functional",
        "Location": "Section 2.1, equations (32)–(36)",
        "Explanation": "The first variation is computed by interchanging differentiation and a singular integral without verifying conditions for differentiation under the integral sign. Given the non-integrable kernel in the force term, the functional is not C¹ on Λ, and the derivation of equation (SC) is invalid."
      },
      {
        "Problem": "Lack of rigorous justification for the N→∞ passage from discrete to continuous system",
        "Location": "Section 2, transition from equation (27) to (28)",
        "Explanation": "The authors replace a singular Riemann sum by an integral as N→∞ without controlling convergence or error, especially in the presence of the singular interaction kernel. There is no proof that discrete choreographies converge to solutions of the proposed continuous equation."
      }
    ],
    "think_summary": [
      "**Analyzing integral and coercivity**\n\nI'm noticing that there's a significant sign issue with the integral expressed as U_C(y). It should use the form with a negative exponent. Also, in equation (43), they incorrectly state part of the integral, which leads to mistakes in deriving bounds. Their coercivity argument is flawed because it suggests that as ν approaches zero, A approaches infinity, but it's actually tending toward zero. This typo and inversion seem to undermine their claims about coercivity. It's crucial to catch these errors!",
      "**Examining minimizers and variations**\n\nI'm analyzing how collisions affect action minimization. Circles aren't minimizers since variations can decrease action, raising questions about their validity. When checking if the variation satisfies certain conditions, it seems fine. However, the speed and variations need examination; speed is determined explicitly by solutions. They find a minimizer but haven't ruled out circles as solutions. Additional issues arise—like ignoring the need for principal value integration in the Euler-Lagrange equation. They also make assumptions during the limit transitions that lack justification."
    ],
    "token_usage": {
      "input": 10011,
      "thinking": 3968,
      "output": 672
    }
  },
  {
    "entry_id": 239,
    "retraction_id": "1202.0569v2",
    "paper_id": "1202.0569v1",
    "retraction_comment": "Due to a flaw in Lemma 9, the paper has been withdrawn",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Unjustified simplification of circulation paths and missing ‘‘revocation’’ for the general case",
        "Location": "Page 4, paragraph beginning “Observe that the conditions…”",
        "Explanation": "The proof of Theorem 4 replaces each subpath of C between degree-3 vertices by a single edge to define the auxiliary graph K, promising later to “revoke” this simplification. However, no construction or argument is ever given to lift the matching-based augmenting-set selection back to the original graph with possibly long paths. Without this, the existence of a valid C-augmenting set A in the true graph is unproven."
      },
      {
        "Problem": "Incorrect or incomplete proof of the existence and correctness of the matching M in Lemma 6",
        "Location": "Page 5–7, Lemma 6 and its proof",
        "Explanation": "The construction of a nonempty matching M with deg_M(U)∈{0,2} for every class U relies on a convoluted alternating-path and ‘‘twist’’-elimination algorithm. The paper omits key invariants, a termination argument, and rigorous verification that every step preserves the matching property and eventually succeeds. Critical cases (e.g. multiple nested twists) are only sketched, leaving the existence of such an M unestablished."
      },
      {
        "Problem": "Gap in the bridge-free refinement argument of Lemma 9 and potential non-termination",
        "Location": "Page 11, Lemma 9 and its algorithmic Steps 1–4",
        "Explanation": "The recursive process alternately applies Lemma 7 and Lemma 8 to refine A until G–A has no bridges and A remains C-augmenting. No formal measure is given to show that each iteration strictly reduces a well-founded complexity parameter without destroying the C-augmenting property. In particular, replacing Ai by a strict subset A0i in Step 2 may invalidate the current circulation Ci, and the loop invariants are neither stated nor proven, so termination and correctness are in doubt."
      },
      {
        "Problem": "Unproven assembly of the final cycle double cover in the inductive step",
        "Location": "Page 12, proof of Theorem 2, final paragraph",
        "Explanation": "The construction defines Q=A∪E(P) and Q0=A∪E(P′) and then sets D=(D0\\C0)∪(Q∪Q0). It is claimed that Q and Q0 are circulations covering each edge exactly twice. Yet no argument ensures that Q0’s cycles are edge-simple or that cycles from D0\\C0 together with Q∪Q0 yield exactly two coverings per edge without conflicts or overlaps. This essential coverage property remains unverified."
      },
      {
        "Problem": "Overlooked interaction between multiple augmenting paths in Lemma 7",
        "Location": "Page 9, Lemma 7 second case",
        "Explanation": "When deg_M(U)=2 for several classes U, the proof adds for each U a path P_{uu'} inside the same component H to form a larger circulation. It is assumed these paths are pairwise edge- and vertex-disjoint, but no argument rules out their mutual intersections across different components or interference with existing cycles, risking creation of bridges or violating the circulation property."
      }
    ],
    "think_summary": [
      "**Analyzing the paper's flaws**\n\nThe user wants me to check a paper for critical errors that may invalidate its conclusions. They specifically request identifying up to five major problems in JSON format, relating to a constructive proof of the cycle double cover conjecture, which is an unresolved issue. I’ll need to focus on logical gaps, especially around Theorem 4 being a direct outcome of Lemma 7 and 9, and examine potential issues in the matching algorithm and the core approach used in the proof.",
      "**Identifying circulation issues**\n\nI see potential problems in defining \"circulation.\" It's described as a set of vertex-disjoint cycles, but it seems they mean edge-disjoint cycles. While edge-disjoint cycles can share vertices, they claim all cycles in C are vertex-disjoint due to a maximum degree in G being 3. This assumption appears flawed; cycles could share vertices at degree-3 points. If two cycles intersect at such a vertex, they can't be both edge-simple and edge-disjoint, raising questions about the algorithm's validity in Lemma 6.",
      "**Highlighting critical flaws**\n\nI've identified some major issues. First, cycles can't share a vertex if the maximum degree is 3, which confirms that edge-disjoint cycles should stay that way. In Lemma 6, the matching algorithm's termination is questionable. Furthermore, when paths are simplified to single edges in Lemma 7 without clear revoking details, it raises concerns. There are also ambiguities around how edges in the augmenting set relate to existing cycles, and especially whether paths internal to components maintain disjointness. These aspects need clearer explanation and validation to ensure correctness."
    ],
    "token_usage": {
      "input": 19006,
      "thinking": 2240,
      "output": 759
    }
  },
  {
    "entry_id": 240,
    "retraction_id": "2111.02352v2",
    "paper_id": "2111.02352v1",
    "retraction_comment": "The previous to the last sentence of Section 4, namely that \"This means that $\\hat{Q}$ and, by Lemma 6, $\\hat{Q}$ too, is less than 1.\" is wrong",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Treating the number of vertices l as a constant",
        "Location": "Section 2.1, paragraph 1",
        "Explanation": "The authors state “G = (V,E) with l vertices and m edges (both l and m are considered constants)”. In fact l grows with the input graph. Many subsequent probability bounds (e.g., Lemma 2) and the claim of constant expected running time depend critically on l being fixed; when l→∞ those probabilities decay to zero and the running time guarantee fails."
      },
      {
        "Problem": "Incorrect claim of independent positive probability for a random specially proper coloring",
        "Location": "Section 3, Lemma 2",
        "Explanation": "The proof asserts that since each vertex avoids at most Δ+⌈αΔ^{4/3}⌉ colors independently, the overall probability of an α-specially proper coloring is positive. However, the events for different vertices are not independent, and the product of probabilities over l vertices tends to zero as l grows, invalidating the claim."
      },
      {
        "Problem": "Invalid uniformity assumption in ValidationAlg across phases",
        "Location": "Section 4, Lemma 5",
        "Explanation": "Lemma 5 asserts that after each phase of ValidationAlg, the vertices’ colors are independent uniform draws from the palette. In reality, conditioning on previous bad-event checks and partial recolorings induces complex dependencies that are not erased by recoloring only sc(B). Thus the uniformity claim is unfounded."
      },
      {
        "Problem": "Asymptotic generating-function analysis lacks a finite Δα",
        "Location": "Section 4, Lemma 8 and below",
        "Explanation": "The bound φ′(τ)<1 ensuring exponential decay is shown only in the limit Δ→∞, yielding the condition α>2^{−1/3}. No explicit finite Δα(α) is provided, so it remains unclear for which actual Δ the claimed bound holds, undermining the constructive statement of Theorem 1."
      }
    ],
    "think_summary": [
      "**Analyzing critical problems**\n\nThe user wants me to identify up to five critical problems in a paper, focusing on significant errors that could undermine its conclusions. I'll analyze Theorem 1, which claims to improve the bound for the acyclic chromatic number. This involves examining the argument that the new bound is better than the prior one. I need to consider the assumptions made, such as the validity of special random coloring and the handling of specific cycles, to ensure the argument's robustness.",
      "**Evaluating Lemma 2 issues**\n\nI'm examining Lemma 2, which discusses the probability of random coloring being α-specially proper. They state independence among vertices, but I notice the events are not truly independent, which is a potential flaw. They claim l and m are constants, but l, as the number of vertices, should vary. Additionally, they argue that the probability remains positive as l increases, which seems incorrect since it will likely approach zero. I'll need to outline these critical points for clarity.",
      "**Assessing special 4-cycles**\n\nI'm evaluating how the algorithm handles special 4-cycles. It doesn't recolor them when pairs are special, which might leave some unresolved bichromatic cycles unless the initial coloring is specially proper. They rely on this initial condition for their handling, but the main algorithm only checks for properness at the end. I’m considering whether the restrictions around special pairs effectively eliminate these cycles. Ultimately, I think I’ll identify four significant issues for the user and prepare them in a proper JSON format."
    ],
    "token_usage": {
      "input": 9779,
      "thinking": 2624,
      "output": 473
    }
  },
  {
    "entry_id": 241,
    "retraction_id": "2207.08480v2",
    "paper_id": "2207.08480v1",
    "retraction_comment": "Errors in the method to determine the S-transformation coefficients, because the characters for the surfaces are not simply the product of characters for cylinders. Errors in the linear spaces assigned to surfaces in string vertices this http URL concrete definition for open sector linear spaces also required",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Heuristic identification of S-transformation with f(z)→i f(z) time–evolution change",
        "Location": "Section 2.1.3 (and 2.2.3)",
        "Explanation": "The paper equates rotating the time–evolution vector field f(z) by an overall factor i with the global modular S on arbitrary punctured or bordered surfaces, but provides only a cylinder–domain check. There is no general proof that f→i f implements the correct mapping–class group action on higher-genus or multi-punctured Riemann surfaces, nor that it intertwines the full space of conformal blocks as required for S-duality. Without such a demonstration, the identification remains an ungrounded conjecture."
      },
      {
        "Problem": "Tautological construction of open-sector linear spaces via dimension matching",
        "Location": "Section 3.1.2, eq. (43)",
        "Explanation": "Open-sector spaces V_{μ₃ μ₁ μ₂} are declared isomorphic to the closed-sector fusion spaces V_{P₃ P₁ P₂} solely on the basis of matching dimensions n_{ω(P₂)}=N_{P₃ P₁ P₂}. No explicit construction of the isomorphism—or of the boundary-condition changing operators realizing it—is given. This circular definition lifts the fusion rules into the hypothesis, rather than deriving them, undermining the explanatory content of the S-map."
      },
      {
        "Problem": "Overly restrictive ‘boundary-condition completeness’ assumption",
        "Location": "Section 3.2",
        "Explanation": "The central condition that admissible boundary conditions μ form a one-to-one (bijective) map onto bulk labels P is assumed for general CFT₂. In practice, only diagonal RCFTs with Cardy states satisfy this completeness; most CFTs (e.g. non-rational or non-diagonal) admit boundary labels that do not match the bulk spectrum. The requirement therefore excludes large classes of theories and is neither necessary nor natural in a general modular setting."
      },
      {
        "Problem": "Unjustified factorization of S-transformation over tensor products",
        "Location": "Section 3.4.1, eq. (83)",
        "Explanation": "The authors assume S[⊗_i A_i] = ⊗_i S[A_i] when sewing surfaces, so that multi-point S-transformations factorize into products of basic ones. No argument is given that modular S commutes with the sewing (tensor) operations in their linear-space formalism. In general, mapping–class group actions and nodal degenerations do not decompose multiplicatively in this fashion, so the factorization property is unproven and likely invalid."
      },
      {
        "Problem": "Neglect of anti-holomorphic sector and potential holomorphic–antiholomorphic mixing",
        "Location": "Section 3.2 ‘The role of anti-holomorphic sector spaces’",
        "Explanation": "Although full modular S acts simultaneously on holomorphic and anti-holomorphic conformal blocks, the definition works entirely in the holomorphic sector and appeals only abstractly to an isomorphism with the anti-holomorphic labels. For non-diagonal theories or those with nontrivial conjugation isomorphisms, S generally mixes holomorphic and anti-holomorphic data. By ignoring this coupling, the construction may fail to capture necessary consistency conditions of the full CFT₂ modular invariance."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 34788,
      "thinking": 1280,
      "output": 798
    }
  },
  {
    "entry_id": 242,
    "retraction_id": "2302.04323v3",
    "paper_id": "2302.04323v2",
    "retraction_comment": "The first statement on page 9 is not necessarily true. Roughly speaking, the problem is that the indices \"i_s\" and \"r\" are competing with each other and therefore what I believed to be immediate, as happens naturally in the case of a single index, and as can be seen in the proof of Theorem 6.7 of the FHHMZ reference, is in fact not immediate in the situation where double indices are involved",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Conflation of the c0‐quotient space [X]=ℓ∞(X)/c0(X) with an ultrapower (X)U and invalid use of ultrafilter‐based lemmas in [X]",
        "Location": "Section 2 (pp. 3–5), Lemmas 2.1–2.2 and the construction of [X]",
        "Explanation": "The authors replace the usual ultrapower XU (built via a free ultrafilter) by the quotient ℓ∞(X)/c0(X) and then apply Goebel–Karlovitz (Lemma 2.1) and Lin’s ultrapower Lemma 2.2 as though they held in this quotient. But those results rely on ultrafilter limits and do not generally transfer to the c0‐quotient. As stated, Lemma 2.2 cannot be applied to [X], invalidating the core fixed‐point/nonexpansive‐mapping argument."
      },
      {
        "Problem": "Invalid finite‐representability claims for the c0‐quotient [X]",
        "Location": "Propositions 3.5–3.6 (p. 12)",
        "Explanation": "The paper asserts that the quotient space [X]=ℓ∞(X)/c0(X) is finitely representable in X (Prop. 3.5) and uses this to characterize B-convexity via spreading models in [X] (Prop. 3.6). Those classical finite‐representability results apply to ultrapowers XU, not to the c0‐quotient. This mix-up breaks the chain of implications linking failure of FPP to the existence of an ℓ1-spreading model."
      },
      {
        "Problem": "False claim that C[0,1] admits a monotone Schauder basis",
        "Location": "Theorem 3.9 (p. 12)",
        "Explanation": "Theorem 3.9 embeds a separable B-convex X isometrically into C[0,1] and invokes a ‘monotone’ basis there to get a 1-EAB. But the standard Faber–Schauder basis of C[0,1] is not monotone (its partial‐sum projections have norm >1) and no known Schauder basis for C[0,1] is monotone. Thus the hypothesis of a 1-EAB in C[0,1] fails."
      },
      {
        "Problem": "Overstatement of fixed‐point conclusions: proving only weak‐FPP but claiming full FPP",
        "Location": "Theorem 3.10 (p. 13)",
        "Explanation": "All of the paper’s main arguments establish only the weak fixed‐point property (for weakly compact convex sets). Yet Theorem 3.10 asserts that super-reflexive spaces have the full fixed‐point property (for all closed bounded convex sets). No extension from weak‐FPP to FPP is provided, so the final claim remains unsupported."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 15173,
      "thinking": 2368,
      "output": 687
    }
  },
  {
    "entry_id": 243,
    "retraction_id": "1911.03748v2",
    "paper_id": "1911.03748v1",
    "retraction_comment": "Unfortunately, our proof contains a serious flaw. Specifically, Lemma 5.3 does not prove the assertion it claims to prove and this collapses the entire argument. We thank [REDACTED-NAME] for pointing out the flaw, and apologize to the community for posting an eventually incorrect proof",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Existence‐vs‐branching mismatch: the algorithm never enforces the “good” assignment z_J when it actually branches",
        "Location": "Proposition 5.4 → Lemma 6.1 (construction of the decision tree)",
        "Explanation": "In Proposition 5.4/Lemma 6.1 one uses Proposition 5.4 to find a small set J and a specific assignment z_J such that f(z_J,0) has high variance.  But in the actual decision‐tree algorithm (Lemma 6.1, Stage 3) one merely queries the variables in J and then continues with the branch corresponding to the *actual* input x_J, which almost never equals z_J.  All subsequent analysis of variance‐drop and block‐sensitivity‐drop implicitly assumes the “beneficial” assignment z_J was taken, even though the tree equally explores *all* assignments.  This conflation of “exists z_J” with “the algorithm follows z_J” is fatal: the algorithm never privileges that branch, so none of the promised variance reductions or depth‐bounds hold."
      },
      {
        "Problem": "Overstated per‐step variance (or bs) decrement ignores tiny matching probability",
        "Location": "Lemma 6.1, super‐martingale argument following displayed inequality (8)",
        "Explanation": "The proof treats the existence of an assignment z_J that would reduce block‐sensitivity by ≥√ε/4 as if *every* step yields that drop.  In reality, only on the tiny fraction of inputs with x_J = z_J does one see that drop; on other branches bs may not decrease at all.  Thus the claimed average drop per iteration is off by a factor ≃2^{–|J|}, and the ensuing bound E[I]≤O(d^2/√ε) collapses to an exponential in |J|, destroying the polynomial‐depth guarantee."
      },
      {
        "Problem": "Misapplication of the Optional Stopping Theorem without checking boundedness or integrability",
        "Location": "Lemma 5.2 and Lemma 6.1 (use of super‐martingale E[X_T]≤X_0 to bound E[I])",
        "Explanation": "Both iterative constructions invoke Doob’s optional stopping theorem on a super‐martingale indexed by the random number of iterations I.  They do not verify the standard sufficient conditions (e.g. bounded stopping time, bounded increments, or L^1‐integrability) required to justify E[X_T]≤X_0.  In particular, I may be unbounded and the martingale increments depend on heavy‐tailed events, invalidating the stopping‐time argument."
      },
      {
        "Problem": "Incorrect parameter choice in Lemma 5.3 when invoking Lemma 5.2",
        "Location": "Lemma 5.3, application of Lemma 5.2 with W_{>k}(h_S)≤2^{–(k+1)}",
        "Explanation": "They fix k so that 2^{–(k+1)}≤ε^{2/3} and then apply Lemma 5.2, which only guarantees k h_S–g k_2^2≤3·W_{>k}(h_S)=3ε^{2/3}.  Yet they assert this ≤ε/2.  But 3ε^{2/3}≤ε/2 requires ε≥(6)^{–3} and fails for small ε.  Hence the promised accuracy bound k h_S–g k_2^2≤ε/2 does not follow."
      },
      {
        "Problem": "Hypercontractivity step in Proposition 4.5 uses 3^d instead of the correct 3^k",
        "Location": "Proposition 4.5 (proof of the degree‐d junta approximation)",
        "Explanation": "They claim ||f''||_2^2≤3^d||T_{1/√3}f''||_2^2, but since f'' has no Fourier mass below level k only, one actually has ||f''||_2^2=3^{k+1}||T_{1/√3}f''||_2^2.  Replacing 3^{k+1} by 3^d grossly weakens the bound (and is inconsistent with d≫k), invalidating the claimed tail‐bound and hence the junta size estimate."
      }
    ],
    "think_summary": [],
    "token_usage": {
      "input": 16962,
      "thinking": 7040,
      "output": 985
    }
  },
  {
    "entry_id": 244,
    "retraction_id": "2212.12846v2",
    "paper_id": "2212.12846v1",
    "retraction_comment": "We found an error in Lemma 3.5.--which is used in the subsequent analysis to establish the rate of convergence. Since the error is not fixable, we would like to withdraw the article",
    "checker_model": "o4-mini",
    "attempt_id": 0,
    "problems": [
      {
        "Problem": "Invalid maximum principle (Lemma 3.3)",
        "Location": "Section 3.2, Lemma 3.3",
        "Explanation": "The proof assumes σ(u)=η(u,z)=0 for |u|>M so that the stochastic terms vanish outside [–M,M], and then argues via a convex test-function that |uΔx|≤M_f. However, the deterministic flux f and diffusion Φ are not assumed to vanish outside [–M,M], so these drift terms can still drive the numerical solution beyond the proposed bound. Without further constraints on f and Φ at large |u|, the discrete maximum principle fails, invalidating subsequent BV and L∞ estimates."
      },
      {
        "Problem": "Lack of uniform control in mollification parameter κ when passing limits in stochastic integral term J6",
        "Location": "Section 4, Lemma 4.8 (estimates of J6,1–J6,4)",
        "Explanation": "The bounds for J6,i are stated as J6,1,J6,2≤C(ξ,κ)δ0^1/4 and J6,3≤C(ξ,κ)δ0^a, with constants C depending on the convolution parameter κ. To send δ0→0 then κ→0, one needs C(ξ,κ) uniformly bounded as κ→0, but no such uniformity is established. As κ→0 the convolution derivatives ∂xx(Φ(uε)*τρ) may blow up, so the limit δ0→0 cannot be interchanged with κ→0, undermining the consistency of the Itô–Lévy term estimates."
      },
      {
        "Problem": "Unjustified Taylor expansions requiring bounds on third derivatives of the entropy approximation βξ",
        "Location": "Section 4, in the estimates of Jˆ4 terms (e.g. Jˆ4,4 and Jˆ4,5)",
        "Explanation": "The analysis expands nonlocal terms via third‐order Taylor remainders and invokes bounds on βξ''' of order O(1/ξ^2). Yet the paper only provides a uniform bound on βξ'' and never specifies a bound on βξ'''. This missing estimate compromises all error bounds relying on third‐order remainders in the doubling‐of‐variables argument."
      },
      {
        "Problem": "Ambiguous order of limits for (δ0, κ, l) without uniform estimates",
        "Location": "Section 4, at the beginning of the doubling‐variables analysis",
        "Explanation": "Throughout Section 4 the authors send δ0→0, then κ→0, then l→0, but do not provide uniform integrability or equicontinuity arguments to justify this order. Key quantities depend on all three parameters, and without establishing estimates independent of the earlier parameters, the successive limit passages are not rigorously justified and may fail to commute."
      },
      {
        "Problem": "Unverified regularization error control for uε via convolution",
        "Location": "Section 4, equation (4.1) and subsequent use of uκε",
        "Explanation": "The proof introduces the mollified viscous solution uκε=uε*τκ, claims ∂xxuκε∈L^2 uniformly in κ, and then drops all κ‐dependence when comparing uκε to uε. However, no estimate is given for ‖uκε−uε‖ or its derivatives as κ→0. Without quantifying the regularization error, passing to the limit κ→0 in all terms cannot be justified."
      }
    ],
    "think_summary": [
      "**Questioning assumptions in the lemma**\n\nThis lemma raises some concerns for me. It assumes that certain values vanish beyond a certain limit, implying no noise, but the drift and diffusion terms remain. I don’t think they justify why those fields won’t affect the value. They're using a specific convex function but claiming that the results mean values remain within bounds even with certain behaviors in the terms outside the defined limits. It seems problematic, especially given the monotonic nature of those terms. They appear to ignore restrictions necessary for valid conclusions.",
      "**Analyzing approximation solutions**\n\nI see potential issues with the solution's range, especially regarding the use of standard mollifiers in Itô integrals. They apply a formula without adequately addressing predictability or integrability, leaving some details vague. Then there's the technical aspect of error terms, where third-order Taylor expansions are discussed, but there's uncertainty about the boundaries on derivatives. Lastly, concerning limits in Itô term, the authors seem to overlook how certain bounds could increase as limits approach zero, which likely affects their conclusions. They really need more clarity in their assumptions and justifications.",
      "**Identifying critical issues in the analysis**\n\nI'm seeing several critical problems in the analysis. Firstly, they haven't ensured that terms remain bounded as certain parameters approach zero, which affects the validity of their maximum principle. Secondly, they don't adequately justify changes in the order of limits, especially concerning convergence issues. Additionally, they seem to overlook possible errors in their mollification process, leading to the potential invalidation of their conclusions in the lemma. These concerns cast doubt on their assumptions and the soundness of their results. It’s essential to clarify these points to ensure accuracy."
    ],
    "token_usage": {
      "input": 41806,
      "thinking": 2112,
      "output": 799
    }
  }
]